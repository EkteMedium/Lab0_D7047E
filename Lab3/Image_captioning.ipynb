{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anton\\AppData\\Local\\Temp\\ipykernel_14628\\2010194887.py:5: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader, dataset\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.rcParams['figure.figsize'] = [12, 30]\n",
    "#plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device is\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dir):\n",
    "        self.image_dir = os.path.join(dir,\"Images\")\n",
    "        self.df = pd.read_csv(os.path.join(dir,\"captions.txt\"))\n",
    "\n",
    "    def __getitem__(self,i):\n",
    "        caption = self.df[\"caption\"][i]\n",
    "        image_path = os.path.join(self.image_dir,self.df[\"image\"][i])\n",
    "        image = transforms.ToTensor()(Image.open(image_path))\n",
    "        return (image,caption)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXAMPLE/TEST TO BE DELETED\n",
    "ds = CaptionDataset(\"archive\")\n",
    "data_loader = DataLoader(ds)\n",
    "it = iter(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        # define the properties\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # lstm cell\n",
    "        self.lstm_cell = nn.LSTMCell(input_size=embed_size, hidden_size=hidden_size)\n",
    "    \n",
    "        # output fully connected layer\n",
    "        self.fc_out = nn.Linear(in_features=self.hidden_size, out_features=self.vocab_size)\n",
    "    \n",
    "        # embedding layer\n",
    "        self.embed = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embed_size)\n",
    "    \n",
    "        # activations\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        \n",
    "        # batch size\n",
    "        batch_size = features.size(0)\n",
    "        \n",
    "        # init the hidden and cell states to zeros\n",
    "        hidden_state = torch.zeros((batch_size, self.hidden_size)).cuda()\n",
    "        cell_state = torch.zeros((batch_size, self.hidden_size)).cuda()\n",
    "    \n",
    "        # define the output tensor placeholder\n",
    "        outputs = torch.empty((batch_size, captions.size(1), self.vocab_size)).cuda()\n",
    "\n",
    "        # embed the captions\n",
    "        captions_embed = self.embed(captions)\n",
    "        \n",
    "        # pass the caption word by word\n",
    "        for t in range(captions.size(1)):\n",
    "\n",
    "            # for the first time step the input is the feature vector\n",
    "            if t == 0:\n",
    "                hidden_state, cell_state = self.lstm_cell(features, (hidden_state, cell_state))\n",
    "                \n",
    "            # for the 2nd+ time step, using teacher forcer\n",
    "            else:\n",
    "                hidden_state, cell_state = self.lstm_cell(captions_embed[:, t, :], (hidden_state, cell_state))\n",
    "            \n",
    "            # output of the attention mechanism\n",
    "            out = self.fc_out(hidden_state)\n",
    "            \n",
    "            # build the output tensor\n",
    "            outputs[:, t, :] = out\n",
    "    \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anton\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Anton\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to create tensor with negative dimension -1: [1000, -1]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 20\u001b[0m\n\u001b[0;32m     13\u001b[0m resnet \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mresnet18(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#Freeze parameters\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#for param in resnet.parameters():\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#    param.requires_grad = False\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#Replace connected layer\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m resnet\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m (\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     22\u001b[0m summary(resnet, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m))\n",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_linear \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\linear.py:98\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[1;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features \u001b[38;5;241m=\u001b[39m in_features\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features \u001b[38;5;241m=\u001b[39m out_features\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty(out_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to create tensor with negative dimension -1: [1000, -1]"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.last_linear = nn.Linear(-1,1000)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.last_linear(x)\n",
    "        return x\n",
    "\n",
    "# Use a predefined model as a feature extractor\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "#Freeze parameters\n",
    "#for param in resnet.parameters():\n",
    "#    param.requires_grad = False\n",
    "\n",
    "#Replace connected layer\n",
    "#resnet.fc = (Linear())\n",
    "\n",
    "summary(resnet, (3, 224, 224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = list()\n",
    "val_losses = list()\n",
    "\n",
    "for epoch in range(1, 10+1):\n",
    "\n",
    "    for i_step in range(1, total_step+1):\n",
    "\n",
    "        #zero the gradients\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "\n",
    "        # set decoder and encoder into train mode\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "\n",
    "        #radnomly sample a caption lengt, and sample indices with that length\n",
    "        indices = train_data_loader.dataset.get_train_indices()\n",
    "\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        train_data_loader.batch_sampler.sampler = new_sampler\n",
    "        \n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(train_data_loader))\n",
    "        \n",
    "        # make the captions for targets and teacher forcer\n",
    "        captions_target = captions[:, 1:].to(device)\n",
    "        captions_train = captions[:, :captions.shape[1]-1].to(device)\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions_train)\n",
    "        \n",
    "        # Calculate the batch loss\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions_target.contiguous().view(-1))\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer\n",
    "        optimizer.step()\n",
    "        \n",
    "        # - - - Validate - - -\n",
    "        # turn the evaluation mode on\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # set the evaluation mode\n",
    "            encoder.eval()\n",
    "            decoder.eval()\n",
    "\n",
    "            # get the validation images and captions\n",
    "            val_images, val_captions = next(iter(val_data_loader))\n",
    "\n",
    "            # define the captions\n",
    "            captions_target = val_captions[:, 1:].to(device)\n",
    "            captions_train = val_captions[:, :val_captions.shape[1]-1].to(device)\n",
    "\n",
    "            # Move batch of images and captions to GPU if CUDA is available.\n",
    "            val_images = val_images.to(device)\n",
    "\n",
    "            # Pass the inputs through the CNN-RNN model.\n",
    "            features = encoder(val_images)\n",
    "            outputs = decoder(features, captions_train)\n",
    "\n",
    "            # Calculate the batch loss.\n",
    "            val_loss = criterion(outputs.view(-1, vocab_size), captions_target.contiguous().view(-1))\n",
    "        \n",
    "        # append the validation loss and training loss\n",
    "        val_losses.append(val_loss.item())\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # save the losses\n",
    "        np.save('losses', np.array(losses))\n",
    "        np.save('val_losses', np.array(val_losses))\n",
    "        \n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Val Loss: %.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), val_loss.item())\n",
    "        \n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "            \n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        print(\"\\nSaving the model\")\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pth' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pth' % epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transforms.Compose([\n",
    "    # Add training augmentations here, remember: we do not want to transform the validation images.\n",
    "    # For information about augmentation see: https://pytorch.org/vision/stable/transforms.html\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(size=(224,224)),\n",
    "    #transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, decoder, optimizer, train_loader, val_loader, epochs, tokenizer, criterion):\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Starting epoch\",epoch,\"/\",epochs)\n",
    "        train_loss_sum=0\n",
    "        train_loss_num=0\n",
    "        for batch_nr, (image, caption) in enumerate(train_loader):\n",
    "            if(batch_nr%20):\n",
    "                print(\"Processing batch:\",batch_nr)\n",
    "            images = images.to(device)\n",
    "            caption = caption.to(device)\n",
    "            features = encoder(image)\n",
    "            prediction = decoder(features)\n",
    "            caption_tokens = tokenizer(caption)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(prediction,caption_tokens)\n",
    "            train_loss_sum+=loss.item()\n",
    "            train_loss_num+=1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        val_loss_sum=0\n",
    "        val_loss_num=0\n",
    "        for batch_nr, (image,caption) in enumerate(val_loader):\n",
    "            images = images.to(device)\n",
    "            caption = caption.to(device)\n",
    "            features = encoder(image)\n",
    "            prediction = decoder(features)\n",
    "            caption_tokens = tokenizer(caption)\n",
    "            loss = criterion(prediction,caption_tokens)\n",
    "            val_loss_sum+=loss.item()\n",
    "            val_loss_num+=1\n",
    "\n",
    "        print(\"Train loss:\",train_loss_sum/train_loss_num,\"\\t Val loss:\",val_loss_sum/val_loss_num)\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is': 0, '.': 2, 'this': 1, 'it': 3, 'not': 4, 'a': 7, 'use': 5, \"'\": 6, 'sentence': 15, 'do': 8, 'for': 9, 'in': 10, 'intended': 11, 'normal': 12, 'please': 13, 's': 14, 'shit': 16, 'test': 17, 'vain': 18}\n",
      "[['this', 'is', 'a', 'normal', 'test', 'sentence', '.', 'please', 'do', 'not', 'use', 'it', 'in', 'vain', 'for', 'this', 'is', 'not', 'it', \"'\", 's', 'intended', 'use', '.'], ['this', 'is', 'shit']]\n",
      "[[1, 0, 7, 12, 17, 15, 2, 13, 8, 4, 5, 3, 10, 18, 9, 1, 0, 4, 3, 6, 14, 11, 5, 2], [1, 0, 16]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenized_reviews)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(numericalized_reviews)\n\u001b[1;32m---> 15\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marray(numericalized_reviews)\n\u001b[0;32m     16\u001b[0m t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray(numericalized_reviews))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "reviews = [\"This is a normal test sentence. Please do not use it in vain for this is not it's intended use.\",\"This is shit\"]\n",
    "import torchtext\n",
    "\n",
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    tokenizer(review) for review in reviews\n",
    ")\n",
    "\n",
    "print(vocab.get_stoi())\n",
    "\n",
    "tokenized_reviews = [tokenizer(review) for review in reviews]\n",
    "numericalized_reviews = [[vocab[token] for token in review] for review in tokenized_reviews]\n",
    "print(tokenized_reviews)\n",
    "print(numericalized_reviews)\n",
    "np.array(numericalized_reviews)\n",
    "t = torch.from_numpy(np.array(numericalized_reviews))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
