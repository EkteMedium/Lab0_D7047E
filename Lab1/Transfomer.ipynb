{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, dataset\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "from data_loading_code import preprocess_pandas\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, classification_report\n",
    "\n",
    "import math\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Tuple\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device is\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.embedding = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.linear = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            src: Tensor, shape ``[seq_len, batch_size]``\n",
    "            src_mask: Tensor, shape ``[seq_len, seq_len]``\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
    "        \"\"\"\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        if src_mask is None:\n",
    "            \"\"\"Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\n",
    "            Unmasked positions are filled with float(0.0).\n",
    "            \"\"\"\n",
    "            src_mask = nn.Transformer.generate_square_subsequent_mask(len(src)).to(device)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.linear(output)\n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([1280,  194,    9,  167,   16,  243,    2,  551,    3,  254,   16,   10,\n",
      "          26,   68,   12]), tensor([   2, 1303,    7,  566,  435,    4,  182,    5,  106, 1295,    6,  419,\n",
      "           1]), tensor([  3,  61, 140,  48,   8,  29,   1]), tensor([   8,   59,   18,  121,    2,  333, 1315,  412,    4,    5,  170,    2,\n",
      "         120,   89,    3,  187,    9,  126,    5,   24,    1]), tensor([  3, 140,  48,   8, 117,   9, 816,  12]), tensor([   5,   30,  119,   21,   14,  124,  674,    6,    4,   10,  209,   51,\n",
      "          47,    2, 1365,   31,  138,    1]), tensor([  73,   11,   25,  123,    2,  162,  327,   22,    3,  138,    4, 1102,\n",
      "          73,   11,   25,   72,    8,   13,    1]), tensor([   3,   92,    2,  606,   49,   16,   14,   94,  333, 1335,    4,    5,\n",
      "          78,   60, 1346,   46, 1127,   14,  892,  257,   38,  155,    1]), tensor([147,  29,   6,   4, 130,  15,  93,  12]), tensor([ 23,  26, 542,  16,   2,  68,   1]), tensor([365,   5,  30,  62, 365,   5,   7, 104,   1]), tensor([  83,   18,   72,   16,  137,    1,    1,    1, 1368,  371,   16,  137,\n",
      "           1]), tensor([ 95,  45, 831,  12]), tensor([996,  29,   1]), tensor([466,  81,  32,   7,  10,  12,   1]), tensor([   2,  490,   15,    2,  112,  763,   20,   38,  106,    5,  379,  557,\n",
      "          10,  234,  211,  228,    3,  268, 1004,  172,   19,    2,  166,  404,\n",
      "         239,    1]), tensor([ 26,  29,   6,  26, 525,   1]), tensor([1340,   62,   83,   18,   72,   12,   12,    1]), tensor([ 88,  27, 102, 196,  16, 193,   4, 669,   2, 899,  41,  15, 613, 128,\n",
      "        502,  22,   3,  27, 177,   9, 746,   1]), tensor([  3,  84,   8, 117,   1]), tensor([   3,  489,   10,  158,  798,    4,   20,   23,  107,   19,    5,    6,\n",
      "          34,    2,   36,   41,    6,  304, 1066,    6,   21,    8,    7,   51,\n",
      "           1]), tensor([   2,  841,  650, 1054,   71,    2, 1061,   22,   33, 1045,    1,  439,\n",
      "           7,   17,    1]), tensor([407,   7,  52,   4, 199,   7,  39,  26,  39, 115,  53,  22, 275,   1]), tensor([   2,  329, 1205,    7,  931,    1]), tensor([   3,  138,   18,  386,  719,    2,  291,   16,  176,  778,   29,   34,\n",
      "           3,  971,    2,  974,   22,    3,  348,   18,   27,   82,    8,  433,\n",
      "        1056,  614,    1]), tensor([  33,   15,   14,  838, 1132,  139,    1]), tensor([ 658,  636,    6,   28,   98,  850,   93,  127,  235,    2, 1297,    1]), tensor([ 28,  64, 231, 270, 494,  19,   5,  24,   2,  49,  90,   2, 972,   7,\n",
      "        735,   1]), tensor([   2,  742,    4,  150,   45, 1095,  257,    1]), tensor([1065,    6,  106,    9,  451,    4,   42,    6,  153,   36,    1]), tensor([109,   7, 147,   4, 864,  15, 538,   1]), tensor([166, 511,   7, 559,  12]), tensor([ 83,  18,  72,  83,  18, 660, 352]), tensor([ 43, 446, 345,   1]), tensor([   3,   11,  464,   65,  784,  604,    6,    4,   14,   32,  536, 1174,\n",
      "          16,   77,  508,    1]), tensor([   2, 1103,   49,  105,   11,   25,  121,    1]), tensor([  3,  61, 598,   9,  18, 260,   8,  74,   5, 255,  78,  23,  43,   1]), tensor([ 51,  47, 238,   1]), tensor([147,  13, 902,   1]), tensor([  2, 150,  16,  21,   4, 161,  45, 134,   1]), tensor([167, 352,   1]), tensor([ 54,  28, 495,   9,  42,   8,  24,  10, 116, 432,  79,   5,   1]), tensor([  64,  142, 1226,  175,    9,  121,  206,   14,  600,  697,  408,  539,\n",
      "         192, 1159,    2,   49,    1]), tensor([   8,   13, 1311,   23,  242,    9,   83,  154,   34,  834,  133,    5,\n",
      "          11,   37,   23,  368,    9,   65,   10,   13,    1]), tensor([  31,  159,  146,   18,  160,  175,   24,  124, 1137, 1330,    1]), tensor([23, 26, 13,  1]), tensor([30, 26,  1]), tensor([  2, 217, 960,   7,   2, 149,   6,   4,  15, 732,   5, 648,   1]), tensor([ 54,  28,  73,  11,  25, 204,   5,   6,  69, 134,   6,  39, 194,   2,\n",
      "        164, 700,  19,  33, 795,  67,   1,   3,  61,  18,  48,   8,   9, 195,\n",
      "          1]), tensor([  3,  73,  11,  25,  56,   8, 477, 302,   1]), tensor([ 14, 422,  20, 147,   1,   1,   1,   1,   1,   8,  20,  14, 856,  81,\n",
      "         32,   6,   4, 191,   5,  20, 143,  75, 104,  47,  14, 248, 158,  70,\n",
      "        101,   3, 895,  12,  12,  12]), tensor([  5, 524, 233, 523,   6, 197, 903,  21,   9,  14, 230,   6,   4, 954,\n",
      "          2, 319, 450,   1]), tensor([  67,   35,    8,   10, 1031,   34,    5,   11,   37,   78,  429,   38,\n",
      "         155,    1]), tensor([  5,  59, 154,   2, 297, 518,   5,  61,   1]), tensor([   3,   27,  579,    9, 1181,    8,   94,   31,  382,  112,  227,    4,\n",
      "          22,   11,   37,  353,  174,  192,  288,    1]), tensor([   8,    7,   10,  185,  325,   13,    9,   42,    6,   34,    2,  647,\n",
      "           7, 1318,    1]), tensor([157,   3, 474, 220,  51, 940,   1]), tensor([ 4,  2, 36, 41,  7, 17,  1]), tensor([   5,   11,   37,  321,   15,  417,    9,   42,   90,   15,  141,    5,\n",
      "         208,    4, 1033,    5,   11,   37,  417,  141,  685,    2,  291,    7,\n",
      "           1]), tensor([ 495,   21, 1062,   53,  148,  194,    4,  194,    1]), tensor([  30,   17,    6,  100,   14,  671,  624,    2,   13,  441,  346,    2,\n",
      "        1128, 1257,  452,   15,  760,    2,  128,    1]), tensor([  3,  84,   8, 111,  12]), tensor([493,  14,  13,   7, 753,   6,  34, 125, 231, 122,  50, 100,   3,  42,\n",
      "          8,   1]), tensor([273,  45, 264,   5, 168,   1]), tensor([336, 399,   1]), tensor([   3,   27,    9,   42,    2, 1228,  300, 1129,    6,   34,    5, 1249,\n",
      "          21,  215,   43,    1]), tensor([  3,  84,   2, 251,   4, 427,  15, 344, 847,   1]), tensor([  5,  11,  37, 102,  14, 394,  32,  16, 193,   1,  17,  36,  26, 149,\n",
      "         26, 329, 665,   1]), tensor([472, 105,  11,  25,  44,   1]), tensor([73, 11, 25, 72,  8, 29,  1]), tensor([  3,  84,  14,  32,   1,   1,  14, 158,  81,  32,   7,  17,   6,   2,\n",
      "        109,   7,  23,  26,   4,   2,  40, 144,   7,  10, 104, 121,   1]), tensor([  3,  35, 165, 193, 277,   4,  60, 978, 221, 129,   1]), tensor([1245,   62,  147,  295,  129,    1]), tensor([ 136,   11,   25,   42,    2,  164,   19, 1265,    6,   18,   26,   24,\n",
      "        1277,   12]), tensor([52, 13,  1]), tensor([   8,   74,    7,   17,    6,  938,    5,    6,  459,    5,  391,  113,\n",
      "        1072,    4,    5,   46,  102,  132,   26,  237,  266,   19,   77,  216]), tensor([  2, 414,  21,   8,   7,  69, 964,  86,  69, 897,   1,   1,   1,   5,\n",
      "        955, 835,  87,  15,  14,  40,   1]), tensor([576,  29,   1]), tensor([   3,   92,    2,  116,   91,    4,   18,  118,   63,   10,  274,    2,\n",
      "          91,   20,  387,    1,    1,    1,    3,  222,    9,  126,    5,   24,\n",
      "           4,    5,  268, 1229,    1]), tensor([ 39, 253, 125, 708,   6,   3, 177,   8,  32,  11,  37, 473,  20,  23,\n",
      "        360,   1]), tensor([   2,  149,   16,    2, 1173,    7,  217,   26,   70,   28,   27,  686,\n",
      "         141,  210,   57,    1]), tensor([  10,  274,  324,   63,    3,  593,    5,    6,    5, 1264,  764,    1]), tensor([1169,  126,   30,   17,    1]), tensor([  5, 240,  75, 104,  47, 183, 442,  90,   3, 361, 882,   4,  22, 178,\n",
      "         24,   2, 190, 530,   1]), tensor([  3, 569,  14,  93, 168,   1]), tensor([   2,   49,    7,   10,  430,  144,   15,  497,    4,   46,   77,  436,\n",
      "          86,  527,  501, 1348,    1]), tensor([   2,   67,  111,   22,  768,   50,    7,    2,  936,  512, 1108,   70,\n",
      "         946,   57,    1]), tensor([ 18, 575,   5,   1]), tensor([ 392,   34,  898,    5,   30,    1,    1,   20, 1101, 1270,  880,    2,\n",
      "         252,  726,   15,    8,   74,    1]), tensor([  2, 126, 138,  18,  44,  23,  43,   1]), tensor([ 162,  145,   39,  488,   27, 1012,    1]), tensor([1109,    4,    5,   30,    1]), tensor([   3,  418,  113, 1225,    5,   21,    2,  800,   15,   14, 1081,   86,\n",
      "         168, 1106,  452,    1]), tensor([364,  13, 139,   1]), tensor([  5,   7, 185,   9,  42,   4,   3,  56,   5,   1]), tensor([   3, 1358,    3,   98,  263,    2,  164,    4,   66,  168,   14,   93,\n",
      "           1]), tensor([1363,   44,  127,   86,  622,   16,   50,    1]), tensor([ 29,  20,  52,   4,  30,  51,  47,   2, 165,  33,   4, 645,  20,   5,\n",
      "        682,  12]), tensor([ 72,  10, 200,  13,  62,  34,  18,   8,   1]), tensor([ 18,  39,  26,  39,   3,  35, 906,   1]), tensor([   5, 1134,  132,   63,    3,   11,  137,  188,    5,   16,   79,  212,\n",
      "           6,   38,    3,  108,  337,  610,   33,   90,    8,    7,    2,  103,\n",
      "          32,    3,   11,   96,  139, 1074,    1]), tensor([ 30,  56,  10, 289,   1,   1,  30,  39, 371,   1]), tensor([   3,  956,  672,    2,  198,   21,    2, 1191,    4,    3,   35,    9,\n",
      "         503,    2,   13,   87,    9,  561,    5,   21,   97,  161,    1]), tensor([   2,  198,  208,   38,  550,    4,  430,    6,    5,    7, 1186,    1]), tensor([ 446, 1271,    8,    7,   10,   26,   41,  116,   91,    4,  110,   11,\n",
      "          37,   18,  143,  515,   79,    5,    1]), tensor([   3,  718,    2,  291,    4,   88,  556,   50,   22,    6,  373,    2,\n",
      "         164,   20,  146,  354, 1341,    6,   54,    3,  136,   11,   25, 1123,\n",
      "          14, 1145,    3,   20,   87,   15,  998,    1]), tensor([   3,  437, 1357,  535,    1,    3,  114,  717,  690,    4,  935,  148,\n",
      "          15,    8, 1115,    1]), tensor([ 157,    6,    2,  959,   45,   38, 1291,   22,    3,  530, 1139,    2,\n",
      "         578,  150,    1]), tensor([105,  11,  25, 248, 160,   1]), tensor([ 973, 1121,    7, 1362, 1352,   12,    1]), tensor([ 77, 217, 927,   1]), tensor([  88,  455,  878,   51,    4,   51,   70,    8,    7,   14,  553,   33,\n",
      "           4,    3,   11,   96,   35,  480, 1080,   69,   57,    1]), tensor([110,  20,  38, 143, 916, 184,   8,  13,  22,   3, 621,   5,  20,   2,\n",
      "        103,   6,  14, 327,   1]), tensor([ 29,   7, 420,  39, 756,   1]), tensor([ 8, 13, 30, 17,  1]), tensor([252,  41,   1]), tensor([   3, 1148,   35,  216,  362,    3,   98,   18,  269,  398,   16,   75,\n",
      "          47,  211,  228,  229,  771,    1]), tensor([   5,  156,   38, 1192,   22,    2,   40,  905,   59,   18,  118,  254,\n",
      "           9,   65,  188,    4,    2,   36,    7,   51,  766,  554,   55,   40,\n",
      "         664,    1]), tensor([17, 49,  4, 68, 12]), tensor([   2,  166,  511, 1287,    2,  893,    7,   52,    1]), tensor([52, 29, 16,  2, 68,  1]), tensor([1069,    6,    3,   80, 1130,    9,   27,   10,   13,  101,   46,   71,\n",
      "          14,  618,    4,  293,   24,    4,  178,   17,  109,    1]), tensor([ 158,   40,  874, 1135,    2,  103,   12,    1]), tensor([ 26,  29,  62, 449, 357,   1]), tensor([ 625,  458,   19,   76, 1207,    4,  701,    1,    3,   48,   28,    8,\n",
      "          49,   12,   12]), tensor([213,  15,   5,  30,   6, 108,  73,  11,  25,  72,   5,   1]), tensor([   5, 1100,  994,   47,  115,  256, 1241,   15,    8,  349,    2,   68,\n",
      "           7,   38,  252,   22,  183,   61,  552,    2,   41,    7,  456,    6,\n",
      "         157,    6,    5,   11,   37,   18,    1]), tensor([   3, 1190,    2,  246,    6,    4,  177,    8,   33,    9,   65,    2,\n",
      "         103,  357,    1]), tensor([ 76,  49,   6, 240,  26,  24,  55, 311,   1]), tensor([ 26,   6,  30, 119,   1]), tensor([ 16,   2,  68,  21, 167,   6,   5,   7,  97,  52,  29,   6, 101,   3,\n",
      "         61, 140,  48,   1]), tensor([  3,  82, 112,  15, 148,   4, 475, 114, 152,   1]), tensor([ 31, 159,   7, 217,  26,   1]), tensor([   2,  128,   59,   66, 1232,  301,   90,    5, 1302,   55,   40,    4,\n",
      "         305,    1]), tensor([342,  31,  62,  26,  72,   1]), tensor([ 154,   79,    5,    7,  119,    4, 1143,   16,    2,   68,    3,    1,\n",
      "         412,    1]), tensor([  3,  11,  96, 187, 218, 200, 300,  16,  14, 135,  13,   4,   8, 158,\n",
      "         33,   7,   2, 120,  33,   3,  11,  96, 177,  22, 156,  14,  40, 290,\n",
      "          1]), tensor([  3,  61, 140,  48,   8,   1]), tensor([ 17, 499, 491, 582,  13, 699,   1]), tensor([ 17, 394,  12]), tensor([52, 81, 32,  1]), tensor([ 76, 314,  16,   2,  68,   4,  88,  44,  17,  12]), tensor([  14,   13,  105,   11,   25, 1223,  196,   14,  116,   99,    4,    2,\n",
      "         889, 1117,   14,   13,   53, 1227,   87,   15,   14,  891,    1]), tensor([ 3, 84,  8, 81, 12]), tensor([134, 109,   1]), tensor([  14,   13,  532,  483,   70,   18,   17,   62,  483,   57,    6,   34,\n",
      "          14,  223,   11,   37,   13,   20,  372, 1300, 1320,    6,  265,  136,\n",
      "          11,   25,  563,   10, 1364,  229,  518,   21,    5,    1]), tensor([925, 297,   1,   1,   1,   1,   3,  35,   9, 263,   5,   1]), tensor([   5,  752,    2, 1133,   15,   10,   81,   32,    1]), tensor([  3,  27, 102,  23, 345,  19,   8, 135,  13,  53, 747,  33,   1]), tensor([  3, 487,   8,  29, 120,   4,  20, 564,  19,   5, 923,   1]), tensor([  88,   83,   18,  248,  310,    6,   34,    7,   18, 1070,  826,    9,\n",
      "         262,    1,  106,    9,  486,    4,    2,   36,    7,  143,   51,   47,\n",
      "         488,    3,   27,  187,    1]), tensor([  4,   3, 108,  84,   2, 698,  12]), tensor([ 62,  57, 482,   6,   2,  91, 524,   9,  44, 119,   1]), tensor([  14, 1183,   46, 1272,  780,   15,  299,   21,  641,  192,  919,  416,\n",
      "           1]), tensor([  3,  27, 187,  95, 662,  19,  14, 397,   4,  14, 453,   4,   5,  30,\n",
      "        108, 119,   1]), tensor([388,  66,   2, 186,   9,  44,  19,  14, 397,   1]), tensor([ 88,  83,  18, 389,  79,   2, 400,  33, 384,   1]), tensor([  5, 105,  11,  25, 123,  28, 251, 173,   1]), tensor([ 814,   28,   58,  432, 1020,   11,   37,  548,  544,    1]), tensor([  3,  80, 490,   8,  19,  14, 319,   6,   4,   3,  98,  18,  65, 312,\n",
      "         19,   5,  38, 155,   1]), tensor([ 131, 1111,   50,    9,   66,    8,  207,    6,    3,   27,   77,  917,\n",
      "           1,    1,    1]), tensor([  3,  60, 358,   2, 496,   9,  65,   2, 127,  33,   6,  34,   5,  46,\n",
      "         69, 253, 454,  16,  50,   1,   2,  26]), tensor([  3,  11,  85, 335,   1]), tensor([ 31,   7, 147,   1]), tensor([61, 48,  8, 74,  1]), tensor([143, 250,  47,   2, 320,   3,  20, 241,   9, 262,   5,  19,   1]), tensor([295, 129,  20, 147,   1]), tensor([30, 43,  1]), tensor([  2,  31,   7, 356,  39,  43,  39,   2, 129,  42, 612,   1]), tensor([  8,   7,  10,  26,  31,   6,   4,   5,  92, 315,  60, 308,   1]), tensor([  64,    6,   54,   55,   13,    7,  236,    6,    8,   49,    7,   18,\n",
      "         241,    9,  519,    5,    6, 1242,  100,  236,  305,  235,    1]), tensor([  3,  11, 137,  56,   9, 263,   5,   1]), tensor([ 36,  41,  21, 197, 303,   7,  52,   6,   3,  42,  32,   9, 151,  14,\n",
      "        223,   4, 376,  14, 223,   9,  42,  32,   9, 151,  50,  12,   1]), tensor([   2, 1278, 1017,  239,    7,   60, 1310,    9,   42,    1]), tensor([142,  18, 566, 435,   1]), tensor([134, 121,   6, 190,  69, 169,   1]), tensor([   8,   29,   35,   10,  541, 1180,  529,   22,   20, 1319,   63,   10,\n",
      "         191,    4,  390,   50,    9,  263,    5]), tensor([  17,   16,  189,   19,   55,  317, 1251,    1]), tensor([17, 68,  6, 69, 12]), tensor([1313,  378,    1]), tensor([215, 144,  15, 207,   1]), tensor([142, 424, 350,   4, 547, 310,   9,  83, 279,  19,   5,   1]), tensor([   8,    7,   10,   23,  627,   13,   19,  134,   31,  159,   22, 1059,\n",
      "          21,   10,  360,  328,    1]), tensor([205,   6,  35,   9, 546, 555,   1]), tensor([269, 377,  53,   2, 506,  12,   1]), tensor([  14,   32,   30,  108, 1088,    1]), tensor([   2,  995, 1060,    7,   17,    6,    2,  656,   19,    2,  977,    7,\n",
      "          23,    1,    1,    1,  616,    1]), tensor([226, 117,   1]), tensor([  2, 472, 110,   7,  10, 951,   6,   4,   2, 149,   7, 340, 252,   1]), tensor([   1,    1,    1,    1,   74,  280,  339,    4,   30,   17,   19,   14,\n",
      "        1019, 1086,  344, 1187, 1224,   13,    4,  531, 1116,   36,   24,   40,\n",
      "        1105,    1]), tensor([ 76, 458,   1]), tensor([60, 26, 29,  1]), tensor([ 309,    6,   63,  353,   86,  855,  555,    2,  534,   15,    2,  965,\n",
      "         170,    4,    3,   98,   18,   42,    5,  115,  467,   21,    2, 1333,\n",
      "           1]), tensor([1326,   13,    6,  528,  748,    1]), tensor([   2,  261,    7,   23,  403,    6,    3,   11,   96,  102,  276,    9,\n",
      "        1176,  196,   14,  908,   19,    2,   13,   24,    2,  984, 1178,   19,\n",
      "          77, 1149,   41,  454,    1]), tensor([496,  81, 818,  72,   1]), tensor([  2, 109,   7,  52,  12]), tensor([  26, 1306,  225,    1]), tensor([  3,  20, 374, 133,   2, 507, 619,  15,   2, 112, 331, 460, 135,  13,\n",
      "        381,   4,   4, 133,  10, 857,  15,   2,  68,   1]), tensor([  3,  84,   8,  13,   6,   5,   7,  23, 440,   4,  46,  10, 209,  15,\n",
      "        839,   1]), tensor([105,  11,  25, 444, 152,   1]), tensor([ 71,   5, 557,  20,  33, 411,  53,  79, 928, 584,   2, 961, 729,   4,\n",
      "          5,  20, 736,   1,   3,  80,  18, 180,   4,   3,  80,  18, 968,   1]), tensor([ 83,  18,  72,  54,  28, 569,   9,  42,   2, 316,   1]), tensor([  3,  27, 337,  95,  16, 197, 307,   4, 861,   6,   4,  71, 808, 221,\n",
      "        232,   4, 415,  15,  42]), tensor([205,   6, 205, 502,   1]), tensor([ 144,   15, 1308,    1]), tensor([ 17, 357,   1]), tensor([30, 16, 50,  1]), tensor([126,  20,   2, 578, 349,   1]), tensor([629,   9,  42,   4, 356,   1]), tensor([   2,   33,  169,  783,   15,    2, 1038, 1098,    7,   22,    2,  150,\n",
      "          21,    2,   13,   11,   37,  436,  733,   22,  459,   28, 1084,    4,\n",
      "        1221, 1237,  465,   87,   63,   10,  176,  522,    1]), tensor([ 789,  218,  200, 1025,  172,    6,    3,  827,  299,    4,    5,   20,\n",
      "          24,   97,  375,   22,   35,  227,    1]), tensor([124, 309,  92,   2, 166,  41,  15,  10,  81,  32, 127,   1]), tensor([26, 49, 12,  1]), tensor([  77,   40,  987,  474,    6,    5,   11,   37, 1292,    4,    2,   36,\n",
      "           7,   17,    1]), tensor([  2,  31,   7, 132,  43,  39,  10, 513,  16,   2, 331,  22, 286,  19,\n",
      "          2,  13, 184, 193, 277,   1]), tensor([ 59, 154,   5, 348,   4,  75,   1]), tensor([  5,   7,  23, 104,   9, 361,  39,  43,   6, 101,   7, 500,   2, 183,\n",
      "        245, 620,  79, 189,  10,  49,   1]), tensor([ 141, 1258,    7,   22,  224]), tensor([   3,   82,    8,   13,   39,   10,  513,   16,   14, 1247,    4,   27,\n",
      "        1156,    5,  266,    1]), tensor([  3, 343,   5, 339,   4,   5,  30,  17,  12,  12]), tensor([ 273,   27,  187, 1321,    4,   88,  197,  306,  363,  212,    1,    1,\n",
      "        1126]), tensor([   5,    7,    2,  103,   91,    3,   27, 1196,   21,    2,  470,  579,\n",
      "           1]), tensor([202,  19,  31,   1]), tensor([17, 13,  1]), tensor([ 17,  16,   2, 320,   1]), tensor([1262,    6,  183,   15,    2,  542,   59,   18,   44,   19,   14,   13,\n",
      "           1]), tensor([571, 949,  21,   2,  13,   1]), tensor([ 145,    7,   22,    2,   40,  988,   45,  181,   15,  360, 1007,    4,\n",
      "         285,  301,    1]), tensor([  81,  261,    7,   26,   62,   10,  176,  174,  277,    3,  249,   14,\n",
      "          13,   24,    2, 1314,    6,   92,   10,  151,    6,    4,  667,    2,\n",
      "         401,  192,   10,  901,    1]), tensor([364, 295, 129,   1]), tensor([321,  15, 849, 196,   1]), tensor([ 165,   11,   37,  638,    6,  157,    6,   45,  201,    9,  563,  118,\n",
      "        1283,  221, 1119, 1096,   45,  185,    1]), tensor([   3,   58,  379,  139,  122,   21,    5,    4,   80,  715, 1185,  131,\n",
      "         224]), tensor([  3,  27, 112,  75, 193, 249,  24,   8, 720,   4,   3, 313,   8,  13,\n",
      "          1]), tensor([   8,   29,    7,  918,   16,  125,   56,   50, 1350,  203,   45,   23,\n",
      "        1198,    1]), tensor([131,  10, 169, 130,  15,  89,   1]), tensor([   3,   42,    8,   29,   24,   10, 1036,  721,  677,  362,  110,    7,\n",
      "          10,  209,   15,  244, 1334,  913,   53,    2,  810,    6,    4,    5,\n",
      "          30,   17,   12]), tensor([966,  88,  27, 102, 424,  76,   4, 443,  21,   2,  13,   1]), tensor([  5, 156, 290,  24, 302,  40,   6,   2,  36,   7, 153,   4, 210,   6,\n",
      "          4,   2, 152, 323,  10, 234,  15, 174,   1]), tensor([1289,    9,   91,   16,  402,  457,   75,   47,  211,    1, 1002,  216,\n",
      "          12,   12]), tensor([ 31, 323,  67,  10, 176, 318,   1]), tensor([ 23,  26,  29,   6,  43, 181,   1]), tensor([   3,  187, 1274,  217,  210,   34, 1211,   21,    2, 1276,  178,  484,\n",
      "           4,    3,   20,  146,  556,    5,  359,   11,   25,   17,    1]), tensor([ 46, 102, 132,  17,   1]), tensor([   3, 1131,    8,   16,    2,  116,   91,    4,    5,   59,   18,   44,\n",
      "           1]), tensor([426, 797,   1]), tensor([  8,   7,  10,  17, 325,  74,   1]), tensor([  8,  74,  78,  17,   6,  34,   5, 170,  63, 212,  15,  42,   1]), tensor([  3, 577,  11,  25,  48, 659,   8,  29,   1]), tensor([ 43, 332,   6, 280,  21,  89,   6,   4,  30,  39, 942,   1]), tensor([1079,   21,    2,   40,    1]), tensor([  3,  58,  11,  25,  42,   8,  49,  90,   2, 529,   7, 772,   1]), tensor([  8,  29,   7,  23, 244,  41, 393, 294,  12,  12,  12,  12,  12,  12]), tensor([533,  26, 509, 259,   4, 801,   6, 142,  22, 185,  12]), tensor([  3,  20, 276,   9,  83, 166, 404,  24,   2, 116,  19,  77, 145,   1]), tensor([  3,  80,  18, 180,  19,   8,   4,   3,  61,  18,  48,   8,  74,   9,\n",
      "        195,   1]), tensor([ 191,    3, 1005,    9,  635,    2,  969,  534,  168,   24,  258,    6,\n",
      "           2, 1018,   99,   46,  175, 1256,   22,    5,  114,  285,   21,    2,\n",
      "         476,  411,    1]), tensor([107,  38, 155,  12,   1]), tensor([  3,  27,   9, 950,   2, 126,   9,  66,   5,   9, 463, 113, 127,   9,\n",
      "         66, 403, 149,   1]), tensor([ 213,   15,    2,  353, 1219,   88, 1199,   19,    2,   32,   61,  269,\n",
      "          24,   14,  203,    1]), tensor([ 2, 31, 30, 17, 12]), tensor([   3,  177,    8,   29,    9,   65, 1336,   69,  169,    1]), tensor([1168,  318,  324,    1]), tensor([120, 161,   2, 109, 352,   6,   3,  27, 255,  35,  75,  47, 227,   6,\n",
      "        139,   1]), tensor([  2,  74, 343,  20, 730,   1]), tensor([366, 235,   2, 781,   1]), tensor([ 17, 311, 434, 117,   1]), tensor([   3, 1112,   75,  762,  703,  354,    2,  888,   86,  284,   13,    6,\n",
      "          34,    3,   27,    2,  845,  512,    4,    5,   11,   37,   10,   17,\n",
      "         395,   12]), tensor([  28,   58,  118,  270, 1197, 1110,   19,    2, 1068,   70,  829,   57,\n",
      "         407,    6,   23,  173,    1]), tensor([   8, 1087,  517,    1]), tensor([   5,   46,  102,   10, 1354,   16,  565,    1]), tensor([   8,   20, 1328,  712,  133,  120,    6,  101,  390,   50,    9,  992,\n",
      "          10,  234,   15,   23,    6,   23,  245,  293,    1]), tensor([  5,  11,  37, 156,  56,  10, 883,   4,   7, 541,   6, 523,   6,   4,\n",
      "        788,   1]), tensor([  3,  35,   9, 179,   9,  10, 351,   4,  82,  10,  94, 477,  13, 101,\n",
      "          7, 132,  17,   1]), tensor([131,  10, 130,  15,  93,   4,  89,  12,   1]), tensor([ 28,  11, 464,  84, 141, 550,   5,   7,   1]), tensor([  3,  80,  75,  47, 107,  19,   8,  29,   1]), tensor([ 18,  10,  26, 282,   1]), tensor([  5,  20, 205,  12,   1]), tensor([ 64, 201,   9, 338,  21,   1,   3,  11, 137,  48, 628,   8,  29,   1]), tensor([ 10,  26,  41, 282,   1,   1,   3,  82,   8,  63,   3,  82,  10, 684,\n",
      "         53, 169, 993,  22, 532, 378,   4, 125,  21,   2, 256, 303, 136,  11,\n",
      "         25, 122,  50,   1]), tensor([ 83,  18, 123,   2, 162, 327,  39,  50,   1]), tensor([17, 13,  1]), tensor([  54,   28,  313,  794,    6,  281,    8,   13,  171,   71, 1008,    1]), tensor([ 355,    5,  114,   18,  510,   14,  319,   37,    6,  758,  714,    5,\n",
      "          53, 1039, 1113, 1240,   70,  920,    6,  830,   31,    6, 1339, 1067,\n",
      "           6,  419,   57,    1]), tensor([  2, 473,  64,  30,  43,   6,  34,  70, 591,   9, 125,   3,  27, 663,\n",
      "         57,   5, 617, 154,   1]), tensor([110,  11,  37,  60, 330, 134,   3,  58, 520,  79,   8,  32,   1]), tensor([  3,  11,  85,  10, 384, 202,   1]), tensor([  5,   7,  10, 952,   9,  42,   1]), tensor([   3,   27,   35, 1023,   16,   79,   10,  275,    4,    8,  687,    3,\n",
      "          82,  220,   16,    2, 1165,   15,    2,  307,    1]), tensor([846, 535, 498,   6, 498,   1]), tensor([  8,  31,   7,  97,  52, 282,  12]), tensor([336, 166, 232,   1]), tensor([   5,    7,  461,    6,  106,    9,   42,    6,    4,   46,   23,  153,\n",
      "         109,    4, 1305,    1]), tensor([  3, 204,   8, 933,   4,  38, 114, 500,  65, 514,   8,  13,   4, 493,\n",
      "        679, 668,   1]), tensor([ 157,    6,   63,   79,   10,  275,    6,    2,  848,  268,    9,   66,\n",
      "         989,    4, 1360,    4,  815,   14,  128,  222,  284,    4,    3,  136,\n",
      "          11,   25, 1146,    4,  258,  172,    1]), tensor([  5,  11,  37,  60, 106,   1]), tensor([  54,   28,   45,  342, 1075,    1,    1,    1,   28, 1041,   27,    8,\n",
      "          12]), tensor([   8,   13,    7,   23,  308,   19,  264,  115,  321,   15, 1016,    4,\n",
      "        1343,  651,    7, 1214,  836,   47, 1118,  214,    3,   27,  188,    1]), tensor([   3,   11,   96, 1027,  480,  172,   90,   15,    8,  508,    1]), tensor([ 23, 773,   1]), tensor([  63,  288, 1071,    6,   95,  381,   44,   17,    1]), tensor([ 482,    4,    3,  854,    9,   64, 1011,    2, 1345,  395,  416,    5,\n",
      "          46,   21,   55,   13,    1]), tensor([  8,   7,  10,  17, 749,   1]), tensor([  3,  35,   9, 260,  10, 200,  49,   1]), tensor([  23,  153,    6,   41,   36,    4,   28,   73,   11,   25,   27,    9,\n",
      "        1015,   19,    2,   36,   21,   55,  453,  266,   28,   27,    2,   36,\n",
      "         150,   21,    2,   32,    1]), tensor([   2,   31,  159,    7,  140, 1317,    1]), tensor([ 28,  66, 423, 211,  38,  22,  28,  58, 670,  87,   2, 151,   4,  18,\n",
      "         66, 740, 161,   1]), tensor([ 17,   5,  20,  94, 332,  76,  30,  26,   6,  77, 216,   4,   5, 286,\n",
      "         24, 250,  89, 163,   3, 238,  12,  12,  12,  12]), tensor([51, 47, 94,  1]), tensor([455, 113,   2,  26,  44, 167,  12,  12]), tensor([ 165,  548,  544, 1338,   14,  554,   10,  176, 1122,    6,  213,   15,\n",
      "         101,   78,    4,    3,  418,  113,  243,    9,   83,   10,  242, 1138,\n",
      "           6, 1355,   87,   71,   14,  296,    1]), tensor([   8,    7,  173,   90,  183,  287,   45,  108, 1057,  110,  601,    2,\n",
      "         128,    9,   66,   71,  346,  113,    1]), tensor([   2,  128,  349,    7,  169,    6,  957, 1077,  982,   43,  175,    6,\n",
      "           4,    2,  199,   41,    7,   52,   16,   10,  199,   13,    1]), tensor([   3,  118,  865,  391,    5,  228,    3,  222,    9,  633,    4,  562,\n",
      "         161,  385,  558,    4, 1351,    4,  479,   22,    5,   67,   35,  580,\n",
      "         249,   24,    2, 1032,    1]), tensor([  3,  27,  35,   8,  13,  16, 184,  10, 275,  99,   6,   4,   3, 114,\n",
      "        549,  28,   6, 142,  18,  22,  17,   1]), tensor([104,   6,  76, 261,   6,  26,  31, 159,   1]), tensor([  5,  46, 154,   3, 254,   4,   3, 136,  11,  25, 376,  16,  75,   1]), tensor([17, 68, 64, 12]), tensor([  3, 337,   8,   4, 363, 174,   5,  20,  77, 467, 132,  12,  12,  12,\n",
      "         12,  12,  12,  12,  12,  12]), tensor([948, 537,  39, 448,  53,   2, 297,  45,  18, 448,   1]), tensor([1234, 1210,  944,    8, 1239,    1]), tensor([  49,   20,   75,   86,  250,   97,  423,   22,    3, 1063,  338,   21,\n",
      "          34,  324,  770,   90,    5,  346,   14,   40,    1]), tensor([1366,    1]), tensor([  3,  61,  18,  48,   8,  74,   9, 195,   1]), tensor([ 10, 144,  15, 207,  22, 170,  63, 229,  21,  14,  13,  16, 174,  12,\n",
      "         12,  12]), tensor([ 10, 406,   1]), tensor([   5,  323,  250,   47, 1052,  211,    6,   54,    3,  370,  272,    9,\n",
      "          42,    2,   13,    1,   14,  223,   46,    2,  162,   13,   19,    2,\n",
      "         162,  145,    1]), tensor([ 12,   3, 755,  48,  12,  12]), tensor([462,   4,  30,  43,   1]), tensor([   2,  939, 1006,    7,  456,    1]), tensor([  8,   7, 528,   2, 103,  81,  32,  16,  36,  41,  12]), tensor([   3,   48,  445,    9,  195,   19,  200,  646,  135, 1092, 1099,   24,\n",
      "           2,  307,    1]), tensor([  38,  110,    7,   77,  190,   16,   50,    9,  126,    5,   24,  315,\n",
      "          24,    2,  565, 1323,    3,  179,  171,   10,  723,    1]), tensor([273,  27, 885,  10, 209,  15, 710,  21,   5,   1]), tensor([  38,  195, 1044,   28,  114,  122, 1082,   15,   55,  401,    1]), tensor([560,   9, 123,  10, 151,  21,  95,   7,  97, 823,  24, 862,   1]), tensor([   3,   20,   23,  335,    9, 1193,   22,    3,   98,  262,   14,   43,\n",
      "        1309, 1273,  316,   16,   14,  640,  431,    1]), tensor([ 355,    5,   11,   37,  106,    9,  369,  592,  148,   19,    2,  876,\n",
      "         150,   54,   28,  369, 1301,    2,   13,    9,   55,  305,  191,  981,\n",
      "           1]), tensor([   2,  149,  546, 1177,   87,   15,    2,  164,    9,   10,  759, 1322,\n",
      "           1]), tensor([ 73,  11,  25, 130,  55,  93,   4,  89,   1]), tensor([141,  58,  22,  65, 224,   2, 225,  41,   7, 336,   1]), tensor([ 17, 225,  12,   1]), tensor([  3, 140,  48,  95,   4, 806, 125,   9, 437, 148,  10, 272,   1]), tensor([68,  7, 26, 69,  1]), tensor([ 142,   10, 1299, 1076,    1]), tensor([  3,  84, 229, 276,   9,  42,  33,  32,  16, 197, 171, 963,   4, 135,\n",
      "          1]), tensor([ 16,  10,  29,  22, 727,  39, 143,  39,   8,  33,  59,   6,   3, 421,\n",
      "          5,   9,  44, 155,  51,   4,  19, 887, 415,  47,   8, 111,  59,   1]), tensor([   3,   56,    2,  833,   22,    5, 1167,  976,  599,   55,   40,    6,\n",
      "         341,   47,  450,    1]), tensor([  3,  11,  96,  35,   8, 642,  32,  16, 220,  89,  99,   4, 146,  18,\n",
      "        104,  19,   2, 190,   5, 156,  21,   2,  40,   1]), tensor([281,   8,  33,  54,  28,  58,   1]), tensor([  5,  20, 340, 104,  24,   2,  40,   1]), tensor([   8,  117,    7,   17,   24,  218, 1218,    1,   57]), tensor([73, 11, 25, 72,  5,  1]), tensor([  2, 590, 186,   7, 372, 649,   1]), tensor([264,   5, 168,   1]), tensor([  64,    2,  375,  362,   14,  164,  170,   57,    1,   62,    3,   11,\n",
      "          85,   18,   69,  852,   15,    2,  999, 1253,    1]), tensor([ 909, 1172,   45,   18,    2,  103,    6,    4,  475,   45,    2,  871,\n",
      "           1]), tensor([  2,  13, 547, 310,   9, 152,  56,   9, 318, 983,   1]), tensor([   3,   11,   85,  189,    5,   19,   97,  947, 1244,   70,   19,   49,\n",
      "          57,    4,    5,  156,  119,    1]), tensor([  5,  30,  17,  19,  10, 116,  91,   6, 304,  54,  28, 231, 126,  24,\n",
      "        112, 595, 133,   2, 162,  89,   1]), tensor([   2, 1003,   15,    2,  466,  793,  442,  306,    1]), tensor([  3,  11,  85, 438,   3, 177,   8,  29,  21, 167,   5,   7, 242,   9,\n",
      "        204,   6,   5, 359,  11,  25, 244, 259,   1]), tensor([   3,  146, 1001,   22, 1030, 1209,   11,   25,  123,  314,    6,  273,\n",
      "         108,  481,   73,   11,   25, 1203,  175,  777,    9,  725,    2,  298,\n",
      "         184,    9,  912,    1]), tensor([125, 728, 122,  50, 271,   4,   3,  35,   9, 503,  87,   2, 796,   4,\n",
      "        271,  21,   2,  13,   1]), tensor([  2, 109,  46, 102, 875,  26,   1]), tensor([   3,   27,   35,  216, 1359,  900,  786,  219,    4,   75,    1]), tensor([ 5, 78, 23, 43,  1]), tensor([   3,   27, 1140,  256,   11,   37,  515,  315,   34,    3,  896,   11,\n",
      "          25,   35,  115,  145,   19,    5,    1]), tensor([  33,  111,    3,  313,    7,    2, 1028, 1202,  657,  133,    2,  527,\n",
      "           1]), tensor([   3,   84,    8,  198,   62,    5,  602,   50,    9,  713,  115, 1024,\n",
      "         117,    9,   14,  491,    1]), tensor([131,  10, 130,   1]), tensor([   2,  445,  681,    4, 1293,   45,   60,   17,    1]), tensor([ 54,  28,  27, 218, 779,  86, 218, 914, 293,   6, 163, 922,   2, 866,\n",
      "         15, 264, 792,  15, 148,  33, 171,  33,   1]), tensor([ 426,   72,    4,  114,   66,  194,   16, 1347,   14,  476,   13,    7]), tensor([  5, 904,  10, 152,  16,  10, 160,  89,   6,   7, 509, 104, 354, 986,\n",
      "        711,   4,   2,  41,  15,  36,   7, 559,   1]), tensor([ 76, 408, 539,  16, 317,  86,  44,   1]), tensor([  26, 1212,    6,  344,    1]), tensor([  2, 158, 413, 121,  14, 203,  23,  43,   1]), tensor([   3,   20,   23,  821,    9,   66,    8,   32,   90,    3, 1284,    5,\n",
      "          20,   60,  741,    1]), tensor([   3,   92,    8,   13,   21, 1144,   53,   10, 1157,    4,    3,   11,\n",
      "          85,  438,    3,  138,    1]), tensor([17, 13,  1]), tensor([ 60, 335,  19,   8,  29,  38, 155,   1]), tensor([858, 859,   1]), tensor([173,  13,   1]), tensor([   2,  644,  329,  666,    7,  607,    6,  101,    7,   23,  245,   16,\n",
      "          10, 1051,  705,    1]), tensor([  32,   30,   17,  367,   20,  332, 1047,    9,  281,  115,  744,    1]), tensor([  3,  11,  85,  60, 202,  71,   3,  27,  99,   7,  10,  91,  22, 105,\n",
      "         11,  25,  44,   1]), tensor([ 52,  36,   6,  31, 159,   4, 929,   9, 643,  12,   1]), tensor([449,  12,   1]), tensor([  3,  20,  18, 180, 171,   8,  29,   1]), tensor([   2,   13,   58,   64,  270,   17,  494,    4,  118, 1331,  696,    1]), tensor([   2,  199,    6,  373, 1136,  133,   97,  447,    1, 1010,    6, 1160,\n",
      "         921,   22,  425,   43,  382,  825,   15,  543,   10, 1158,  244, 1163,\n",
      "           1]), tensor([  3,  67, 188,   5, 112, 174,   6,   4,   5, 359,  11,  25, 278, 106,\n",
      "          9, 122,  19,   1]), tensor([ 23, 202,   1]), tensor([  3, 304,  84,   2, 160,  31, 159,   1]), tensor([ 88, 181,   8,  49,  69, 267,   4,   7,  23, 201,   9, 451,   1]), tensor([   2, 1042,  537,    5,    1,   14, 1236,   20,  775,    1]), tensor([ 74,  59,  18, 326, 334,   1]), tensor([   3,   82,    5,   16,   14, 1034,    4,  265,   35,   10,  145,   19,\n",
      "           2,   31,    1]), tensor([ 23, 202,  24, 589,   1]), tensor([ 213,   15,    2,   94, 1055,   27,  139,  340,   78, 1125,    1]), tensor([  88, 1154,    9, 1153,   86,  262,    1]), tensor([ 28, 254, 112, 311,   9, 486,   2, 128,   1,   8, 186, 943,   7, 750,\n",
      "        484,   4, 231, 706,  19,  94, 186, 757,   1]), tensor([   2,  298, 1021,   65,  812,   24, 1281,   34,    3,   98,   18, 1246,\n",
      "         243,   95,   24,   14,   40,    1]), tensor([  5, 990, 292,   9,   2,  13, 118, 100,   5,   7, 363,  10, 234, 428,\n",
      "        383,   2, 112,   1]), tensor([  5,  20,  97, 934, 144,   6,  34,   3,  61, 146,  27, 238,  51,  41,\n",
      "          1]), tensor([ 26, 501,   4,  59,  18, 123,  13,  69, 655,   1]), tensor([   2,  316,   22,  280,  138,   18,  326,    2, 1093,   24,    2,  594,\n",
      "           1]), tensor([   3,   80,   23,  180,   19,    2,  247,   22,  124,  138,   21,    2,\n",
      "        1259,   15,    8,   13,    1]), tensor([143,  51,  47,   2, 242, 497, 287,   1]), tensor([  3,  82,  95, 907,   3,  98, 123,  14,  81,  32, 121,  51,  34,  95,\n",
      "        551, 181,   5, 924,   9, 361,   1]), tensor([ 22,  33, 405,  11,  25,  44, 302,   1]), tensor([178,  10, 219, 100, 256, 165, 214, 574,  11,  25,   1]), tensor([  58,   11,   25,  351,  279,   34,   13, 1050,    9, 1215,    1]), tensor([   8,   29,    7,  691,   18, 1142,   16, 1120,   89,    6,    3,   73,\n",
      "          11,   25,  389,  141,  173,    5,  208,    6,   54,   28,   58,   11,\n",
      "          25,  549,   10,  151,    7,  702,   24,    5,   11,   37,  576,    1]), tensor([410,  72,   5,   1]), tensor([   2,   13,    7, 1260,    4, 1342,    1]), tensor([ 23, 107,  19,   8,  29,   1]), tensor([30, 17, 12,  1]), tensor([   2,   13,  738,  233,    4,   99,    3,   27,    9,   66,    5, 1161,\n",
      "           1]), tensor([  28,  574,   11,   25, 1155,    5,   12]), tensor([17, 29,  1]), tensor([   3,  140,   48,    8, 1029,  259,  675,   13,    1]), tensor([  3,  35, 487,  10, 124, 296, 198,   6,  92,  10,  23,  43, 844,   4,\n",
      "        132,  29,   1]), tensor([   2,   31, 1182,  235,  339,    1]), tensor([  3,  60,  48,   8, 832, 266,   5, 208,  23,  76,   6, 804,   4, 173,\n",
      "          1]), tensor([   3,  188,  639,   70,   10,  434, 1124,   28,   58,  204,   21,    2,\n",
      "         246,   57,    9, 1304,  296,    9,    2,   13,    1,    2,   68,   15,\n",
      "           2,  198,   20,   52,    1]), tensor([356,  62,   3,  11,  85, 881, 113,   1]), tensor([17, 13, 12,  1]), tensor([ 76, 298,   4,  41,   1]), tensor([  52, 1248,  573,   32,    1]), tensor([1290, 1208,    4,   41,   29,    1]), tensor([ 103,   15,   71,    7,    2, 1179,  239,    6,   23,  443,    1]), tensor([  54,    2,  112,  571, 1200,  171,   10, 1014,  581,  863,    3,  268,\n",
      "           9,  478,  819,  538,    4,  873,   36,   53,    2,   32,    1]), tensor([  2,  36,  41,   7,  26,   4, 867,   7, 226,   1]), tensor([84,  8, 13,  1]), tensor([ 243, 1312,   19,  149,    1]), tensor([   2,   91,   78,   16,   79,   10,  274,    4,  163,  233, 1252,  288,\n",
      "          14,   13,    1]), tensor([ 71,  24,  71,   6,   3,  11, 137, 238,  10,  51, 400, 422,  53, 124,\n",
      "          1]), tensor([364, 139,   1]), tensor([   3, 1104,    5,   24,   67,    9,  204,   87,   18,   10,  745,  111,\n",
      "          78,    1]), tensor([1327,  294,    1,    1,   36,   41,    7,  147,    1]), tensor([  77, 1206,    6,   77,  652,    6,   77, 1089,    6,   18,  118,   10,\n",
      "        1188,    6,  330,   12,    3,  136,   11,   25,   65,   75,  312,   19,\n",
      "          14,   94,   33,   16,    2,  785,    1]), tensor([ 166,   41,    6,  219, 1255,    6,  236,  172,    1]), tensor([  28,   66,  131,   28, 1085,   16,    3,  890,    1]), tensor([   3,  398,   14,  223,   11,   37,   81,    6,   70,  124,  910,   57,\n",
      "           9,   14,   13,    4,    5,   78,   56,   10,  289, 1349,    2,   13,\n",
      "          20,   24,   14,  499,   86,    2,   49,    1]), tensor([ 3, 98, 18, 48, 95, 75,  1]), tensor([  5,   7,  43, 181,   6, 106,   9, 588,   2,  13,   4,  46,  10, 440,\n",
      "          6, 761, 230, 694,   1]), tensor([   5,  240, 1107,  716,    6,    2, 1013,   45,  201,    9, 1043,    6,\n",
      "           4,    2,  150,   45,   38, 1150,   22,    5,    7,  201,    9,  505,\n",
      "         148,    1]), tensor([  14, 1216,   46,   33,   64,    4,  265,  997,    5,    1]), tensor([ 52,  29,   6,   3,  80,  23, 345,  19,   2, 260,   1]), tensor([ 30,  56,  10, 289,   5,  44,   2, 162,  39,   2,  33,   3,  92,  19,\n",
      "          2,  13,   1]), tensor([246,   7, 822, 350,   1]), tensor([ 18,  67, 138,   2, 186,  83,  10,  17, 247,  15,   8,   6,   3,  98,\n",
      "         64, 123,  14, 489, 516, 433,  14, 824, 673, 192,  97, 246, 292,   1]), tensor([ 100,    5, 1058,    6,    2,   31,  292,    7,  387,    4,    2,  117,\n",
      "           7,  562,  161,    1]), tensor([ 526,  222,   23, 1231,    1]), tensor([  8,   7,  38, 805,   4,  64,  14, 203, 915,  54,   3, 272,   9, 505,\n",
      "          2,  40, 126, 206,  14,  40,   1]), tensor([320, 811,   7, 226,  69,  12]), tensor([  8,   7,  10, 632,  13,   1]), tensor([ 17,  29,   6, 308, 347,  12,   1]), tensor([ 8,  7, 10, 17, 13, 12,  1]), tensor([  3,  20,  23, 180,  19,   2,  68,  15,   2, 287,   1]), tensor([420, 131,   3, 358,   1]), tensor([   3,  343,   14, 1269,   94,  124,    4,  615,  110,   20,   18,   10,\n",
      "         326,  383,    2,   13,    4,    2,   91,    1]), tensor([586,  17,   1]), tensor([  8,   7,   2, 120,  13,   3,  11,  96,  35,  22,  46, 102,  38, 683,\n",
      "        181,   1]), tensor([  60, 1316,    1]), tensor([  76, 1233,  958,    1]), tensor([   3,  286,  184,   53,  165,   90,  688,   46, 1048,  135,  214,    1,\n",
      "           1,    1,    1,    2,  120,  111,    3,  479,   20,    2,   60,  134,\n",
      "         129,    1]), tensor([78, 17, 12,  1]), tensor([255,  92,   5,  12,  12,  12,  12,  12]), tensor([   2,   13,  985, 1266,   12]), tensor([481, 110,   7,  10, 145,  19,   2, 596,   1]), tensor([  5, 405,  11,  25,  44,   6, 125,  58,  18, 122,  50, 100,   3, 271,\n",
      "          1]), tensor([  5,  64,  35,  10,  94, 145,   1]), tensor([ 59,  18, 152,   2, 689,  70, 623,  57,  13,   1]), tensor([  8,   7, 937,   1]), tensor([106,   9,  42,   1]), tensor([ 695,    8,    9,   55,  230,  114,  754,  123,   28,  427,   56,  676,\n",
      "          11,   37, 1324,    1]), tensor([410, 130,  55,  93,   1,   1,   1]), tensor([441,  20,  23, 180, 100, 241,  53,   2, 331,  31,   9,   2, 828,  31,\n",
      "          1]), tensor([189,  71, 300,   6, 249,  86, 127,   6,   8, 111, 114,  18, 269,  21,\n",
      "         14,  40,   1]), tensor([  38,    3,  380, 1286,   14,   93,   87,    2, 1353,   16,  330,    1]), tensor([  3,  67, 122, 872,  16, 225,   1]), tensor([ 76,  32, 259, 127,   1]), tensor([1175, 1053,  184,  152,  347,    1]), tensor([   2,  251,   15,    5,    7,   23, 1204,    4,    2,  128,    7,   76,\n",
      "           4,  153,    6,   19,   17,  886,    1]), tensor([519,  55,  93,   1,   1,   1,   1,   3,  11,  96,  35,   8,  74,  16,\n",
      "        212,  99,   1]), tensor([ 52, 787,  32,   1]), tensor([ 10, 215,  26,  29,   1]), tensor([   5,   11,   37,   23,  722,    4,  185,    9,   42,   62,  178,  247,\n",
      "         409,  367,  182,    2,  116, 1171,   38,  143, 1230,    1]), tensor([ 941, 1189,   19,    2,  124,  342,   13,    1]), tensor([237, 111,  21,  13,  44, 257,   6, 265,  56,   5,   1]), tensor([   5,   67, 1152,    2,   13,   39,  142,  540,  117,    1]), tensor([   3,   61,   48,  504,    2,  158,  953, 1201,  101,   30,  429,   19,\n",
      "          14, 1035,  506,    6,  179,  840,    1]), tensor([ 97, 226,  94, 251,  16, 425,  12,   1]), tensor([ 30, 119,   1]), tensor([  2,  67,  26, 111,  20,  22,   5, 156, 290,  21, 267, 203,   1]), tensor([   4,  213,   15,    2, 1298,    7,  587,    1]), tensor([   3,  338,    2,  967, 1064,   21,    5,   70,  567,    1,  869,   57,\n",
      "           6,    4,    5,   99,  979,    9,  350,    9,   10,  739,    4,  465,\n",
      "         113,  237,  485,   24,   10,  191,    1]), tensor([ 51,  47,  28,  11, 137, 421,   1]), tensor([  38,    3,   82,   79,   15,   95,    4, 1184,  605,   15,   93,    1]), tensor([  3,  82,   8,  31,  19,  10, 731,  53, 167,   4,   3,  11,  85,  23,\n",
      "        107,  19,  14, 260,   1]), tensor([   3,  820,    2,  531,  813, 1369,   16,    8,    4,    3,   11,   85,\n",
      "         215,  107,   19,   22,  751,    1]), tensor([393, 853, 583,  12,   1]), tensor([ 63, 560, 253, 253, 439, 870,   8,   7,   2,  33,  22, 309,  30,  43,\n",
      "          1]), tensor([ 108, 1141,   21,    2, 1243,  603,  182,   28,  520, 1367,    1]), tensor([769,   1]), tensor([ 17, 414,   1]), tensor([   2,  413,  680,    2,   36,  767,  206,   55,   40,    4, 1194,    9,\n",
      "         930,    2,   36,  149,    4,  232,    1]), tensor([  1, 160, 457,  31,  70,  28,  73,  11,  25,  27,   9, 510,   5,  39,\n",
      "        860,  39, 220,  15,   2, 431, 214,  57,   1]), tensor([140,  48,  16, 115,  33, 572,  46,  10, 385, 558,  13,   1]), tensor([  3,  27,  62, 227,  21,  14, 135,  13, 100,   3,  80, 317,   6,  34,\n",
      "         28, 388,  18, 122, 279,   1]), tensor([ 2, 36, 41,  7, 52, 39, 43,  1]), tensor([185,   6, 462,   4,  17, 121,   1]), tensor([  54,   28,   56,   10,  210,  661,    9, 1073,   71,   55,  402,    6,\n",
      "         163,    8,   13,    7,   16,   28,   12]), tensor([   3,   80,  241,    9,   27,    9,   65,    2,  120,    9, 1046, 1170,\n",
      "           8,   29,    1]), tensor([   2,  704,   45,    2,  183, 1026,    1]), tensor([  2,  67, 111,  22,   3, 552,  98, 926,   7,   2,  36, 970,  87,  53,\n",
      "          2,  32,   1]), tensor([76, 36,  1]), tensor([  5,  20,  10, 130,  15,  14,  93,   1]), tensor([52, 36, 41,  1]), tensor([  5, 842,  14, 135,  13, 127, 377, 100,   3, 809,   2, 116,   1]), tensor([1279,   28,   16,  543,   17,  129,    1]), tensor([ 14, 837,  46,   2, 567,   6,   4,   2,  31,   7, 791,   1]), tensor([   2,  103,  803,   15,    2,  626,  851, 1307,    1]), tensor([ 73,  11,  25, 386,  62, 179,   9,   2, 351,   1]), tensor([   2,   40,  654,   67, 1097, 1040,   24,   33,   40,    1]), tensor([1332,  573,  517,    4,   38,   59,    8,  392,  325,   13,   12]), tensor([ 17,  16, 945,  69,   1]), tensor([  8,   7,  10,  17,  29,   1,   1,   1,   1,   1, 545, 631, 189,  55,\n",
      "        843,  12,   1]), tensor([  3,  92,   5,  90,   5,  20,  38, 267,   4, 597,   1]), tensor([  5,  11,  37,  18, 131,   5, 521,   5,   7,   1]), tensor([ 34,   5,  59,  66,  51, 109,   4, 232,  47, 115,  13,   3,  11,  96,\n",
      "         35, 228,   1]), tensor([   3,  322,   22,  533,  868,    6,   34,    9,   50,    5, 1195,   56,\n",
      "        1220, 1275,   22,  577,   11,   25,   44,   43,    1,   43,    6,    8,\n",
      "          33,   30,   17,    1]), tensor([84,  8, 29,  1]), tensor([   8,   29,    7,   17,    1,    1,    1,    5,  182,  132,   10,  209,\n",
      "         799,    3,   58,  179,    9,    2,  724,  191,  568,   21,  444,   16,\n",
      "        1235,    1]), tensor([ 59,  18, 121,   1]), tensor([130,  15, 653,   1]), tensor([ 78, 257,  12]), tensor([103,  81,  21,   2, 470,   1]), tensor([ 355,    2,  368,    9,  370,  322,   28,   45, 1147,   10,  151,    7,\n",
      "          10,  341,  245,  239,    4,    8,   13,    7, 1094,   24,   22, 1164,\n",
      "           1]), tensor([  5,   7,  23, 104,  21,   2,  40,   1]), tensor([  3,  11,  85, 514, 148,   1]), tensor([  2,  36,   7, 153,   4,   2, 125,   3, 271,   9,  21,   5,  45, 374,\n",
      "        133,   2,  41,  69,   1]), tensor([  2, 347,  89,  20,  64,  23, 507,  12]), tensor([980,   9,  14,  13, 192, 802,   1]), tensor([380,   2, 129,  20,  23, 134,   1]), tensor([  3,  92,   8,  13, 196,   2, 303,  15, 471,   4,   3,  11,  85, 233,\n",
      "        564,  19,   5,   1]), tensor([  17,  186,   16, 1037,    1]), tensor([   2,  637,  709,    3,   27,    7,    6,    2,   31,  782, 1268,    1]), tensor([   3,  118,  236,    8,   13,  206,   10, 1254,    4,    5,   20, 1261,\n",
      "          16,  522,    4,    5,  146,   30,   17,   12]), tensor([  3,  27,  67,  35,   5,  16,  10, 176, 570,   6,  34,  38, 155,   6,\n",
      "         38,  26,   1]), tensor([  5, 278, 743,  87,   4, 182,  10, 283, 283, 283,  36, 163, 521, 219,\n",
      "        306,   1]), tensor([  3,  20, 468,  16,   8,  32,  16,  10, 160,  89,   4,  99,  22,   3,\n",
      "         11,  96,  92,   5,   3, 136,  11,  25,  65, 312,   1]), tensor([ 73,  11,  25, 130,  55,  93,  12,   1]), tensor([ 572,   24,  221,  127, 1022,    7,  884,   72,    8,   31,  224,    1]), tensor([   3,   20, 1217,   24,   14, 1329,    6,   19,    2,  737,   21,   14,\n",
      "         230,    6,    4,    2,   32,  469,  219,    1]), tensor([ 461, 1344,    6,    3,  894,  478,    5,    7,  110,    1]), tensor([ 28,  58,  18, 611, 172,  19,   2, 164,   6, 255,  78, 485,  12]), tensor([ 157,    6,    2,   40, 1078,  396,  161,  301,    4,   63,   67,   33,\n",
      "         274,    3,  469,   33,    1]), tensor([962, 278,  26,   1]), tensor([585, 207,   1]), tensor([1166,   14,   13,    9,   56,   94,  492,    1]), tensor([  58,   11,   25, 1325,  516,   53,   10,  553, 1083,    1]), tensor([154,  78,  21,   2, 120, 272,   1,   2, 117,  20, 678, 807,  24,  10,\n",
      "        692, 190,   4,   2, 399, 240,  26,   1]), tensor([ 20,  18, 107,   1]), tensor([ 157,    6,   14,  879,   20,  707,   22,  220,   89,    2,   13,  105,\n",
      "          11,   25, 1337,  113,   56, 1049,   13,   59,    1]), tensor([  63,  212,    6,  128,  108,  222,  284,   71,   15,   10, 1263,    1]), tensor([178,   2, 247, 409,   1]), tensor([ 88,  44,  79, 570, 163, 285,   1]), tensor([ 18, 210, 175,   4, 105,  11,  25, 561,  21,  56,   5, 348,   1]), tensor([221, 328, 734,  24, 991, 609,   7, 205,   1]), tensor([34,  5,  7, 17,  6,  3, 61, 60, 48,  5]), tensor([  3,  20,  18, 107,  19,   8,  74,   1]), tensor([   3,   27,   35, 1294,  129,   16,   86,  193,   99,    6,    4,    3,\n",
      "          80,  215,  107,   19,    5,    1]), tensor([ 146,  568,    1,    1,    1,    1,    1,    1,    3,   11,   85,  545,\n",
      "           8,   74,   61,   44,   43,    1,    1,   54,    3,  139, 1151,    5,\n",
      "          12]), tensor([ 23, 776, 492,   1]), tensor([  3,  80,  64,  23, 107,  19,   2,  68,   1]), tensor([   5,  693,  206,  258,   24,   10,  190,   22,  182,   28, 1361,  141,\n",
      "         160,   22, 1009,   61,  248,    1]), tensor([   3,   27,   10,  165,  460,   13,    4,   88,   44,   43, 1296,    6,\n",
      "          26,  109,    4,  261,   22,  817,  428,  463,   15, 1213,    1]), tensor([ 18, 131,   3, 358,   1]), tensor([1288,  113,    9,    8,  525]), tensor([30, 17,  1]), tensor([ 267,    6, 1222,    6,  447,  468,    6, 1114,  526,   19,  608,  540,\n",
      "          24,  258,    1]), tensor([1250,  153,   15,    8,   29,    4,  179,   19,    2,  877,  333, 1162,\n",
      "        1091,    6,  101,  396,   24,   10, 1285,    1]), tensor([  2,  49,   7,  17,   4,  30, 119,  19,   2,   1]), tensor([1267,   13,   21,   10,   17,  328,    1]), tensor([ 575,  237, 1090,    1]), tensor([105,  11,  25,  44,   1]), tensor([208,  26,  24,   2, 334,   6,  34,   8,  49,  20,  10, 911, 406,  12,\n",
      "         12]), tensor([  2, 199,  21,   2,  13, 471,  65, 188,  39,  10, 790, 100, 932,   1,\n",
      "          1,   1,   3,  11, 137, 341,  65, 189,  10, 774, 163,   8,   1]), tensor([ 54,   3, 270,  10, 334,   6,   2,  31, 299,  10, 630,   6,   4, 536,\n",
      "        634,   6, 975,  50, 322, 142, 765,   1]), tensor([   3,   11,   96,   82,  366, 1356,  314,   22,   36,   51,   47,   95,\n",
      "           1]), tensor([144,  15, 207,   1]), tensor([1282,  214,   45,  132,  108,  119,   99,    1]), tensor([  5,  11,  37,  10, 144,  15, 294,  12]), tensor([  82, 1000,   16,    2,   91,    6,  101,  170, 1238,   63,  504,    1])]\n",
      "[tensor([ 38, 155,  38,  26,  12,   1]), tensor([   5,  138,   18,   44,   24,   14,  135,   13,  126,    3,   80,   23,\n",
      "         113, 1202,   19,    2,   91,   12,    1]), tensor([  2,   0,   7, 146, 456,  24,   0,   1]), tensor([336,  41,   4, 129,   1]), tensor([   2,  439, 1082,   30,  119,    6,   34,  163,    2,  116, 1311,    9,\n",
      "           0,    2,    0,    0,    6,    4,    2,    0,    0,    1, 1069,    6,\n",
      "           3,  146,    0,    8,  117,  244,    1]), tensor([  5,  11,  37, 215, 106,   1]), tensor([  2, 513, 764,  24,  10, 176, 570,   1]), tensor([  2, 958,   7,  10,  76,   0, 383,  10, 864,   0,   4,   2,   0, 135,\n",
      "         13,   0,   0,   1]), tensor([336,  29,   1]), tensor([1069,    6,    3,   61,   48,    8,   13,  184,    2,   94,    0,    1]), tensor([73, 11, 25, 72,  8, 29,  1]), tensor([ 99,   3, 322,  22,   3, 181,  10,   0, 751,   1]), tensor([142,   2, 103,  32,   3,  27, 188,   1]), tensor([336,  41,   1]), tensor([ 552,    5,  184,  100,   28,  495,    9,  489,    8,   33,   12,    8,\n",
      "         545,    7,    2,  248, 1035,   13,   16,   50,   12]), tensor([106,   9,   0,  19,  14, 344, 135,   1]), tensor([ 18, 175, 149,   1]), tensor([   3,  343,   14,    0,   43,  363,    2,  347,    0,    6,  154,   20,\n",
      "          24,   26,  132,    0,    4, 1069,    6,    3,   80,   23,  821,    9,\n",
      "          27,    8,    0,    1]), tensor([  0,  59,  18,   0, 175, 288,   0,   1]), tensor([  5,  46,  71,   2, 839,   3, 569]), tensor([525,   0, 339,   4, 143, 682,  47,   2,   0,   1]), tensor([  8,  74,   7, 426,   4,  30, 257,  12]), tensor([  8,   7,  97,  52,   0,   6, 304, 100,   0,  19,  55,  13,  11,  37,\n",
      "          0,   1]), tensor([1279,   28,   16,    0,   14,   93,    1]), tensor([  3, 598, 816,  83,  18,  65,   0,  12]), tensor([ 28, 255, 322,  54,  28,   0,   5, 242, 175,  86,   2, 127,   0,  15,\n",
      "        555,  16,   2,   0,  28, 569,  86,  18,   1]), tensor([  8,  49, 524,  43, 181,   1]), tensor([52, 29,  1]), tensor([ 10, 209,  15,   0,  27, 102,   0,   8,  10,  23,  26,  13,   4,  38,\n",
      "         83,   3,   1]), tensor([   0,   13,    0,   45,   10, 1041,   27,    1]), tensor([23,  0, 19, 22,  1]), tensor([   2,  958,    7,   60, 1366,   24,    0,    4,    7, 1260,  175,    3,\n",
      "          73,   11,   25,  421,  115,  216,    1]), tensor([  3,  80,  23, 180,  19,   8,  32,  53, 496,   1]), tensor([ 5, 20, 10, 17, 13,  1]), tensor([52, 12,  1]), tensor([ 95,  45, 678,  23, 104,   4, 867,   7, 403,   1]), tensor([   3,   11,   96, 1074,    8,   13,   16,  212,   99,    4,   58,  520,\n",
      "          22,    5,   11,   37,    2,  103,    0,   13,    3,   11,   96,   35,\n",
      "           1]), tensor([   8,    7,    2,   13,    9,   66,   16,    1,    1,    1,    1,    3,\n",
      "         108,   82,   14,    0,    4,   71,    3,   58,  520,    7, 1367,   12]), tensor([  0, 117,  30,  17,  12,  12,   1]), tensor([ 23, 143, 202,  19,   8, 291,   1]), tensor([ 5,  0, 20, 18, 39, 26, 39, 14, 37,  1]), tensor([  3, 487,   8,  16, 531,   0,   0,  34,   3, 552,   5,  67,  78, 485,\n",
      "         70,   0, 100,   3, 120, 188,   5,  57,   1]), tensor([336, 271,  89, 492,   1]), tensor([  3, 231, 123, 172, 133,   0,   0,   1]), tensor([  18,   10,   26,   74,    1,    1,    5,   78,   16,   10,  191,  163,\n",
      "         268,  243,  216,   24,   14,    0,    0,    0, 1098,    1]), tensor([208,  17,   4,   7, 541,   1]), tensor([ 632,    0, 1283,    1]), tensor([  5, 208,  23,  76,   1]), tensor([ 38, 108, 636,   1]), tensor([  2,  67,  23, 769, 111,  20, 110,  20,  77,   0,  12,  12,  12,  12]), tensor([ 18, 180,   1]), tensor([104,  24,  14, 891,   1]), tensor([  34,    6,   24,  115,   49,    6,    2,  103, 1082,    7,    6,   28,\n",
      "          58,    0,   95,  494,    9,   55,    0,  189,    0,    6,   86,  118,\n",
      "           0,  494,   53,   55,    0,    9,    2,   13,    1]), tensor([  3,  61,  27, 880,  77, 535,  54,   3,  20, 276,   1]), tensor([   3,   11,   85,  560,    9,  263,    5,   16,   10, 1153,    1]), tensor([355,   5, 138,  18,  44,   1]), tensor([  0,   9,   8,   0,  21, 237, 151,   3,  20,   0,   9,   0, 189,   8,\n",
      "         32,   1]), tensor([   8,  291,  152,   50,   10,    0,    0,    4,  146,   18,  880,   50,\n",
      "          14, 1153,  168,    1]), tensor([  8,   7, 814,  10,   0,   0,  22,  59,  18,   0,   1]), tensor([  3,  58, 122, 191,   3,  11,  85,   0,  24,   2, 116,   6,   4,   0,\n",
      "         73,  11,  25, 118,  27,   9, 338,   5,  21,   5,  11,  37,   0,   0,\n",
      "          1]), tensor([  3,  35, 586,  77, 145,  19,   8,  32,   0,   9,  14, 640,   0,  12]), tensor([ 16,   2,  68,   8,  20,  10,  17, 749,   1]), tensor([202,   1]), tensor([163,  10, 176, 174, 324,   2,  10,   0,  15,   0, 286,  87,  15,   2,\n",
      "         13, 191,  24,  42,   1]), tensor([   8,   13,    7, 1226,    4,  461,    4,    2,  407,    7,  632,    1]), tensor([   3,    0,   73,   11,   25,   56,    0,   34,    8,   33,    7,   23,\n",
      "         462,  367,  105,   11,   25, 1015,  113,   14,    0,    1]), tensor([  69,  134,   28,   27,    9, 1085,  113,    9,    0,   10, 1031,   16,\n",
      "           2,  129,   12]), tensor([  2,  68,  20,  23,  26,   4,  19,   2, 434, 347,   4,  71,   5,  20,\n",
      "         10,  26, 260,   1]), tensor([ 73,  11,  25, 123,   2, 162, 327,   3, 138,   1]), tensor([  2, 497,   0,  60, 106,  21,   8, 694,   1]), tensor([  5,   7, 461,   6,  46,   0,  15,  31,   0,   6,   4,   7,  23,   0,\n",
      "          9, 361,  16,   0, 828,   0,  15,  89,   1]), tensor([ 31,   7, 903, 113,  43,   1]), tensor([  3,  80,   0,   3, 181,   8, 260,   1]), tensor([  2, 120, 111,  22,   0,  20,  22,   2,   0,  20, 161,   1]), tensor([ 57, 526, 136,  11,  25,  27, 102,   0,   1]), tensor([   8,   13,    7,  215, 1260,    4,    3,   11,   96,  255,   35,  115,\n",
      "         964,  216,   19,    5,    1]), tensor([225,  41,   7, 336,   6,  23, 336,   1]), tensor([ 23, 335,  19,   8,  32,   1]), tensor([  5,   7, 106,   9, 561,  21,   4, 161, 100,  28,  45,  24,   2, 116,\n",
      "          4,   2, 149,   0,  45, 340,   0,   1]), tensor([  3,  11,  96,  35,   8,  16,   0, 193,   4,   5,  46,  78,  17,  16,\n",
      "         50,   1]), tensor([ 271,   79, 1326,  295,  129,    1]), tensor([  29,    7, 1326,    6,  266,    5,   59,   18,   27,  175,  288,    0,\n",
      "           9,  152,    2,    0,    3,   20,    0,    9,   42,    5,   19,    1]), tensor([108, 131,   3, 358,   1]), tensor([  0,   9, 520,   6,   3,   0,  14,  93,   1]), tensor([  2, 109, 554,   8,  32,   7,  52,   1]), tensor([  5,  46, 956, 113,  23,  43,   1]), tensor([ 14,   0, 367,   3,  99,  66,  17, 109,   1,  10, 325, 826,   6,  34,\n",
      "        492,   7,  17,   1]), tensor([   3,  177,    2,   29,    9,   65,  106,    9, 1202,  113,    4,   42,\n",
      "           1]), tensor([  3,  84,  71,   2, 839,   4, 433,   0,   1]), tensor([26, 68,  1]), tensor([269, 377,  53,   8, 351,   6,  65,   0,   1]), tensor([  8,   0,  71,   2, 216,   1]), tensor([498,   6,   3,   0,  83,  18,   0,   5,   7, 575, 142,   0,  68,   0,\n",
      "          1]), tensor([  0,  46,   2, 103, 295, 129,   0,   1]), tensor([ 156,  290,    6,  286,   19,  353, 1219,   15,  793, 1293,    1]), tensor([30, 17,  1]), tensor([  5, 105,  11,  25,  44,  24,   0,  86,   0,   1]), tensor([  8,  33,  30,   4,  20, 259, 127,   1]), tensor([ 63,  10, 275,   2,  31, 222, 233,   0,  21,  14,  32,   1]), tensor([  3,  20, 907,  16,  75,   1]), tensor([  5,  11,  37,  10,  17,   0,  16,   0,   6,   0,   6,   4, 296,   0,\n",
      "          1, 482,   6,  65, 545,   9,  42,   0,   1,  16,   0, 296,   0,   0,\n",
      "         12]), tensor([   0,  378,    0,    0,    6, 1291,    0,   36,    4,    0,    0,   16,\n",
      "        1282,   21,    2,  256,  303,   15,    2,  151,    1]), tensor([   5,   20,   22,  210,    1,  438,    9,  520,   22,    2,  496,    0,\n",
      "          10,    0,  292,    9,   14,  135,    4,   19,   77,  538,  789, 1049,\n",
      "          42,    1]), tensor([  3,  84,   2, 199,   6,   5,  11,  37,  60, 215,  26,  41,   1]), tensor([  3, 405,  11,  25, 569,   2, 694, 241, 184,   2,   0,  15,  14,  40,\n",
      "          6,   0,   0,   1]), tensor([163,   3, 820,  16,   2, 162,  13,   6, 118,  22,  35,   2, 162, 145,\n",
      "          1]), tensor([142, 109,   7,  23,  23, 336,   1]), tensor([131,  10, 130,  15,  89,  12]), tensor([  26,  357,    6,   30,  119,   62, 1113,    0,    0,    6,  116,    6,\n",
      "          86, 1339, 1067,    1]), tensor([  8,   0,  15, 314,   7,   2, 364,  22,   3,  27, 139,  35,   0,   1]), tensor([  3,  17, 109,  71,   2,  89,   1]), tensor([   5,    0,   69,  461,    4, 1291,    1,    1]), tensor([  8, 325, 117,  46,   0,  14,   0,   0,   4, 181,  14, 159,  10,   0,\n",
      "        209, 799,   1]), tensor([202,  12,   1]), tensor([ 26,  74,   6, 252,  68,   1]), tensor([298,   0, 224,   1]), tensor([134,  41,   1]), tensor([  95,    0,  733,  113,    2,  245,  461,    0,  584,    2,   40, 1067,\n",
      "           1]), tensor([  5,   7, 392,   6,   4,   5, 427,   4, 251, 108,  39, 392,   1]), tensor([ 23, 202,   1]), tensor([  3,  60,  56,   8,  29, 184,   2, 124,  90,   5,   7,   0,   0,  21,\n",
      "          2,  40, 144,   4,   2, 472,   1]), tensor([134, 394,   1]), tensor([  5, 156,  14,  40,  43,   4,   7, 104,  21,   1]), tensor([701,  16,   0,   0, 307,   1]), tensor([ 83,  18, 260,   8,  13,   1]), tensor([131,  10, 406]), tensor([  0,  23, 335,  19,  14, 260,   1]), tensor([  3,  27,   8,  13,   4,   5,   7,  10,   0,  24,  14, 527,   6,   3,\n",
      "         60,   0,   5,   1]), tensor([  5, 182,  23,   0,   0,   0, 228,   5,   0,   2, 151,   1]), tensor([  3, 188,   9, 271,  21,   5,  16,  62, 582, 318,   4,   2,  31,  61,\n",
      "         65, 983,   0,   4, 791,   1]), tensor([52,  0,  0, 13, 49,  1]), tensor([  3,  27, 188, 218,  13,  24, 112, 193,   6,  34,   8,  33,   7,   2,\n",
      "        103,   1]), tensor([  0,   1,   1,   1,   0, 235,   2, 781,  90,  15,  10, 360,   0,  12]), tensor([ 24,   0,   5, 240,   0, 208,  39,  54,   2,  13,   7,  71, 462, 392,\n",
      "        497,   1]), tensor([ 34, 758,  95, 176,   0,   6,   8,  49,   7,  15,   0,  41,   4,  43,\n",
      "        575,   2,   0, 727,  15,   0,  97,   0,   0,  29,   1]), tensor([   2,  172,  411,    6,    2,   13,  700,   21,    4,  161,  133,  114,\n",
      "           6,    2,  128,    0,  284,    4,    2,  364,   15,   71,    5,    0,\n",
      "        1174,    0,    1]), tensor([103,  32, 139,  12,  12,  12,   1]), tensor([  2,   0,  32,  20, 543,  10,   0,   1]), tensor([157,   6,   0, 442,  45,   0,  18,  26,  16, 217,  89, 871,  56,   0,\n",
      "          0, 266,   2, 225,   0,   0,  50, 113,   1]), tensor([52, 29, 16,  2, 68,  1]), tensor([  3,  73,  11,  25, 552,   5,  61, 444,   5,  69,   0,  21,  55, 230,\n",
      "          1]), tensor([ 63,   2, 120, 152, 956, 241,   0,  63, 211,   1]), tensor([  0,  26, 501,   4, 208,   0,   6,  69,   1]), tensor([136,  11,  25, 840,   5,  87]), tensor([  3,  11,  85, 146,   0,  19,   8,  13,   1]), tensor([23,  0,  1]), tensor([  38,    3,   35,    9,  270,    2,   31,   87,   15,    2,   13,  338,\n",
      "           5,   71,  168, 1296,    4,  163,    0,    5,    1]), tensor([385,   0,   7, 106,   9,  42,   1]), tensor([ 133,  120,    3, 1284,    3,   20,    0,   10,   26,  749,  133,  366,\n",
      "           1,    6,    0,    3, 1104,    5,  206,   14,   13,   70,    0,  342,\n",
      "          57,    1]), tensor([  0,   6,   0, 144,  15, 207,   1]), tensor([ 71,   2, 256, 287,   3,  11,  96, 187,   0, 425,   0,  63,  10, 176,\n",
      "        212,  34,   8,  33, 524,   9,  65,  24,  16,   2, 160,   0,   1]), tensor([26, 41,  1]), tensor([61, 18,  0,  1]), tensor([336,  36,  41,   1]), tensor([793, 144,   0, 301,   1]), tensor([  39,   97,    0, 1170,    0,    6,  126,   24,    8,   91,    4,  330,\n",
      "           0,    1]), tensor([ 5, 11, 37, 10, 17, 74,  1]), tensor([ 3, 61, 48,  5,  1]), tensor([   3, 1021,   27,  885,   10,    0,    6,   34,    3,   61,   18,    0,\n",
      "         659,    5,  194,   90,   15,    2,    0,   41,  603,    1]), tensor([ 30,  39, 756,   1]), tensor([ 142,   10,  325,    0,   34,    3,  552,    0,  142,    0,   21,    0,\n",
      "           4,    5,  517,    4,    0,    0,  127,  235,    9,  142,   31,    0,\n",
      "        1222,    0,  458,   49,    1]), tensor([153,   0,  36,   1]), tensor([ 73,  11,  25, 130,  55,   0,  21,   8,  33,   1]), tensor([108,  60,  26,   1,   1,  38, 155,   6, 500,   2, 103,   0,  32,   3,\n",
      "         27, 139,  35,   1]), tensor([1245,    0,   16,    8,  129,    1]), tensor([  10, 1041,    0,   16,  195,    0,   24,    2,  364,    0,   15,    0,\n",
      "         298,    1]), tensor([  2, 364,  13,   3,  11,  96, 139,  35,   1,   1,   1,   1,  67,  35,\n",
      "          5,  16,  10, 176, 212,   1]), tensor([ 28,  98,  67, 270,   0, 133,  10,  89,   4,   2,  41,  20,  23, 336,\n",
      "          1]), tensor([147,   1,   1,  14, 116, 114,  18,   0,   8,   0,   1]), tensor([ 17,  36,   4, 129,   1]), tensor([  8, 602,   2,   0,  15,   0,   0,  16,   2, 162, 746,   4,  89,  63,\n",
      "          2, 120,   1]), tensor([ 81,  59,  18,  44,   6,  13,   0, 113,   6,   0, 108,   0, 113,   4,\n",
      "         99,   5, 108, 182, 172,   0, 191,  24,  14, 499,   0,   1]), tensor([   8,    7,    0,   10, 1041,   27,   54,   55,    0,   59,   18,    0,\n",
      "         135,   13,    0,  191,    0,    1]), tensor([ 100,    3, 1095,   14,    0,  206,    2,   49,    6,   18,   67,   20,\n",
      "           5,   18,    0,    6,   34,  110,   20,   10,  209,   15,  423, 1178,\n",
      "          21,    2,    0,    1]), tensor([  3,  11, 464,  65, 468,  16,  10,  94, 414,   1]), tensor([154,  79,   8,  29,   7, 578,   1, 120]), tensor([   0,    9,  370,    0,    2,  331,   31,   53,    0,   22,  286,   19,\n",
      "          14, 1330,    1]), tensor([  2,  13, 178, 424,   0,  12]), tensor([ 52, 311, 434,   0,   1]), tensor([  54,   28,   56,   10,  210,  661,    9, 1073,   71,   55,  402,    6,\n",
      "         163,    8,   13,    7,   16,   28,   12]), tensor([   8,   13, 1021,   43,   65,    2,  364,    3,   11,   96,  139,   35,\n",
      "          24,  115,  646,    1]), tensor([  3, 138,  18,  27, 115, 145,  19,   8,  74,   4,  61,   0,   5, 194,\n",
      "         54, 474,   1]), tensor([120,  15,  71,   6,   5, 105,  11,  25, 361,  43,   1]), tensor([ 0,  2, 13, 21, 71,  0,  1]), tensor([0, 0, 1]), tensor([110,  11,  37,  10, 205,   0,  36,  24,   2,   0,  21,  71,  14, 172,\n",
      "         22,   3,  27, 255, 827, 228,   1]), tensor([  5, 405,  11,  25, 152,  16,  50,   1]), tensor([ 31,  46,  77, 159,   1]), tensor([406,   1,   1,   3, 313, 279,  22,   0,  24,  14,  40,   1]), tensor([ 26, 225,  41,   1]), tensor([   2,   36,   41,   16,    2,  117,    7, 1317,    1, 1323,   28,   45,\n",
      "          24,   10,   60,    0,  375,    6,   28,  372,  122,  330,    1]), tensor([769,   0,  53,  10,  26,   0,   1]), tensor([ 13,   0,  87, 301,   1]), tensor([182,   5, 799,   9, 455, 113,  19,  14,  81, 100,   3,  11,  85,  18,\n",
      "          0,   5,   1]), tensor([ 18,   0,  39,  26, 468,  39,   2, 167, 334, 182,   5, 251,   1]), tensor([ 78,  23,  43,   0,  19,  10,   0,   0,   4,  39,  10,   0,  32,  21,\n",
      "         14, 491,  70, 189,  10,   0,  81,   0,  57,   1]), tensor([108,  59,  18,  44,   1]), tensor([  54,  110,    7,   10,    0,    6,    5,    7,  233, 1326,    1]), tensor([   0,  348,   27,  220,  190,    9,    0,   10,  134,   13,   16,   10,\n",
      "           0,  164,   86, 1235,   12]), tensor([188,   4,   0,   1])]\n",
      "[tensor([17, 29, 16,  2, 68, 12,  1]), tensor([84,  5,  1,  1, 17,  0,  1]), tensor([ 22, 291,   7,  10, 951,   1]), tensor([ 0, 16,  2,  0,  1]), tensor([  2, 442,  45, 106,   9,  42,   4, 816, 997, 148,   1]), tensor([364, 295, 129, 139,   1]), tensor([ 90, 197, 203,  45,   0,   6,   0,   7,  18,   0, 133,  71,   1]), tensor([   0,    7,  108, 1267,    1]), tensor([30, 17, 12,  1]), tensor([ 28, 254, 133,   0,   0,   9,  66,   9,  55,  13,   0,  53,   2,  89,\n",
      "         28, 120, 561,  21,   2,  13,   1,  31, 159,   7,   0,   1]), tensor([  3,  80, 438,   3, 337,   5,   1]), tensor([ 52, 129,  12,  12,  12,  12,  12,  12,  12,  12,   1]), tensor([  2,  91, 280, 363,   2,   0,   0,   6,  34,   5, 138,  18,  44,   1]), tensor([   2, 1241,    7,   15,  252,   41,   38,   39, 1004,    2,    0, 1298,\n",
      "          36,   23,  392,    1]), tensor([70,  5, 30, 12, 57]), tensor([ 23, 356, 129,  53,   0,  12]), tensor([   3,   59,   18, 1001,   10,  292,   19,    2,  397,  191,    5,    7,\n",
      "          21,   14,    0,    1]), tensor([280, 339,   4, 143, 250, 826,  47, 488, 229,   0,   1]), tensor([114,   0,  53, 148, 194,  12]), tensor([ 54,  28,  45, 468,  16,  10,  26,  41, 124,  32, 455, 468,   6,   8,\n",
      "          0,  11,  25,   5,   1]), tensor([169, 406,  19,   0,   0,   1]), tensor([  5,   0,  18,  44, 183,  15,   2,  89,  19,  14, 477,   1]), tensor([   3,   11,   96,   35,   77, 1312,    0,    2,  246,    6,    0,  516,\n",
      "          86,    0,  115,   15,    2,    0,    1]), tensor([  2, 477,   0,   0, 198, 138,  18,  44,  19,  14,  13,   1]), tensor([  0,  31, 207,  12,   1]), tensor([  2, 525,   7,  23,   0,   4,   0,   6,   3,  61,   0,  72,  53, 148,\n",
      "        194,   1]), tensor([ 73,  11,  25,   0, 221,   0,   4,  73,  11,  25, 421, 115, 443, 544,\n",
      "          1]), tensor([   2,    1,    0,    0,  199,    6,  229,   10, 1082,   15,   10,   13,\n",
      "           6,    7,  509,   26,    1]), tensor([  8,   7,  10,  17, 749,   1]), tensor([  2, 246, 588,  20, 119,   6,   5,   2,   0,   0,  22,   5,  78,   1]), tensor([17, 29,  4, 68,  1]), tensor([  34,   99,   22,    5,    7,   87,   15, 1341,    2,  162,  216,    0,\n",
      "           1,    0,  463,    1,    1,    1,  338,   55,   93,    0,    0,    1,\n",
      "           1,    1,  689,  114,   18,  544,    5,    1]), tensor([ 18,  26, 175,  16,   2,  68,   1]), tensor([  3,  64, 405,  11,  25,  56,   2,  21, 657,   6,   5,   0,  56,   5,\n",
      "         61,   0,  19,  42,   1]), tensor([  2, 472,   7,  17,   1]), tensor([17, 29,  1]), tensor([ 481,   88,   27,   10,  147,  295,  129,    6,   38,   28,   66,  131,\n",
      "          28, 1085,   16,    1]), tensor([  39,    3,  518,  584,    1,    1,    1,    1,  215, 1326,   12]), tensor([  3,  56, 298,   4, 251,  15, 158,   0,   2,  40, 442,   4,   7, 215,\n",
      "          0,   9, 361, 318,  10, 747, 192,   0,  24,   2,  40,   1]), tensor([  2, 298,   7,  23,  26,   1,   1]), tensor([364, 186, 139, 188,   1,   1,   1,   1,  54,   3,  98, 437,   8,   0,\n",
      "          0,   3,  61,   1]), tensor([  3,  27, 278, 188,   0, 442,   4,   2,   0,  53,   2, 573,   7,  23,\n",
      "        443,   1]), tensor([  3,  84,   2, 516,  90,  88,  45,  38,   0,  12]), tensor([103,   3,  11,  96, 177,  38, 155,   1,   1,   1,   1,   3,  11,  96,\n",
      "        187, 256,   0,   4,   8,  33,  46,   2, 103,  41,  70,  16, 197,  50,\n",
      "          4,   2,   0,  57,  39,  43,  39, 415,  15, 189,   1]), tensor([  0,   5,  11,  37,   0,  21, 436, 527,  62,   4,   3,  84,   5,  12]), tensor([256,  47,  22,   6,   2, 458,   7,  76,   4,   0,   2, 121,   7,  23,\n",
      "          0,   2, 740,  87,  16,   2, 305,   7,  10,  26,   0,   1]), tensor([  3,  80,  23, 107]), tensor([   5,   11,   37,   38, 1258,    9,   27,    9,  455,  659,   94,  681,\n",
      "           6,  116,  681,    6,    0,    6,  314,    4,  116,    0,  237,   89,\n",
      "          10,   94,   13,  700,   87,    1]), tensor([753, 294,   1]), tensor([  63,    0,   19,  165,    0,    2,  236,  172,  273, 1168,    2,  214,\n",
      "          63,  112,  174,    1]), tensor([  5,   7,   0,  24,  10,   0, 116, 133,   0,   0,   1]), tensor([  63,   14,   13,   92,    9,   65,   79,   10,  275,  484,    6,    5,\n",
      "          11,   37,  102,    0,  648,  758,  143,  389,   21,   14, 1082,    1]), tensor([  3,  27,  82,   8, 477, 135,  13,  10, 176, 570, 277,   4,   5,  20,\n",
      "         10,   0,   1]), tensor([   2,  395,    7,  118,    0,   47,    3, 1284,    5,   61,   65,    6,\n",
      "           4,    2,  886,   45,    0, 1204,    1]), tensor([ 0, 23,  0, 12, 12, 12, 12, 12,  1]), tensor([   3,  405,   11,   25,  552,   22,    2,  940, 1129,  571,  443,    9,\n",
      "          50,    1]), tensor([   2,    0,  463,    1,    1,    1,  610,  576,    6,  392,    0,   53,\n",
      "        1245,    1]), tensor([   2,   31,    7,  233, 1326,    9,   50,    1]), tensor([  8,   0,  24,   2,  13, 229, 302,   0, 133,   0, 149,  86,   0,   1]), tensor([  3,  66, 586, 205, 109,  24,  14,   0,   6, 362,  19, 256, 214,   3,\n",
      "         27,  18,  35,   8, 145,   1]), tensor([153,   0, 172,   6, 160,  31, 159,   6, 160, 261,   1]), tensor([1107,    0,    0,    1]), tensor([623,   7,  18, 153,   6,  36,   7,  23,   0,   4,  28,  27,   9,   0,\n",
      "        100,  28, 271,   1]), tensor([163,   3,  35,   9,   0, 490,   5,   0, 266,   5,   0, 956, 786,   1]), tensor([   2,  334, 1163,    7,  155,  382,  131,  256,    0,  214,   45,    0,\n",
      "           0,    1]), tensor([ 88,  45,  38, 173,  12]), tensor([134, 260,   1]), tensor([ 24,  14, 908,   3,  20, 878, 236, 734,   0,   4,  77, 734,  24,  14,\n",
      "          0,   1]), tensor([ 95, 314, 571,  10,  17, 204,  62,   4,   3, 552,  88,  45, 493,   2,\n",
      "        103, 260,   3,  11,  96, 181,  24,   2, 248, 218, 193,  62,   0,   1]), tensor([  2,   0,   0,   7, 201,   4,   0,   1]), tensor([17, 13,  1]), tensor([  3,  11,  96,  67,  35,  14,  81,  16,  10, 176, 570,   6,  34,   3,\n",
      "         60,  56,   5,   1]), tensor([ 0, 13, 12, 12, 12]), tensor([ 14,  67, 709,   7,   2,   0,  36, 149,   7,  10, 325, 252, 118, 100,\n",
      "        562, 113,   9,  70,  15,  57]), tensor([  0,  16,   2,   0,   0,   7,   0,   6,   4,   2,   0,   4, 303,   0,\n",
      "         45,  18, 362,   3, 421, 148,   9,  65,   1,   1]), tensor([ 64, 182,   5, 799,   9, 444,  21,   9,   1]), tensor([  28,   27,    9,  444,    2,   13,  133,   10,    0,    0,   16,    2,\n",
      "         256, 1083,    9,  122,   28,  691,    1]), tensor([ 31,   0,   7, 340, 160,   1]), tensor([  2, 298,   7,  23,   0,   6,  39,   2,  40, 694,   7,  18,  23, 104,\n",
      "        133,  71,   1]), tensor([ 76,  41,   0,   6,   0, 220, 392,   0,  87, 110,   1]), tensor([   5,   46,   10,   17,  199,    0, 1038,    6,    4,    2,    0,   45,\n",
      "          76,    4,  153,   19,   17,  334,   41,    1]), tensor([   3,   61,    0,   48,    2,  158,    0,   16, 1282,  572,   45,  468,\n",
      "          16,  701,    6,  232,    4,   10,   17,   68,   12]), tensor([   0, 1275,  352,    1]), tensor([17, 13, 12,  1]), tensor([  8,   0,   0,  61,  18,  44,  19,  14, 124, 506,   0,   1]), tensor([ 3, 61, 48,  8,  1]), tensor([  3, 489,  15,  95, 287,   4,  61,   0, 610,   1]), tensor([  10,    0,  958,  370,    0,   10, 1087,  206,   10,    0,    0,    0,\n",
      "         452,   15,  108,   10,    0,    0,    1]), tensor([  59,   18,   44,   16,  981,    9, 1040,   19,    2,  689,    1]), tensor([ 23, 769,   1]), tensor([ 59,  18, 121,   1]), tensor([ 54,  28, 528, 569,  10, 267, 431,  13,   0, 251,   0,  39,   2, 423,\n",
      "          0, 367,   0,  45,   0,   1]), tensor([ 24,   2,   0,  15,  97,   0,   6,   3,  35, 112, 125,   0,   0,  62,\n",
      "          7,  22,   2,  94,  13,  21,   0, 224,  12, 224]), tensor([  8,   7, 311, 235,   2, 103,  13,   3,  11,  96, 139,  35,   1]), tensor([28, 58, 11, 25,  0,  2, 68, 21, 95,  1]), tensor([1052,   14,    0,    2,  103,   13,    3,   27,  139,   35,    1]), tensor([  8,  49,  46,   0,   2,  33, 275,   0,   4, 191,   5,   0,   0,  15,\n",
      "        361,   6,   5,   7, 580,   0,   1]), tensor([273,  61,  48,  95,   9, 488,   1]), tensor([  18,   26,  100,    0,   10,    0,   86, 1265,    1]), tensor([131,  10, 144,  15, 207,   1,   1,   3, 992,  75, 172,  21,   8,  13,\n",
      "          1]), tensor([  3,   0,   0,   4, 110,   7,  77, 239,  16,   5, 101,   7,  60, 769,\n",
      "          1]), tensor([ 38, 155,   5,  46,  78,  56,  10, 289,   1]), tensor([84,  8, 32, 12]), tensor([ 71,  24,  71,   3, 552,   5,  20,  10,  26,   0,   1]), tensor([17, 91,  1]), tensor([ 5, 11, 37,  0, 12]), tensor([  63, 1147,    4,  189,    2,   29,   16,  108,  174,    5,  170,    1]), tensor([ 23,   0, 129,   1]), tensor([  0, 700,  19,  10, 541, 461,  22,  28,  58,  42,   9, 461, 113,  55,\n",
      "        199,   0,   6,   4, 118,   0,   0,   0,  70,   0,  12]), tensor([   0,   24,    2,   40,    6,   73,   11,   25,   42,   19,  460, 1335,\n",
      "          70,    0,   57,    1]), tensor([  22,  229,  518,    6,   16,   10,   13,    6,    2,  199,    7,   23,\n",
      "          76,  253,  173,    0,    9, 1097,   19,    6,    4, 1331,    7,  403,\n",
      "          39,   43,    1]), tensor([ 23, 392, 497,   6,   0,  56,  97, 484,   0,   0,   1]), tensor([1126,    0,  199,   62,   23,   76,  494,    6,   64,   46,  173,    0,\n",
      "          56,  284,    4,    0,    6,    4,   75,    1]), tensor([0, 0, 1]), tensor([  2, 291,   0,  14,  29,  23,   0,   4,   5,  30,  43,   1]), tensor([ 26,  49,   6,  52, 357,   1]), tensor([  2,  32,   0,  14,   0,  38,   3,  80, 107,  19,  14, 260,   1]), tensor([ 73,  11,  25, 130,  55,  93,   1]), tensor([1102,  410,  130,   55,   93,    1]), tensor([  3, 361,   5,   0,   4,   5, 904, 113,  23,  43,   1]), tensor([ 17,  32,   6,  23, 180,  62,   0,   1]), tensor([ 116,   91,   39,   43,   39,    0,   91,   45,  448,    9,  123,  545,\n",
      "          28,  255, 1181,   87,   15,    0,    1,    0,    0]), tensor([17, 81, 12,  1]), tensor([ 17,   1,   1,   1,  77, 216, 133,  71,  12,   1]), tensor([610,   0,  79,   8,  13,  11,  37,   0,   7,  22,   5,  60, 208, 341,\n",
      "          0,   6, 304,  24,   2,  71, 284,   0,   1]), tensor([   3,   11,   96,   64,   35,  216,   19,    2,   13, 1141,    2,    0,\n",
      "           0,   24,  101,    3,  278,  561,    5,   21,    4,  163,  161,  194,\n",
      "           1]), tensor([130,  15,  93,   1]), tensor([ 181,   23, 1260,    1]), tensor([   5,    7,    0, 1326,    4,  138,   18,    0,  115,  321,   15,    0,\n",
      "           9,   14,  109,   63,    3,   82,    5,    1]), tensor([ 73,  11,  25,  72,   8,  29,  62,   5, 834,  12,   1]), tensor([579,   0,   0,   9,  42,   2, 162,   0,  91, 298,   1]), tensor([  3,  11,  96, 236,  14,  13,  75, 555,  47,   3,  58, 520,   6, 118,\n",
      "         21,   0,   4,  14,  13,   7, 146,  17,  70,   0,  21,   0,  12,  57,\n",
      "          1]), tensor([  3, 140,  48,   8,  49,   1]), tensor([ 2, 13, 20,  0,  4, 20, 18, 94,  1]), tensor([  23,   26,   41, 1283]), tensor([   5,   11,   37,    0,    4,    2,   36,   41,    7,  340,  336,    0,\n",
      "          19,    2,   13,   70,  342,   57,   86,   19,   14, 1118, 1356,   32,\n",
      "          70,   22, 1104,  206,   97,  460,   57,    1]), tensor([221,   0,   4,   0,   0, 481,   0, 131,  88,  11,   0,   0,   1]), tensor([  2, 103,  13,  24, 470,  57,   1]), tensor([ 3,  0,  5,  0, 90, 15,  2, 36, 41,  1]), tensor([105,  11,  25,  44, 133,  71,   1,   1,   3,   0,   5,  16,  14,   0,\n",
      "          4, 142,  18, 132,   1]), tensor([  5,  61, 270,  69, 160,   9,   0, 141, 134,  14, 295, 129, 422,  46,\n",
      "        102,  19, 167,   1]), tensor([  0,  33, 747,   4, 163,   0, 113,   1]), tensor([  13,    7, 1260,   39,   71,  477,  630,  214,   45,    1]), tensor([105,  11,  25,  83,   2, 247,   1]), tensor([ 3, 84,  8, 13, 12,  1]), tensor([  0, 145,   1,   1,   1,   1,  23,   0]), tensor([  3, 361, 882,   4,   5, 156, 119,  19, 148,   1]), tensor([ 71, 353, 170, 363, 112, 212,  15,  42,   1]), tensor([  77,    0,   40,  874, 1129,    6,    4,   77,  940, 1348,    1]), tensor([  0, 298,   1]), tensor([  8,   7, 171, 155,   2, 364, 260,   3,  11,  96, 181,  21, 167,   1]), tensor([ 71,  24,  71,   6,   3,  11,  85, 340, 345,  19,   8, 260,   1]), tensor([  2, 940, 405,  11,  25,   0,  22,  10, 473,   0,  98,  65, 188,   1]), tensor([ 34, 100,   3,   0, 166,   0, 133,   0,   6,   2,   0,   0,   0, 161,\n",
      "         10, 176, 522, 206,   2, 120,   0,   6,   4, 163,   3,  11,  85, 469,\n",
      "          1]), tensor([  0,   4,   0,   0,   9, 262,   5, 194,   4,   0,  50,   9,  72, 610,\n",
      "         13, 192, 115, 321,  15,   0,   0,   1]), tensor([104, 121,  62,  28, 254,  55,  32,   9,  65, 104,  16, 133,   0,  97,\n",
      "          0, 133,  10,  89,   6,  54,  18,  16,  97,   0, 747,   1]), tensor([ 23, 106,   9,  42,   1]), tensor([199, 395,   0,   7, 378,   1]), tensor([  8,   0,  13, 289,   7, 597,   4,  23,   0, 672,   1]), tensor([  3,  27, 102,  23, 107,  19,   2,   4,  27,  35,  77,   0,  53, 115,\n",
      "         33,   0,  14,  36,  41,  21, 221, 303,   1]), tensor([ 2,  0, 20, 21, 89,  1]), tensor([   0,    6,  252,   41,   36,    6,    4,    2,    0,  196,    2,    0,\n",
      "          11,   37,    0,   20, 1107, 1266,    0,    4,    0,  161,    1]), tensor([336,   0,   1]), tensor([   8,    0,    0,    0,   13, 1241,    7,  226,    1]), tensor([ 5, 11, 37, 23,  0,  4,  0,  9, 65, 15, 26, 41,  1]), tensor([  3,  82,   8,   9,  42,  19,  14,   0, 845,   4, 586,   0,   5,  12]), tensor([ 13,  99, 904, 152,  56,   5, 138, 100,   5,  20,  94,   1]), tensor([205,  13,   1]), tensor([ 71,   3,  58,  83,   7,   0,  21,   2, 246,   6,  38, 315,   5,   0,\n",
      "          1,   2,  75,   3,  42,   2, 111,   2, 250,   3,  56,   5,   1]), tensor([  3,   0,  14, 325,  93,  19,   8, 414,   1]), tensor([150,  45,  69, 267,   1]), tensor([ 115,    0,    0,    1,    1, 1069,   26,   13,    9,   72,    1,    1]), tensor([  2, 364, 144,  15, 294, 139, 604,  19,   2, 165, 295, 129,   1]), tensor([59, 18, 44,  1]), tensor([273, 343,  10,   0,   0,  31,  22,   0,   0,  63,  10, 234,   0, 331,\n",
      "        188,   9, 248,  10, 274,  62,  34, 146, 323, 467,  47,   0]), tensor([   5,    7, 1266,  391,  113,   16,   42,   39,   10,  267,    0,    0,\n",
      "           6,    4,    0,   24, 1282,    0,    1]), tensor([94, 31, 30, 17, 24, 13,  1]), tensor([  0,   4, 153,   1]), tensor([ 14,   0, 484, 477,  53,   0, 904,   2, 152,  10, 209,  51,  47,   8,\n",
      "          1]), tensor([  3,  11,  85, 107,  79,   8,   0,  26,  41,   4, 252,  68,   1]), tensor([  5,   7,  38, 267,   4,  28,  73,  11,  25, 118,   0,  22,   5,   7,\n",
      "        110,  63,  10, 191,  15, 878, 188,   9,   5,   1]), tensor([ 142,    0,    4,   23, 1204,    0,    2,  385,    7,   10,   23,   76,\n",
      "         395,    1]), tensor([  3, 343,  14,  32,  24,  26,  89,   4,  20, 107,  19,   5,   1]), tensor([  3,  11,  85,  23, 202,  19,  14, 751,   1]), tensor([ 31, 159,   7,  64,  17,  12]), tensor([  2, 399,  15,   2, 442,   7, 336,   1]), tensor([ 23,  43, 181,   4, 156,  14,   0,   0, 257,   1]), tensor([ 23, 104,   1]), tensor([308, 129,   1]), tensor([77,  0,  0, 21,  8, 33, 12,  1]), tensor([ 98,  18,  66, 541, 175, 219,   1]), tensor([  2, 364,  13,  53, 344,   1,   1,   1, 294,   1,   1,   1,   1,   1,\n",
      "          8,   0, 344, 412,   1,   1,   7,   2, 364,   3,  27, 139,   0,   1]), tensor([  3, 222,  21, 124,  11,  37,   0,   4,   0,  71,   0,   6,  34,  98,\n",
      "         18,  66,   5,   9,   0, 194,   1]), tensor([  64,    6,    2,   13,  105,   11,   25, 1194,    9,    0,  279,    0,\n",
      "           0,    0,    6,    0,    0,  171,    0,    0, 1098,    1]), tensor([ 118,   24,   14,    0, 1201,  101,    7,    0,    0,    6,    3,   27,\n",
      "        1312,    0,  131,    2,  256,    0,    7, 1185,    1]), tensor([  22,   11,   37,   10,  911,  298,    0,   70, 1323,    3,   11,   85,\n",
      "          18,  189,    5,    0,    6,  101,    3,   73,   11,   25,  552,    7,\n",
      "           2,   49,   57,    1]), tensor([  18,   67,  114,    5,  781,   55, 1098,    6,   34,  471,   64,    0,\n",
      "           0,    5,    1]), tensor([248,  89, 659,  53,  28,   1]), tensor([1340,   62,  269,  377,    1]), tensor([  0,  10, 282,   1])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anton\\AppData\\Roaming\\Python\\Python312\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator       \n",
    "import copy     # pre-process\n",
    "\n",
    "data = pd.read_csv(\"amazon_cells_labelled.txt\", delimiter='\\t', header=None)\n",
    "data.columns = ['Sentence', 'Class']\n",
    "data['index'] = data.index                                          # add new column index\n",
    "columns = ['index', 'Class', 'Sentence']\n",
    "data = preprocess_pandas(data, columns)   \n",
    "\n",
    "train_iter, val_iter, test_iter = \\\n",
    "              np.split(data.sample(frac=1, random_state=42), \n",
    "                       [int(.6*len(data)), int(.8*len(data))])\n",
    "\n",
    "train_iter2 = copy.deepcopy(train_iter)\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter2[\"Sentence\"]), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "# get data, pre-process and split              \n",
    "\n",
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data2 = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    print(data2)\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data2)))\n",
    "\n",
    "\n",
    "train_data = data_process(train_iter[\"Sentence\"])\n",
    "val_data = data_process(val_iter[\"Sentence\"])\n",
    "test_data = data_process(test_iter[\"Sentence\"])\n",
    "\n",
    "\n",
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"Divides the data into ``bsz`` separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Arguments:\n",
    "        data: Tensor, shape ``[N]``\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape ``[N // bsz, bsz]``\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 40\n",
    "eval_batch_size = 40\n",
    "train_data = batchify(train_data, batch_size)  # shape ``[seq_len, batch_size]``\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)\n",
    "\n",
    "#training_data, validation_data, training_labels, validation_labels = train_test_split( # split the data into training, validation, and test splits\n",
    "#    data['Sentence'].values.astype('U'),\n",
    "#    data['Class'].values.astype('int32'),\n",
    "#    test_size=0.10,\n",
    "#    random_state=0,\n",
    "#    shuffle=True\n",
    "#)\n",
    "#\n",
    "## vectorize data using TFIDF and transform for PyTorch for scalability\n",
    "#word_vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,2), max_features=50000, max_df=0.5, use_idf=True, norm='l2')\n",
    "#training_data = word_vectorizer.fit_transform(training_data)        # transform texts to sparse matrix\n",
    "#training_data = training_data.todense()                             # convert to dense matrix for Pytorch\n",
    "#vocab_size = len(word_vectorizer.vocabulary_)\n",
    "#validation_data = word_vectorizer.transform(validation_data)\n",
    "#validation_data = validation_data.todense()\n",
    "#\n",
    "#train_x_tensor = torch.from_numpy(np.array(training_data)).type(torch.FloatTensor)\n",
    "#train_y_tensor = torch.from_numpy(np.array(training_labels)).long()\n",
    "#validation_x_tensor = torch.from_numpy(np.array(validation_data)).type(torch.FloatTensor)\n",
    "#validation_y_tensor = torch.from_numpy(np.array(validation_labels)).long()\n",
    "#\n",
    "#class TransfromerDataset(torch.utils.data.Dataset):\n",
    "#    def __init__(self, datasetA, bptt):\n",
    "#        self.source = datasetA\n",
    "#        self.bptt = bptt\n",
    "#\n",
    "#    def __getitem__(self, i):\n",
    "#        seq_len = min(self.bptt, len(self.source) - 1 - i)\n",
    "#        data = self.source[i:i+seq_len]\n",
    "#        target = self.source[i+1:i+1+seq_len].reshape(-1)\n",
    "#        return data, target\n",
    "#\n",
    "#    def __len__(self):\n",
    "#        return min(len(self.datasetA))\n",
    "#    \n",
    "#train_ds = ConcatDataset(train_x_tensor,train_y_tensor)\n",
    "#val_ds = ConcatDataset(validation_x_tensor,validation_y_tensor)\n",
    "#train_loader = DataLoader(train_ds,batch_size=5)\n",
    "#val_loader = DataLoader(val_ds,batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape ``[full_seq_len, batch_size]``\n",
    "        i: int\n",
    "\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape ``[seq_len, batch_size]`` and\n",
    "        target has shape ``[seq_len * batch_size]``\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anton\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "ntokens = len(vocab)  # size of vocabulary\n",
    "emsize = 200  # embedding dimension\n",
    "d_hid = 200  # dimension of the feedforward network model in ``nn.TransformerEncoder``\n",
    "nlayers = 2  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``\n",
    "nhead = 2  # number of heads in ``nn.MultiheadAttention``\n",
    "dropout = 0.5  # dropout probability\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "def train(model: nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 2\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(train_data) // bptt\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        output = model(data)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        loss = criterion(output_flat, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            seq_len = data.size(0)\n",
    "            output = model(data)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += seq_len * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |     2/    5 batches | lr 5.00 | ms/batch 230.23 | loss 11.57 | ppl 106252.98\n",
      "| epoch   1 |     4/    5 batches | lr 5.00 | ms/batch 155.49 | loss  7.47 | ppl  1753.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time:  0.89s | valid loss  7.90 | valid ppl  2688.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |     2/    5 batches | lr 4.75 | ms/batch 233.65 | loss 10.54 | ppl 37723.93\n",
      "| epoch   2 |     4/    5 batches | lr 4.75 | ms/batch 158.36 | loss  6.59 | ppl   730.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time:  0.90s | valid loss  6.89 | valid ppl   982.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |     2/    5 batches | lr 4.51 | ms/batch 221.01 | loss 10.55 | ppl 38148.53\n",
      "| epoch   3 |     4/    5 batches | lr 4.51 | ms/batch 151.29 | loss  6.40 | ppl   601.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time:  0.87s | valid loss  6.26 | valid ppl   525.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |     2/    5 batches | lr 4.29 | ms/batch 204.92 | loss  8.94 | ppl  7659.00\n",
      "| epoch   4 |     4/    5 batches | lr 4.29 | ms/batch 144.92 | loss  6.29 | ppl   540.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time:  0.83s | valid loss  6.14 | valid ppl   465.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |     2/    5 batches | lr 4.07 | ms/batch 210.22 | loss  8.78 | ppl  6483.04\n",
      "| epoch   5 |     4/    5 batches | lr 4.07 | ms/batch 140.63 | loss  5.51 | ppl   247.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time:  0.81s | valid loss  5.96 | valid ppl   387.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |     2/    5 batches | lr 3.87 | ms/batch 203.73 | loss  8.34 | ppl  4186.15\n",
      "| epoch   6 |     4/    5 batches | lr 3.87 | ms/batch 138.52 | loss  5.46 | ppl   233.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time:  0.80s | valid loss  5.65 | valid ppl   284.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |     2/    5 batches | lr 3.68 | ms/batch 214.30 | loss  8.11 | ppl  3337.98\n",
      "| epoch   7 |     4/    5 batches | lr 3.68 | ms/batch 136.62 | loss  5.23 | ppl   187.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time:  0.82s | valid loss  5.73 | valid ppl   308.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |     2/    5 batches | lr 3.49 | ms/batch 216.55 | loss  7.83 | ppl  2520.99\n",
      "| epoch   8 |     4/    5 batches | lr 3.49 | ms/batch 142.14 | loss  5.07 | ppl   159.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time:  0.82s | valid loss  5.61 | valid ppl   273.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |     2/    5 batches | lr 3.32 | ms/batch 208.01 | loss  7.63 | ppl  2064.50\n",
      "| epoch   9 |     4/    5 batches | lr 3.32 | ms/batch 153.94 | loss  4.95 | ppl   141.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time:  0.83s | valid loss  5.46 | valid ppl   236.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |     2/    5 batches | lr 3.15 | ms/batch 206.57 | loss  7.26 | ppl  1420.20\n",
      "| epoch  10 |     4/    5 batches | lr 3.15 | ms/batch 136.13 | loss  4.85 | ppl   127.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time:  0.81s | valid loss  5.40 | valid ppl   221.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |     2/    5 batches | lr 2.99 | ms/batch 206.70 | loss  7.08 | ppl  1185.21\n",
      "| epoch  11 |     4/    5 batches | lr 2.99 | ms/batch 137.35 | loss  4.88 | ppl   131.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time:  0.80s | valid loss  5.40 | valid ppl   220.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |     2/    5 batches | lr 2.84 | ms/batch 212.77 | loss  6.91 | ppl  1006.38\n",
      "| epoch  12 |     4/    5 batches | lr 2.84 | ms/batch 139.93 | loss  4.70 | ppl   109.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time:  0.82s | valid loss  5.41 | valid ppl   224.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |     2/    5 batches | lr 2.70 | ms/batch 209.34 | loss  6.78 | ppl   881.92\n",
      "| epoch  13 |     4/    5 batches | lr 2.70 | ms/batch 133.01 | loss  4.56 | ppl    95.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time:  0.79s | valid loss  5.37 | valid ppl   214.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |     2/    5 batches | lr 2.57 | ms/batch 214.53 | loss  6.64 | ppl   768.70\n",
      "| epoch  14 |     4/    5 batches | lr 2.57 | ms/batch 138.44 | loss  4.48 | ppl    87.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time:  0.82s | valid loss  5.42 | valid ppl   224.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |     2/    5 batches | lr 2.44 | ms/batch 202.23 | loss  6.51 | ppl   670.26\n",
      "| epoch  15 |     4/    5 batches | lr 2.44 | ms/batch 140.09 | loss  4.40 | ppl    81.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time:  0.80s | valid loss  5.45 | valid ppl   233.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |     2/    5 batches | lr 2.32 | ms/batch 232.41 | loss  6.47 | ppl   646.09\n",
      "| epoch  16 |     4/    5 batches | lr 2.32 | ms/batch 151.05 | loss  4.28 | ppl    72.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time:  0.90s | valid loss  5.45 | valid ppl   233.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |     2/    5 batches | lr 2.20 | ms/batch 201.66 | loss  6.28 | ppl   531.84\n",
      "| epoch  17 |     4/    5 batches | lr 2.20 | ms/batch 134.07 | loss  4.21 | ppl    67.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time:  0.78s | valid loss  5.47 | valid ppl   238.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |     2/    5 batches | lr 2.09 | ms/batch 199.97 | loss  6.20 | ppl   490.99\n",
      "| epoch  18 |     4/    5 batches | lr 2.09 | ms/batch 142.37 | loss  4.13 | ppl    62.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time:  0.80s | valid loss  5.51 | valid ppl   247.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |     2/    5 batches | lr 1.99 | ms/batch 213.61 | loss  6.07 | ppl   434.14\n",
      "| epoch  19 |     4/    5 batches | lr 1.99 | ms/batch 136.29 | loss  4.08 | ppl    59.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time:  0.81s | valid loss  5.49 | valid ppl   241.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |     2/    5 batches | lr 1.89 | ms/batch 198.78 | loss  5.97 | ppl   391.63\n",
      "| epoch  20 |     4/    5 batches | lr 1.89 | ms/batch 142.63 | loss  4.04 | ppl    56.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time:  0.80s | valid loss  5.51 | valid ppl   246.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  21 |     2/    5 batches | lr 1.79 | ms/batch 265.58 | loss  5.87 | ppl   352.72\n",
      "| epoch  21 |     4/    5 batches | lr 1.79 | ms/batch 136.62 | loss  3.93 | ppl    51.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time:  0.92s | valid loss  5.54 | valid ppl   255.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  22 |     2/    5 batches | lr 1.70 | ms/batch 203.59 | loss  5.79 | ppl   328.05\n",
      "| epoch  22 |     4/    5 batches | lr 1.70 | ms/batch 132.17 | loss  3.90 | ppl    49.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time:  0.79s | valid loss  5.54 | valid ppl   253.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  23 |     2/    5 batches | lr 1.62 | ms/batch 211.69 | loss  5.72 | ppl   305.40\n",
      "| epoch  23 |     4/    5 batches | lr 1.62 | ms/batch 135.53 | loss  3.82 | ppl    45.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time:  0.81s | valid loss  5.57 | valid ppl   262.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  24 |     2/    5 batches | lr 1.54 | ms/batch 206.20 | loss  5.61 | ppl   274.42\n",
      "| epoch  24 |     4/    5 batches | lr 1.54 | ms/batch 139.04 | loss  3.75 | ppl    42.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time:  0.79s | valid loss  5.60 | valid ppl   269.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  25 |     2/    5 batches | lr 1.46 | ms/batch 199.80 | loss  5.55 | ppl   257.58\n",
      "| epoch  25 |     4/    5 batches | lr 1.46 | ms/batch 135.31 | loss  3.72 | ppl    41.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time:  0.78s | valid loss  5.64 | valid ppl   281.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  26 |     2/    5 batches | lr 1.39 | ms/batch 200.24 | loss  5.50 | ppl   244.90\n",
      "| epoch  26 |     4/    5 batches | lr 1.39 | ms/batch 132.59 | loss  3.68 | ppl    39.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time:  0.78s | valid loss  5.65 | valid ppl   283.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  27 |     2/    5 batches | lr 1.32 | ms/batch 203.68 | loss  5.43 | ppl   228.33\n",
      "| epoch  27 |     4/    5 batches | lr 1.32 | ms/batch 139.88 | loss  3.66 | ppl    38.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time:  0.79s | valid loss  5.68 | valid ppl   293.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  28 |     2/    5 batches | lr 1.25 | ms/batch 203.72 | loss  5.35 | ppl   210.44\n",
      "| epoch  28 |     4/    5 batches | lr 1.25 | ms/batch 141.49 | loss  3.59 | ppl    36.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time:  0.80s | valid loss  5.70 | valid ppl   300.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  29 |     2/    5 batches | lr 1.19 | ms/batch 200.79 | loss  5.30 | ppl   201.16\n",
      "| epoch  29 |     4/    5 batches | lr 1.19 | ms/batch 134.60 | loss  3.57 | ppl    35.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time:  0.78s | valid loss  5.73 | valid ppl   308.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  30 |     2/    5 batches | lr 1.13 | ms/batch 198.49 | loss  5.27 | ppl   194.24\n",
      "| epoch  30 |     4/    5 batches | lr 1.13 | ms/batch 136.91 | loss  3.53 | ppl    34.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time:  0.78s | valid loss  5.74 | valid ppl   310.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  31 |     2/    5 batches | lr 1.07 | ms/batch 229.09 | loss  5.18 | ppl   178.41\n",
      "| epoch  31 |     4/    5 batches | lr 1.07 | ms/batch 145.17 | loss  3.50 | ppl    33.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time:  0.86s | valid loss  5.76 | valid ppl   316.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  32 |     2/    5 batches | lr 1.02 | ms/batch 191.40 | loss  5.17 | ppl   175.74\n",
      "| epoch  32 |     4/    5 batches | lr 1.02 | ms/batch 131.21 | loss  3.45 | ppl    31.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time:  0.78s | valid loss  5.78 | valid ppl   322.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  33 |     2/    5 batches | lr 0.97 | ms/batch 255.43 | loss  5.07 | ppl   159.51\n",
      "| epoch  33 |     4/    5 batches | lr 0.97 | ms/batch 143.17 | loss  3.42 | ppl    30.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time:  0.91s | valid loss  5.79 | valid ppl   326.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  34 |     2/    5 batches | lr 0.92 | ms/batch 207.15 | loss  5.07 | ppl   159.13\n",
      "| epoch  34 |     4/    5 batches | lr 0.92 | ms/batch 132.03 | loss  3.38 | ppl    29.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time:  0.80s | valid loss  5.82 | valid ppl   335.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  35 |     2/    5 batches | lr 0.87 | ms/batch 200.74 | loss  5.00 | ppl   147.91\n",
      "| epoch  35 |     4/    5 batches | lr 0.87 | ms/batch 135.78 | loss  3.35 | ppl    28.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time:  0.78s | valid loss  5.83 | valid ppl   339.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  36 |     2/    5 batches | lr 0.83 | ms/batch 196.57 | loss  4.99 | ppl   147.42\n",
      "| epoch  36 |     4/    5 batches | lr 0.83 | ms/batch 135.06 | loss  3.33 | ppl    27.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time:  0.77s | valid loss  5.84 | valid ppl   344.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  37 |     2/    5 batches | lr 0.79 | ms/batch 196.27 | loss  4.95 | ppl   141.27\n",
      "| epoch  37 |     4/    5 batches | lr 0.79 | ms/batch 131.14 | loss  3.31 | ppl    27.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time:  0.76s | valid loss  5.85 | valid ppl   348.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  38 |     2/    5 batches | lr 0.75 | ms/batch 208.77 | loss  4.89 | ppl   133.18\n",
      "| epoch  38 |     4/    5 batches | lr 0.75 | ms/batch 133.39 | loss  3.29 | ppl    26.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time:  0.80s | valid loss  5.87 | valid ppl   352.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  39 |     2/    5 batches | lr 0.71 | ms/batch 200.66 | loss  4.86 | ppl   128.69\n",
      "| epoch  39 |     4/    5 batches | lr 0.71 | ms/batch 133.77 | loss  3.24 | ppl    25.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time:  0.78s | valid loss  5.88 | valid ppl   357.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  40 |     2/    5 batches | lr 0.68 | ms/batch 198.16 | loss  4.85 | ppl   127.48\n",
      "| epoch  40 |     4/    5 batches | lr 0.68 | ms/batch 138.41 | loss  3.24 | ppl    25.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time:  0.79s | valid loss  5.89 | valid ppl   359.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  41 |     2/    5 batches | lr 0.64 | ms/batch 218.31 | loss  4.84 | ppl   126.44\n",
      "| epoch  41 |     4/    5 batches | lr 0.64 | ms/batch 140.86 | loss  3.23 | ppl    25.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  41 | time:  0.85s | valid loss  5.90 | valid ppl   366.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  42 |     2/    5 batches | lr 0.61 | ms/batch 209.05 | loss  4.81 | ppl   122.27\n",
      "| epoch  42 |     4/    5 batches | lr 0.61 | ms/batch 138.33 | loss  3.22 | ppl    24.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  42 | time:  0.80s | valid loss  5.92 | valid ppl   372.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  43 |     2/    5 batches | lr 0.58 | ms/batch 216.95 | loss  4.76 | ppl   116.91\n",
      "| epoch  43 |     4/    5 batches | lr 0.58 | ms/batch 135.48 | loss  3.19 | ppl    24.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  43 | time:  0.82s | valid loss  5.92 | valid ppl   372.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  44 |     2/    5 batches | lr 0.55 | ms/batch 221.64 | loss  4.74 | ppl   114.46\n",
      "| epoch  44 |     4/    5 batches | lr 0.55 | ms/batch 137.20 | loss  3.18 | ppl    23.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  44 | time:  0.84s | valid loss  5.93 | valid ppl   376.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  45 |     2/    5 batches | lr 0.52 | ms/batch 213.87 | loss  4.71 | ppl   110.84\n",
      "| epoch  45 |     4/    5 batches | lr 0.52 | ms/batch 143.98 | loss  3.18 | ppl    23.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  45 | time:  0.83s | valid loss  5.94 | valid ppl   380.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  46 |     2/    5 batches | lr 0.50 | ms/batch 203.44 | loss  4.70 | ppl   110.24\n",
      "| epoch  46 |     4/    5 batches | lr 0.50 | ms/batch 147.63 | loss  3.15 | ppl    23.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  46 | time:  0.81s | valid loss  5.95 | valid ppl   383.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  47 |     2/    5 batches | lr 0.47 | ms/batch 226.07 | loss  4.67 | ppl   106.59\n",
      "| epoch  47 |     4/    5 batches | lr 0.47 | ms/batch 150.05 | loss  3.14 | ppl    23.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  47 | time:  0.88s | valid loss  5.97 | valid ppl   390.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  48 |     2/    5 batches | lr 0.45 | ms/batch 226.32 | loss  4.62 | ppl   101.19\n",
      "| epoch  48 |     4/    5 batches | lr 0.45 | ms/batch 139.53 | loss  3.13 | ppl    22.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  48 | time:  0.85s | valid loss  5.97 | valid ppl   392.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  49 |     2/    5 batches | lr 0.43 | ms/batch 209.36 | loss  4.66 | ppl   105.60\n",
      "| epoch  49 |     4/    5 batches | lr 0.43 | ms/batch 138.28 | loss  3.11 | ppl    22.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  49 | time:  0.81s | valid loss  5.98 | valid ppl   396.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  50 |     2/    5 batches | lr 0.40 | ms/batch 212.12 | loss  4.62 | ppl   101.07\n",
      "| epoch  50 |     4/    5 batches | lr 0.40 | ms/batch 129.61 | loss  3.08 | ppl    21.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  50 | time:  0.80s | valid loss  5.98 | valid ppl   396.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  51 |     2/    5 batches | lr 0.38 | ms/batch 205.63 | loss  4.59 | ppl    98.52\n",
      "| epoch  51 |     4/    5 batches | lr 0.38 | ms/batch 139.99 | loss  3.06 | ppl    21.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  51 | time:  0.80s | valid loss  6.00 | valid ppl   403.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  52 |     2/    5 batches | lr 0.37 | ms/batch 204.51 | loss  4.57 | ppl    96.44\n",
      "| epoch  52 |     4/    5 batches | lr 0.37 | ms/batch 156.48 | loss  3.07 | ppl    21.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  52 | time:  0.84s | valid loss  6.01 | valid ppl   408.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  53 |     2/    5 batches | lr 0.35 | ms/batch 206.20 | loss  4.55 | ppl    94.18\n",
      "| epoch  53 |     4/    5 batches | lr 0.35 | ms/batch 142.31 | loss  3.07 | ppl    21.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  53 | time:  0.82s | valid loss  6.01 | valid ppl   406.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  54 |     2/    5 batches | lr 0.33 | ms/batch 209.91 | loss  4.56 | ppl    95.31\n",
      "| epoch  54 |     4/    5 batches | lr 0.33 | ms/batch 139.43 | loss  3.05 | ppl    21.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  54 | time:  0.82s | valid loss  6.02 | valid ppl   409.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  55 |     2/    5 batches | lr 0.31 | ms/batch 228.34 | loss  4.56 | ppl    95.23\n",
      "| epoch  55 |     4/    5 batches | lr 0.31 | ms/batch 133.93 | loss  3.04 | ppl    21.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  55 | time:  0.84s | valid loss  6.02 | valid ppl   412.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  56 |     2/    5 batches | lr 0.30 | ms/batch 213.16 | loss  4.51 | ppl    90.94\n",
      "| epoch  56 |     4/    5 batches | lr 0.30 | ms/batch 154.81 | loss  3.02 | ppl    20.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  56 | time:  0.87s | valid loss  6.03 | valid ppl   415.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  57 |     2/    5 batches | lr 0.28 | ms/batch 216.32 | loss  4.50 | ppl    89.73\n",
      "| epoch  57 |     4/    5 batches | lr 0.28 | ms/batch 145.44 | loss  3.04 | ppl    20.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  57 | time:  0.84s | valid loss  6.03 | valid ppl   416.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  58 |     2/    5 batches | lr 0.27 | ms/batch 206.66 | loss  4.51 | ppl    90.63\n",
      "| epoch  58 |     4/    5 batches | lr 0.27 | ms/batch 148.72 | loss  3.02 | ppl    20.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  58 | time:  0.83s | valid loss  6.04 | valid ppl   420.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  59 |     2/    5 batches | lr 0.26 | ms/batch 262.73 | loss  4.51 | ppl    90.74\n",
      "| epoch  59 |     4/    5 batches | lr 0.26 | ms/batch 180.60 | loss  3.03 | ppl    20.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  59 | time:  1.04s | valid loss  6.05 | valid ppl   422.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  60 |     2/    5 batches | lr 0.24 | ms/batch 277.14 | loss  4.49 | ppl    88.75\n",
      "| epoch  60 |     4/    5 batches | lr 0.24 | ms/batch 189.08 | loss  3.00 | ppl    20.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  60 | time:  1.07s | valid loss  6.05 | valid ppl   423.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  61 |     2/    5 batches | lr 0.23 | ms/batch 255.94 | loss  4.48 | ppl    88.66\n",
      "| epoch  61 |     4/    5 batches | lr 0.23 | ms/batch 193.43 | loss  2.99 | ppl    19.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  61 | time:  1.06s | valid loss  6.05 | valid ppl   425.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  62 |     2/    5 batches | lr 0.22 | ms/batch 354.97 | loss  4.45 | ppl    85.99\n",
      "| epoch  62 |     4/    5 batches | lr 0.22 | ms/batch 155.15 | loss  2.98 | ppl    19.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  62 | time:  1.17s | valid loss  6.06 | valid ppl   426.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  63 |     2/    5 batches | lr 0.21 | ms/batch 216.80 | loss  4.42 | ppl    83.29\n",
      "| epoch  63 |     4/    5 batches | lr 0.21 | ms/batch 139.02 | loss  2.96 | ppl    19.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  63 | time:  0.82s | valid loss  6.06 | valid ppl   430.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  64 |     2/    5 batches | lr 0.20 | ms/batch 200.78 | loss  4.46 | ppl    86.91\n",
      "| epoch  64 |     4/    5 batches | lr 0.20 | ms/batch 138.03 | loss  2.97 | ppl    19.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  64 | time:  0.78s | valid loss  6.07 | valid ppl   431.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  65 |     2/    5 batches | lr 0.19 | ms/batch 207.71 | loss  4.44 | ppl    85.10\n",
      "| epoch  65 |     4/    5 batches | lr 0.19 | ms/batch 133.04 | loss  2.96 | ppl    19.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  65 | time:  0.79s | valid loss  6.07 | valid ppl   433.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  66 |     2/    5 batches | lr 0.18 | ms/batch 204.39 | loss  4.45 | ppl    85.91\n",
      "| epoch  66 |     4/    5 batches | lr 0.18 | ms/batch 131.32 | loss  2.96 | ppl    19.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  66 | time:  0.78s | valid loss  6.08 | valid ppl   436.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  67 |     2/    5 batches | lr 0.17 | ms/batch 206.22 | loss  4.40 | ppl    81.71\n",
      "| epoch  67 |     4/    5 batches | lr 0.17 | ms/batch 135.45 | loss  2.96 | ppl    19.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  67 | time:  0.79s | valid loss  6.07 | valid ppl   434.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  68 |     2/    5 batches | lr 0.16 | ms/batch 198.85 | loss  4.43 | ppl    83.63\n",
      "| epoch  68 |     4/    5 batches | lr 0.16 | ms/batch 165.14 | loss  2.99 | ppl    19.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  68 | time:  0.88s | valid loss  6.08 | valid ppl   436.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  69 |     2/    5 batches | lr 0.15 | ms/batch 208.20 | loss  4.41 | ppl    82.59\n",
      "| epoch  69 |     4/    5 batches | lr 0.15 | ms/batch 139.70 | loss  2.96 | ppl    19.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  69 | time:  0.82s | valid loss  6.08 | valid ppl   437.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  70 |     2/    5 batches | lr 0.15 | ms/batch 211.56 | loss  4.41 | ppl    82.28\n",
      "| epoch  70 |     4/    5 batches | lr 0.15 | ms/batch 138.42 | loss  2.96 | ppl    19.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  70 | time:  0.82s | valid loss  6.08 | valid ppl   439.13\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs = 70\n",
    "\n",
    "with TemporaryDirectory() as tempdir:\n",
    "    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(model)\n",
    "        val_loss = evaluate(model, val_data)\n",
    "        val_ppl = math.exp(val_loss)\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "        print('-' * 89)\n",
    "        print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "            f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "        print('-' * 89)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "        scheduler.step()\n",
    "    model.load_state_dict(torch.load(best_model_params_path)) # load best model states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like this phone\n",
      "['i', 'like', 'this', 'phone']\n",
      "tensor([[ 3, 56,  8, 13]])\n",
      "torch.Size([1, 4])\n",
      "torch.int64\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 4, 1370])\n",
      "torch.Size([4, 1370])\n",
      "tensor([[-0.6971,  1.9899,  1.9780,  ...,  0.0844, -0.1582, -0.0935],\n",
      "        [ 0.0340,  5.3120,  7.1593,  ..., -0.4306,  0.1480, -0.5205],\n",
      "        [ 0.2668,  5.0340,  2.4351,  ..., -0.0494, -1.0521, -0.9537],\n",
      "        [-0.8155,  7.3042,  2.9632,  ..., -1.0899,  0.6805, -0.0595]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "inp = \"I like this phone\"\n",
    "tokens = tokenizer(inp)\n",
    "embed = torch.tensor([vocab(tokens)], dtype=torch.long)\n",
    "print(inp)\n",
    "print(tokens)\n",
    "print(embed)\n",
    "print(embed.shape)\n",
    "print(embed.dtype)\n",
    "print(type(embed))\n",
    "output = model(embed)\n",
    "print(output.shape)\n",
    "output_flat = output.view(-1, ntokens)\n",
    "print(output_flat.shape)\n",
    "print(output_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFF(nn.Module):\n",
    "    def __init__(self,vocab_sz, encoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.linear1=nn.Linear(vocab_sz,100)\n",
    "        self.linear2=nn.Linear(100,10)\n",
    "        self.linear3=nn.Linear(10,2)\n",
    "        self.sm = nn.Softmax()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.encoder(x)\n",
    "        x = torch.mean(x.view(-1, ntokens),dim=0)\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.sm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1370])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Size([1, 4, 1370])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9346, 0.3909, 0.5329, 0.2755, 0.5809, 0.2202, 0.9995, 0.4187],\n",
      "        [0.2758, 0.9138, 0.3508, 0.8425, 0.5495, 0.7500, 0.1883, 0.6494],\n",
      "        [0.0229, 0.4548, 0.8889, 0.8081, 0.4092, 0.6145, 0.7072, 0.9636],\n",
      "        [0.5418, 0.4581, 0.7521, 0.3205, 0.6139, 0.8473, 0.0696, 0.2117]])\n",
      "tensor([0.9346, 0.3909, 0.5329, 0.2755, 0.5809, 0.2202, 0.9995, 0.4187])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((4,8))\n",
    "print(x)\n",
    "x = x[0,:]\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this allows the possibility of double booking for the same date and time after the first.'\n",
      " 'my sister has one also and she loves it.'\n",
      " \"the one big drawback of the mp player is that the buttons on the phone's front cover that let you pause and skip songs lock out after a few seconds.\"\n",
      " 'the cutouts and buttons are placed perfectly.'\n",
      " 'this is definitely a must have if your state does not allow cell phone usage while driving.'\n",
      " 'these are fabulous!' 'nice sound.'\n",
      " \"i can't use this case because the smell is disgusting.\"\n",
      " 'i really like this product over the motorola because it is allot clearer on the ear piece and the mic.'\n",
      " 'fast service.' 'i found this product to be waaay too big.'\n",
      " \"it plays louder than any other speaker of this size; the price is so low that most would think the quality is lacking, however, it's not.\"\n",
      " 'no buyers remorse on this one!.'\n",
      " 'i had to go to a store and bought a new nokia phone which is working great.'\n",
      " 'poor quality and service.'\n",
      " 'the worst piece of crap ever along with the verizon customer service.'\n",
      " \"no shifting, no bubbling, no peeling, not even a scratch, nothing!i couldn't be more happier with my new one for the droid.\"\n",
      " 'same problem as others have mentioned.'\n",
      " 'this results in the phone being either stuck at max volume or mute.'\n",
      " 'i got the aluminum case for my new palm vx and it worked really well--it has protected my handheld perfectly so far.'\n",
      " 'i am sorry i made this purchase.'\n",
      " 'problem is that the ear loops are made of weak material and break easily.'\n",
      " 'iam very pleased with my purchase.'\n",
      " 'i have had mine for about a year and this christmas i bought some for the rest of the family.'\n",
      " \"essentially you can forget microsoft's tech support.\"\n",
      " \"it's not what it says it is.\" 'battery life is also great!'\n",
      " 'i am very impressed with this headset from plantronics.'\n",
      " 'i like the fact that it rests lightly against your ear, rather than inside.'\n",
      " 'it is cheap, and it feel and look just as cheap.'\n",
      " 'plug was the wrong size.' \"don't bother - go to the store.\"\n",
      " 'piece of trash.' 'jabra ear gels \"r\" the best!.'\n",
      " 'cheap but hey it works.. was pleasantly suprised given the low cost of this item.'\n",
      " \"all the other cases i've tried normally fall apart after a few months but this one seems to be in for the long haul.\"\n",
      " 'also its slim enough to fit into my alarm clock docking station without removing the case.'\n",
      " 'needless to say, i wasted my money.' 'razr battery - good buy.'\n",
      " 'excellent dual-purpose headset.'\n",
      " 'buyer beware, you could flush money right down the toilet.'\n",
      " 'the charger worked for about a week and then completely stopped charging my phone.'\n",
      " 'my only complaint is the standard sound volume is a little low even when turned up to (of )'\n",
      " 'it is very comfortable to wear as well, which is probably the most important aspect about using a case.'\n",
      " 'best of all is the rotating feature, very helpful.'\n",
      " \"also the area where my unit broke).- i'm not too fond of the magnetic strap.\"\n",
      " 'magical help.' \"better than you'd expect.\" 'i love this thing!'\n",
      " 'car charger as well as ac charger are included to make sure you never run out of juice.highy recommended'\n",
      " 'worked perfectly!'\n",
      " 'this is a simple little phone to use, but the breakage is unacceptible.'\n",
      " 'very pleased with this headset.'\n",
      " 'the nano stated it.my son was dissapointed.'\n",
      " 'it is the best charger i have seen on the market yet.'\n",
      " \"i'm really disappointed all i have now is a charger that doesn't work.\"\n",
      " 'nice solid keyboard.'\n",
      " 'it is light, easy to use, and has very clear reception and transmission.'\n",
      " 'last time buying from you.' \"you won't regret it!\"\n",
      " 'it works great with a car charger, especially if you cannot plug in two adapters at the same time.'\n",
      " 'you also cannot take pictures with it in the case because the lense is covered.'\n",
      " 'too bad you have to pay up to $$$ a month for the service!'\n",
      " 'but, in any case, the best part is, you can download these pictures to your laptop using ir, or even send pictures from your laptop to the phone.'\n",
      " 'but it is great, i would really recommend it'\n",
      " 'clear skype calls, long battery life, long range.'\n",
      " 'i am also very happy with the price.'\n",
      " 'the mic there is a joke, and the volume is quite low.'\n",
      " 'works like a charm.. works as advertised.'\n",
      " 'att is not clear, sound is very distorted and you have to yell when you talk.'\n",
      " 'it is easy to turn on and off when you are in the car and the volume controls are quite accessable.'\n",
      " 'perfect for the ps.'\n",
      " \"i got this phone on reccomendation from a relative and i'm glad i did.\"\n",
      " 'not as good as i had hoped.'\n",
      " 'i would definitely recommend the jabra btv for those who are looking for comfort, clarity and a great price!'\n",
      " 'disappointing.' 'big disappointment with calendar sync.'\n",
      " 'when i placed my treo into the case, not only was it not snug, but there was a lot of extra room on the sides.'\n",
      " \"i didn't want the clip going over the top of my ear, causing discomfort.\"\n",
      " 'the phone takes forever to charge like  to  hours literally.'\n",
      " 'i really wanted the plantronics  to be the right one, but it has too many issues for me.the good'\n",
      " 'you can not answer calls with the unit, never worked once!'\n",
      " 'very disappointed in accessoryone.'\n",
      " \"it's uncomfortable and the sound quality is quite poor compared with the phone (razr) or with my previous wired headset (that plugged into an lg).\"\n",
      " 'the text messaging feature is really tricky to use.' 'happy so far!.'\n",
      " 'unfortunately it will not recharge my iphone s, despite connecting it from multiple power sources (imac, external battery, wall outlet, etc).'\n",
      " 'very well made and fits my surefire gx perfectly.'\n",
      " 'the reception has been generally good.'\n",
      " 'battery lasts only a few hours.'\n",
      " 'however, the ear pads come off easily and after only one week i lost one.'\n",
      " 'dont buy it.' 'warning - stay away.'\n",
      " 'i have to jiggle the plug to get it to line up right to get decent volume.'\n",
      " 'crisp and clear.'\n",
      " 'the volume for the ringer is real good (you have choices how loud).'\n",
      " 'this is simply the best bluetooth headset for sound quality!'\n",
      " 'i would highly recommend this.'\n",
      " 'utter crap.. sound quality is terrible.' 'i was hoping for more.'\n",
      " 'i would advise to not purchase this item it never worked very well.'\n",
      " 'the bose noise cancelling is amazing, which is very important for a nyc commuter.'\n",
      " 'the reception is excellent!' 'not enough volume.'\n",
      " \"gets a signal when other verizon phones won't.\"\n",
      " 'steer clear of this product and go with the genuine palm replacementr pens, which come in a three-pack.'\n",
      " 'battery has no life.'\n",
      " 'i am very impressed with the job that motorola did on the sturdiness of this phone.'\n",
      " 'the phone is sturdy and waterproof.'\n",
      " 'a lot of websites have been rating this a very good phone and so do i.'\n",
      " 'great product, fast shipping!.'\n",
      " 'i like design and look of jabra behing the ear headsets and  is pretty comfortible to wear  hours a day without pain in the ear.'\n",
      " 'poor talk time performance.'\n",
      " 'this product is very high quality chinese crap!!!!!!'\n",
      " 'it was quite comfortable in the ear.' 'lasted one day and then blew up.'\n",
      " 'and i just love the colors!'\n",
      " \"i've bought $ wired headphones that sound better than these.\"\n",
      " 'it has a great camera thats mp, and the pics are nice and clear with great picture quality.'\n",
      " 'it feels poorly constructed, the menus are difficult to navigate, and the buttons are so recessed that it is difficult to push them.'\n",
      " 'does not work.' 'they are so cool!' \"it's pretty easy.\"\n",
      " 'returned  hours later.'\n",
      " \"it's so stupid to have to keep buying new chargers, car chargers, cradles, headphones and car kits every time a new phone comes out.\"\n",
      " 'the voice recognition thru the handset is excellent.'\n",
      " 'poor construction.'\n",
      " 'i ended up sliding it on the edge of my pants or back pockets instead.'\n",
      " 'works great!.'\n",
      " 'overall, i am psyched to have a phone which has all my appointments and contacts in and gets great reception.'\n",
      " 'the noise shield is incrediable.'\n",
      " 'my -year old nokia  from tracfone holds the charge a lot better than this.'\n",
      " 'this is a great product..... sure beats using your fingers!.'\n",
      " 'the internet access was fine, it the rare instance that it worked.'\n",
      " \"these headphones were a great find - and i think they are perhaps the best purchase i've made in the last several years - seriously.\"\n",
      " 'i purchased this and within  days it was no longer working!!!!!!!!!'\n",
      " 'and none of the tones is acceptable.'\n",
      " 'i have had problems wit hit dropping signal and more.'\n",
      " 'it is light, has plenty of battery capacity, and is very confortable to wear for somewhat extended periods of time.'\n",
      " \"i've had this bluetoooth headset for some time now and still not comfortable with the way it fits on the ear.\"\n",
      " 'all it took was one drop from about  inches above the kitchen counter and it was cracked.i am not impressed and i am not laughing.'\n",
      " 'i love this phone!.'\n",
      " 'the color is even prettier than i thought it would be, and the graphics are incredibly sharp.'\n",
      " 'it definitely was not as good as my s.' 'painful on the ear.'\n",
      " 'cant get the software to work with my computer.'\n",
      " 'the only thing that i think could improve is the sound leaks out from the headset.'\n",
      " 'great for the jawbone.'\n",
      " 'it holds a charge for a long time, is reasonably comfortable under long-wearing conditions and the quality of sound is tremendous.'\n",
      " 'do not buy if you want to use the holster.'\n",
      " 'i love all the features and form factor.'\n",
      " 'looks good in the picture, but this case was a huge disappointment!!'\n",
      " 'the headsets are easy to use and everyone loves them.'\n",
      " \"i'm trying to return it for a refund.\" 'love this product.'\n",
      " 'they work about  weeks then break.'\n",
      " 'talk about useless customer service.' 'the battery works great!'\n",
      " 'much better than the hard plastic cases.'\n",
      " 'the case is great and works fine with the .'\n",
      " 'i does not maintain a connection with the computer while it is on my lap.'\n",
      " 'i have always used corded headsets and the freedom from the wireless is very helpful.'\n",
      " 'i wish i could return the unit and get back my money.' 'what a waste.'\n",
      " \"doesn't do the job.\" 'those phones are working just fine now.'\n",
      " 'fantastic buy and will get again for whatever my next phone is'\n",
      " 'the update procedure is difficult and cumbersome.'\n",
      " 'a piece of junk that broke after being on my phone for  days!!!'\n",
      " 'an awesome new look for fall !.'\n",
      " 'it lasts less than o minutes, if i actually try to use the phone.my wife has the same phone with the same problem.'\n",
      " 'i have purchased these for both family and friends, and all enjoy their clarity and ease of use'\n",
      " \"not loud enough and doesn't turn on like it should.\"\n",
      " 'this is essentially a communications tool that does not communicate.'\n",
      " 'in addition it feels &amp; looks as if the phone is all lightweight cheap plastic.'\n",
      " 'poor reliability.' 'nice case, feels good in your hands.'\n",
      " 'works great!.' 'restored my phone to like new performance.'\n",
      " 'how can that be?the audio quality is poor.'\n",
      " 'just reading on the specs alone makes you say wow.' 'very comfortable.'\n",
      " 'comfortable, nice range, good battery life.' '(it works!)'\n",
      " 'but now that it is \"out of warranty\" the same problems reoccure.bottom line... put your money somewhere else... cingular will not support it.'\n",
      " 'worthless product.'\n",
      " \"i have read other's reviews here but i haven't had any problem with it.\"\n",
      " \"this is by far the worst purchase i've made on amazon.\"\n",
      " 'nice quality build, unlike some cheap s*** out there.'\n",
      " 'simple, lightweight and great fit.' 'very wind-resistant.'\n",
      " 'i did not bother contacting the company for few dollar product but i learned the lesson that i should not have bought this form online anyway.'\n",
      " 'stay away from this store, be careful.'\n",
      " \"i exchanged the sony ericson za for this and i'm pretty happy with that decision.\"\n",
      " \"also, the phone doesn't seem to accept anything except cbr mps, preferably ripped by windows media player.\"\n",
      " 'the look of it is very sharp and the screen is nice and clear, with great graphics.'\n",
      " 'the only thing that disappoint me is the infra red port (irda).'\n",
      " 'adapter does not provide enough charging current.'\n",
      " 'it is super charged up for use as a small hybrid palmtop/camera/cellphone, and excels in those roles.'\n",
      " 'and the sound quality is great.'\n",
      " 'pros:-good camera - very nice pictures , also has cool styles like black and white, and more.'\n",
      " 'great for ipods too.' \"i'd like to return it.\"\n",
      " \"still waiting...... i'm sure this item would work well.. if i ever recieve it!\"\n",
      " \"but it does get better reception and clarity than any phone i've had before.\"\n",
      " 'excellent hands free tool.' 'good case!.'\n",
      " 'i am pairing this with my iphone, and i could not be happier with it so far.'\n",
      " 'everything worked on the first try.the device was certainly engineered in a clever way and the construction feels good.'\n",
      " '# it works - # it is comfortable.' 'logitech bluetooth headset is a !.'\n",
      " 'works good.' 'horrible, had to switch  times.'\n",
      " \"i went on motorola's website and followed all directions, but could not get it to pair again.\"\n",
      " 'not nearly as good looking as the amazon picture makes it look.'\n",
      " 'great hands free device.' 'price is good too.' 'great phone!.'\n",
      " 'even in my bmw  series which is fairly quiet, i have trouble hearing what the other person is saying.'\n",
      " 'disappointed with battery.'\n",
      " 'if you like a loud buzzing to override all your conversations, then this phone is for you!'\n",
      " 'i would not recommend this item to anyone.'\n",
      " 'as i said above....pretty useless!'\n",
      " \"i got this phone around the end of may and i'm completely unhappy with it.\"\n",
      " 'do not purchase this phone.'\n",
      " 'do not buy for d...wrongly advertised for d.'\n",
      " 'this case seems well made.' 'i would have given no star if i was able.'\n",
      " 'absolutel junk.' ' thumbs up to this seller'\n",
      " 'i have only had it for a few weeks, but so far, so good.'\n",
      " 'does not fit.'\n",
      " 'you have to hold the phone at a particular angle for the other party to hear you clearly.'\n",
      " '$ down the drain.'\n",
      " 'a usable keyboard actually turns a pda into a real-world useful machine instead of just a neat gadget.'\n",
      " 'very displeased.' 'gets the job done.' 'battery is terrible.'\n",
      " 'i came over from verizon because cingulair has nicer cell phones.... the first thing i noticed was the really bad service.'\n",
      " 'blue ant is easy to use.' 'very satisifed with that.'\n",
      " 'the volume switch rocketed out of the unit to a destination unknown.'\n",
      " 'it has been a winner for us.'\n",
      " 'if the two were seperated by a mere + ft i started to notice excessive static and garbled sound from the headset.'\n",
      " 'this item is fantastic and works perfectly!' 'very disappointing.'\n",
      " 'it did not work in my cell phone plug i am very up set with the charger!.'\n",
      " 'who in their right mind is gonna buy this battery?.'\n",
      " 'one of my favorite purchases ever.'\n",
      " 'the eargels channel the sound directly into your ear and seem to increase the sound volume and clarity.'\n",
      " 'the phone was unusable and was not new.'\n",
      " 'sprint - terrible customer service.' 'the phone gets extremely hot!'\n",
      " 'however, the keypads are so tinny that i sometimes reach the wrong buttons.'\n",
      " 'not good enough for the price.' 'great phone.' 'good item, low price.'\n",
      " 'the pairing of the two devices was so easy it barely took a couple minutes before i started making calls with the voice dialing feature.'\n",
      " 'i have a verizon lg phone and they work well together, good reception and range that exceeds  feet line of sight.'\n",
      " 'were jerks on the phone.' 'bad fit, way too big.' 'so far so good!.'\n",
      " 'design flaw?.' 'excellent product for the price.'\n",
      " 'it was an inexpensive piece, but i would still have expected better quality.'\n",
      " 'the \".\" mega pixel camera, being a part of a phone, is reasonably good.'\n",
      " 'i use this product in a motor control center where there is a lot of high voltage humming from the equipment, and it works great!'\n",
      " \"i've dropped my phone more times than i can say, even on concrete and my phone is still great (knock on wood!).\"\n",
      " 'phone now holds charge like it did when it was new.'\n",
      " 'worst customer service ever.' \"couldn't figure it out\"\n",
      " 'excellent service!!!!!!!!.' 'great phone!.'\n",
      " 'you get extra minutes so that you can carry out the call and not get cut off.\"'\n",
      " '!i definitly recommend!!' 'now i know that i made a wise decision.'\n",
      " 'none of the three sizes they sent with the headset would stay in my ears.'\n",
      " 'its extremely slow and takes forever to do anything with it.'\n",
      " 'i want my money back.'\n",
      " 'i love this phone , it is very handy and has a lot of features .'\n",
      " 'thanks again to amazon for having the things i need for a good price!'\n",
      " 'reception is terrible and full of static.'\n",
      " \"the pleather case doesn't fit.\" 'would recommend this item.'\n",
      " 'the case is a flimsy piece of plastic and has no front or side protection whatsoever.'\n",
      " 'so i had to take the battery out of the phone put it all back together and then restart it.'\n",
      " \"i only used it two days, and it wasn't always easy to hear with.\"\n",
      " 'very happy with this product.'\n",
      " 'i bought this to use with my kindle fire and absolutely loved it!'\n",
      " 'in the span of an hour, i had two people exclaim \"whoa - is that the new phone on tv?!?'\n",
      " 'it was a great phone.'\n",
      " \"it's fits like a glove and is strong, secure, and durable.\"\n",
      " 'no additional ear gels provided, and no instructions whatsoever.'\n",
      " 'this fixes all the problems.'\n",
      " 'you need at least  mins to get to your phone book from the time you first turn on the phone.battery life is short.'\n",
      " 'not a good bargain.' 'wi is just superb.'\n",
      " 'it fits comfortably in either ear, the sound is clear and loud, and the charge lasts a couple of days.'\n",
      " \"i don't like this nokia either.\" 'the delivery was on time.'\n",
      " 'a must study for anyone interested in the \"worst sins\" of industrial design.'\n",
      " 'i purcashed this for the car charger and it does not work.'\n",
      " \"i'm happy about this purchase- good quality and low price.\"\n",
      " 'soyo technology sucks.' 'very dissapointing performance.'\n",
      " 'i am more than happy with this product.' 'worked great!.'\n",
      " 'portable and it works.'\n",
      " 'the earpiece on this is too large or too heavy...it keeps falling out of my ear.'\n",
      " 'i have been very satisfied with this cell phone from day one.'\n",
      " 'just does not work.' 'exactly what i wanted.' 'that company is a joke.'\n",
      " 'there was so much hype over this phone that i assumed it was the best, my mistake.'\n",
      " 'oh and i forgot to also mention the weird color effect it has on your phone.'\n",
      " 'if you hate earbugs, avoid this phone by all means.'\n",
      " 'thank you for wasting my money.' 'great value.'\n",
      " 'i was very excited to get this headset because i thought it was really cute.'\n",
      " 'waste of  bucks.' 'easy to pair with my samsung cell.'\n",
      " 'if there is a wind, it is completely useless.' 'worst customer service.'\n",
      " \"couldn't use the unit with sunglasses, not good in texas!\"\n",
      " 'i even fully charged it before i went to bed and turned off blue tooth and wi-fi and noticed that it only had  % left in the morning.'\n",
      " 'phone is sturdy as all nokia bar phones are.'\n",
      " 'unfortunately it did not work.'\n",
      " 'this phone is very fast with sending any kind of messages and web browsing is significantly faster than previous phones i have used.'\n",
      " 'unfortunately the ability to actually know you are receiving a call is a rather important feature and this phone is pitiful in that respect.'\n",
      " 'a pretty good product.'\n",
      " \"it didn't work, people can not hear me when i talk.\" 'great audio!.'\n",
      " 'a week later after i activated it, it suddenly died.'\n",
      " \"i can hear while i'm driving in the car, and usually don't even have to put it on it's loudest setting.\"\n",
      " 'you never know if you pushed it hard enough or the right number of times for the function you want or not.'\n",
      " \"there's a horrible tick sound in the background on all my calls that i have never experienced before.\"\n",
      " \"don't waste your money!.\" 'great charger.'\n",
      " 'the jabra eargels fit my ears very well.'\n",
      " 'you need two hands to operate the screen.this software interface is decade old and cannot compete with new software designs.'\n",
      " 'bluetooth does not work, phone locks up, screens just flash up and now it just makes calls randomly while in my pocket locked.'\n",
      " 'it only recognizes the phone as its storage device.'\n",
      " 'it is a joy to use.'\n",
      " 'i have used several phone in two years, but this one is the best.'\n",
      " 'integrated seamlessly with the motorola razr phone.'\n",
      " 'arrived quickly and much less expensive than others being sold.'\n",
      " 'works well.'\n",
      " 'i ordered this for sony ericsson wi but i think it only worked once (thats when i first used it).'\n",
      " \"very clear, quality sound and you don't have to mess with the sound on your ipod since you have the sound buttons on the headset.\"\n",
      " \"this phone might well be the worst i've ever had in any brand.\"\n",
      " '* comes with a strong light that you can use to light up your camera shots, and even flash sos signals (seriously!'\n",
      " 'does not work for listening to music with the cingular .'\n",
      " 'after charging overnight, these batteries work great.'\n",
      " 'saggy, floppy piece of junk.' 'freezes frequently.'\n",
      " 'the picture resolution is far below what other comparably-priced phones are offering today.'\n",
      " 'it always cuts out and makes a beep beep beep sound then says signal failed.'\n",
      " \"another note about this phone's appearance is that it really looks rather bland, especially in the all black model.\"\n",
      " 'they do not care about the consumer one bit.' 'great bluetooth!.'\n",
      " 'when it opens, the battery connection is broken and the device is turned off.'\n",
      " 'highly recommend for any one who has a blue tooth phone.'\n",
      " 'i had verizon  years ago and really liked their service.'\n",
      " 'worth every penny.' 'everything about this product is wrong.first'\n",
      " 'if you have several dozen or several hundred contacts, then imagine the fun of sending each of them one by one.'\n",
      " 'i was amazed at the quick arrival of the two original lg cell phone batteries and and at a fraction of the price.'\n",
      " 'so i basically threw my money out the window for nothing.'\n",
      " 'great product and price.'\n",
      " \"my phone doesn't slide around my car now and the grip prevents my phone from slipping out of my hand.\"\n",
      " 'if i take a picture, the battery drops a bar, and starts beeping, letting me know its dieing.'\n",
      " 'clear crisp sound.'\n",
      " 'i received my supposedly new motorola  and apparently there was not a match between the phone and the charger.'\n",
      " 'good price.'\n",
      " 'i own a jabra earset and was very happy with it, but the sound quality, especially outgoing, on this is better.'\n",
      " 'after receiving and using the product for just  days it broke.'\n",
      " 'good transmit audio.' 'someone shouldve invented this sooner.'\n",
      " 'good protection and does not make phone too bulky.'\n",
      " 'for a product that costs as much as this one does, i expect it to work far better and with greater ease than this thing does.'\n",
      " 'excellent starter wireless headset.'\n",
      " 'i did not have any problem with this item and would order it again if needed.'\n",
      " 'bt battery junk!.' 'the commercials are the most misleading.'\n",
      " 'not a good item.. it worked for a while then started having problems in my auto reverse tape player.'\n",
      " 'worst software ever used.... if i could give this zero stars i would.'\n",
      " \"if you don't find it, too bad, as again the unit comes with one earpad only.i would not recommend this to anyone.\"\n",
      " 'worst phone ever.'\n",
      " 'the tracfonewebsite is user friendly and makes it easy toactivate, etc.'\n",
      " 'the sound quality for the device is unacceptable.unless you are in a really quiet area, you almost hear nothing.'\n",
      " 'this is infuriating.' 'for the price this was a great deal.'\n",
      " 'nice leather.' 'i love the ringtones because they are so upbeat!'\n",
      " 'great headset, very impressed - h.' 'great phone.'\n",
      " 'the replacement died in a few weeks.'\n",
      " 'also, if your phone is dropped, this case is not going to save it, specially when dropped face down.'\n",
      " 'does everything it should and more.'\n",
      " 'this is so embarassing and also my ears hurt if i try to push the ear plug into my ear.'\n",
      " \"i posted more detailed comments under the grey or black phone, but i have the fire red and it's a great color!\"\n",
      " 'incredible!.' 't-mobile has the best customer service anywhere.'\n",
      " \"don't waste your money and time.\" 'not impressed.'\n",
      " 'yet plantronincs continues to use the same flawed charger design.'\n",
      " 'very prompt service.' 'keep up the good work amazon!!'\n",
      " 'finally, after three or four times the spring of the latch broke and i could not use it any longer on the visor.'\n",
      " 'i might have gotten a defect, but i would not risk buying it again because of the built quality alone.'\n",
      " 'headset works great & was packaged nicely to avoid any damage.'\n",
      " 'great sound and service.' 'the igo chargers and tips are really great.'\n",
      " \"it's really easy.\"\n",
      " 'great it was new packaged nice works good, no problems and it came in less time then i expected!!!!'\n",
      " 'beautiful styling though.' 'the sound quality is excellent as well.'\n",
      " \"don't make the same mistake that i did and please don't buy this phone.\"\n",
      " 'if you plan to use this in a car forget about it.'\n",
      " 'i bought these hoping i could make my bluetooth headset fit better but these things made it impossible to wear.'\n",
      " 'well im satisfied.' 'this is a beautiful phone.'\n",
      " 'the phone loads super!' 'it fits my ear well and is comfortable on.'\n",
      " 'so there is no way for me to plug it in here in the us unless i go by a converter.'\n",
      " 'this is a great deal.'\n",
      " \"uncomfortable in the ear, don't use with lg vx (env).\"\n",
      " \"i tried talking real loud but shouting on the telephone gets old and i was still told it wasn't great.\"\n",
      " 'i am not impressed with this and i would not recommend this item to anyone.'\n",
      " 'i love my  headset.. my jabra bluetooth headset is great, the reception is very good and the ear piece is a comfortable fit.'\n",
      " 'i bought this phone as a replacement for my startac and have regretted it since.'\n",
      " 'not only did the software do a great job of this, i could also make my own ringtones form my existing cds without an internet connection.'\n",
      " \"all in all, i'd expected a better consumer experience from motorola.\"\n",
      " 'the plug did not work very well.' 'definitely a bargain.'\n",
      " 'so just beware.' 'very easy to use.'\n",
      " \"the range is very decent, i've been able to roam around my house with the phone in the living room with no reception/sound quality issues.\"\n",
      " 'the majority of the logitech earbud headsets failed.'\n",
      " 'excellent wallet type phone case.'\n",
      " 'i plugged it in only to find out not a darn thing worked.'\n",
      " 'fits comfortably, came with three sizes of earbud tips.'\n",
      " 'worked very well paired with a treo w and as a skype headset on my pc (using a usb bluetooth transceiver).'\n",
      " 'i checked everywhere and there is no feature for it which is really disappointing.'\n",
      " 'awesome device.' 'battery charge-life is quite long.'\n",
      " \"i've owned this phone for  months now and can say that it's the best mobile phone i've had.\"\n",
      " 'obviously they have a terrible customer service, so you get what you pay for.'\n",
      " \"this is the first phone i've had that has been so cheaply made.\"\n",
      " 'how stupid is that?' 'it also had a new problem.'\n",
      " 'tied to charger for conversations lasting more than  minutes.major problems!!'\n",
      " 'buyer--be very careful!!!!!.'\n",
      " 'the loudspeaker option is great, the bumpers with the lights is very ... appealing.'\n",
      " 'excellent!.' 'great product.' 'horrible, horrible protector.'\n",
      " 'please dont waste your money.' 'excellent product for the price.'\n",
      " 'these are certainly very comfortable and functionality is decent.'\n",
      " 'plan on ordering from them again and again.'\n",
      " \"i was looking for this headset for a long time and now that i've got it i couldn't be happier.\"\n",
      " 'camera color balance is awful.'\n",
      " 'it is practically useless and did not add any kind of boost to my reception after i bought it.'\n",
      " 'the plastic breaks really easy on this clip.' 'really good product.'\n",
      " 'very much disappointed with this company.'\n",
      " \"i've also had problems with the phone reading the memory card in which i always turn it on and then off again.\"\n",
      " 'buy a different phone - but not this.'\n",
      " 'its the best headset i have used.' 'nice design and quality.'\n",
      " 'authentic leather with nice shine and comfort .i recommend you this case !!'\n",
      " 'its a little geeky but i think thats its sex on toast and it rocks and oozes sex right down to its battery embedded sleek stylish leather case.'\n",
      " 'performed awful -- muffled, tinny incoming sound and severe echo for those on the other end of the call.'\n",
      " \"unfortunately it's easy to accidentally activate them with the gentle-touch buttons if you accidentally touch the phone to your face while listening.\"\n",
      " 'basically the service was very bad.'\n",
      " 'great for using with your home stereo.'\n",
      " 'it dit not work most of the time with my nokia .'\n",
      " 'they have been around for years and carries the highest quality of anti-glare screen protector that i have found to date.'\n",
      " 'my headset works just peachy-keen.'\n",
      " 'if you are razr owner...you must have this!' 'defective crap.'\n",
      " 'all three broke within two months of use.' 'just what i wanted.'\n",
      " \"this is an excellent tool, especially when paired with your phone's auto-answer.\"\n",
      " 'works great.'\n",
      " 'i have to use the smallest earpieces provided, but it stays on pretty well.'\n",
      " 'their network coverage in los angeles is horrible.'\n",
      " \"i know that sounds funny, but to me it seemed like sketchy technology that wouldn't work well.well, this one works great.\"\n",
      " 'cumbersome design.'\n",
      " 'perhaps my phone is defective, but people cannot hear me when i use this.'\n",
      " 'new battery works great in phone.' 'best headset ever!!!.'\n",
      " 'item does not match picture.' 'the shipping time was also very quick!'\n",
      " 'very disappointed.' 'what a disappointment' 'worst ever.'\n",
      " 'great software for motorolas.'\n",
      " 'i find this inexcusable and so will probably be returning this phone and perhaps changing carriers.'\n",
      " 'i found the product to be easy to set up and use.'\n",
      " 'setup went very smoothly.' 'i wear glasses and it fits fine with them.'\n",
      " 'this was utterly confusing at first, which caused me to lose a couple of very, very important contacts.'\n",
      " \"i'm pleased.\"\n",
      " 'this pair of headphones is the worst that i have ever had sound-wise.'\n",
      " 'sending it back.'\n",
      " 'then i had to continue pairing it periodically since it somehow kept dropping.'\n",
      " 'i could not recommend these more.'\n",
      " 'my experience was terrible..... this was my fourth bluetooth headset, and while it was much more comfortable than my last jabra (which i hated!!!'\n",
      " 'works like a charm; it work the same as the one i got with the phone.'\n",
      " 'obviously there is a problem with the adhesive.'\n",
      " 'the buttons for on and off are bad.'\n",
      " 'that being said, for a phone, the camera is very nice; many cool effects to play with, and video is decent as well.'\n",
      " 'does not charge the cingular (att)  phone.' 'this pda rocks.'\n",
      " 'it is simple to use and i like it.'\n",
      " \"there's really nothing bad i can say about this headset.\"\n",
      " 'lightweight and works well.' 'awsome device works great!!.'\n",
      " \"if you are looking for a good quality motorola headset keep looking, this isn't it.\"\n",
      " 'good value, works fine - power via usb, car, or wall outlet.'\n",
      " 'the speaker is of low quality so as making the ring tones sound very cheap.'\n",
      " \"i'm still infatuated with this phone.\"\n",
      " 'it works fine on my motorola  cellphone, and a lot better than the worn-out battery did.'\n",
      " 'battery is holding up well.' \"first of all, it doesn't wear well.\"\n",
      " 'this little device has transformed my organizational capability and made my life a whole lot easier.'\n",
      " \"the instructions didn't explain that a microphone jack could be used.\"\n",
      " 'the mic is great.'\n",
      " 'the screen does get smudged easily because it touches your ear and face.'\n",
      " 'disappointed!.' 'i highly recommend this case.'\n",
      " \"this product is clearly not ready for prime time, i don't care how cool it looks, if you can't tell a call is coming in it's worthless.\"\n",
      " 'it seems completely secure, both holding on to my belt, and keeping the iphone inside.'\n",
      " 'i received my orders well within the shipping timeframe, everything was in good working order and overall, i am very excited to have this source.'\n",
      " 'i am very happy'\n",
      " \"i bought this battery with a coupon from amazon and i'm very happy with my purchase.\"\n",
      " 'bad reception.' 'great case and price!'\n",
      " 'i love being able to use one headset for both by land-line and cell.'\n",
      " 'i wasted my little money with this earpiece.'\n",
      " 'i got it because it was so small and adorable.'\n",
      " 'i have had tmobile service for  or  years now, and i am pretty happy with it.'\n",
      " 'every thing on phone work perfectly, she like it.'\n",
      " 'i love this bluetooth!'\n",
      " \"i've tried several different earpieces for my cell phone and this jabra one is the first one i've found that fits my ear comfortably.\"\n",
      " 'easy to use.'\n",
      " 'you could only take  videos at a time and the quality was very poor.'\n",
      " 'very good quality though'\n",
      " \"no ear loop needed, it's tiny and the sound is great.\"\n",
      " 'not good when wearing a hat or sunglasses.' 'better than expected.'\n",
      " 'buttons are too small.'\n",
      " 'all i can do is whine on the internet, so here it goes.the more i use the thing the less i like it.'\n",
      " ':-)oh, the charger seems to work fine.' 'lousy product.'\n",
      " 'the phone crashed completely and now i have to get it replaced.'\n",
      " 'if you like a loud buzzing to override all your conversations, then this phone is for you!'\n",
      " 'light weight, i hardly notice it is there.'\n",
      " 'any ring tone..overall good phone to buy..'\n",
      " 'case was more or less an extra that i originally put on but later discarded because it scratched my ear.'\n",
      " 'bluetooth range is good - a few days ago i left my phone in the trunk, got a call, and carried the conversation without a hitch.'\n",
      " 'the bottowm line...another worthless, cheap gimmick from sprint.'\n",
      " \"i still maintain that monkeys shouldn't make headphones, we just obviously don't share enough dna to copy the design over to humans.\"\n",
      " \"that's a huge design flaw (unless i'm not using it correctly, which i don't think is the case).\"\n",
      " 'i advise everyone do not be fooled!'\n",
      " 'i can barely ever hear on it and am constantly saying \"what?\"'\n",
      " 'motorola finally got the voice quality of a bluetooth headset right.'\n",
      " 'poor quality.' 'i am glad i purchased it.'\n",
      " 'rip off---- over charge shipping.'\n",
      " \"the keyboard is really worthwhile in usefulness and is sturdy enough i don't expect any problems.\"\n",
      " 'bad purchase.'\n",
      " 'this company charge me a restocking fee and still not given me my refund back.'\n",
      " 'i especially love the long battery life.'\n",
      " 'bought mainly for the charger, which broke soon after purchasing.'\n",
      " 'the ear buds only play music in one ear.'\n",
      " 'i highly recommend this modest priced cellular phone.'\n",
      " 'the battery runs down quickly.'\n",
      " 'product is useless, since it does not have enough charging current to charge the  cellphones i was planning to use it with.'\n",
      " \"my phone sounded ok ( not great - ok), but my wife's phone was almost totally unintelligible, she couldn't understand a word being said on it.\"\n",
      " 'the handsfree part works fine, but then the car tries to download the address book, and the treo reboots.overall, i still rate this device high.'\n",
      " 'after a year the battery went completely dead on my headset.'\n",
      " 'i love the look and feel of samsung flipphones.'\n",
      " 'it fits so securely that the ear hook does not even need to be used and the sound is better directed through your ear canal.'\n",
      " 'very cheap plastic, creaks like an old wooden floor.'\n",
      " \"it has everything i need and i couldn't ask for more.\"\n",
      " 'so i bought about  of these and saved alot of money.'\n",
      " 'the reception through this headset is excellent.'\n",
      " 'the ngage is still lacking in earbuds.' 'great product for the price!.'\n",
      " 'first off the reception sucks, i have never had more than  bars, ever.'\n",
      " 'this device is great in several situations:.)'\n",
      " 'very good product, well made.'\n",
      " 'as an earlier review noted, plug in this charger and nothing happens.'\n",
      " 'excellent sound quality.'\n",
      " \"it's kind of embarrassing to use because of how it looks and mostly it's embarrassing how child-like the company is.\"\n",
      " \"makes it easier to keep up with my bluetooth when i'm not wearing it.\"\n",
      " 'what a waste of time!'\n",
      " 'this item is great, installed it, let it charged up overnite and it has been working good every since with no problems'\n",
      " \"doesn't hold charge.\" 'absolutely great.' 'great...no problems at all!.'\n",
      " 'improper description.... i had to return it.'\n",
      " 'earbud piece breaks easily.'\n",
      " \"i connected my wife's bluetooth,(motorola hs) to my phone and it worked like a charm whether the phone was in my pocket or the case.\"\n",
      " \"doesn't last long.\" 'good audio quality.'\n",
      " 'protects the phone on all sides.' 'wont work right or atleast for me.'\n",
      " 'i received my headset in good time and was happy with it.'\n",
      " 'it was that loud.glad to say that the plantronics  maintains a flawless connection to my cell and with no static during normal use.'\n",
      " 'i get absolutely horrible reception in my apartment, where with other phones i have not had this problem.'\n",
      " \"but when i check voice mail at night, the keypad backlight turns off a few seconds into the first message, and then i'm lost.\"\n",
      " 'very good stuff for the price.' 'does not fit.'\n",
      " 'i have bought this nokia cell phone a few weeks ago and it was a nightmare.'\n",
      " 'this product had a strong rubber/petroleum smell that was unbearable after a while and caused me to return it'\n",
      " 'having trouble with volume.' 'great phone.'\n",
      " 'i have tried these cables with my computer and my ipod and it works just fine.'\n",
      " 'this is the phone to get for .... i just bought my sa and all i can say is wow!'\n",
      " \"don't waste your $$$ on this one.\"\n",
      " 'linksys should have some way to exchange a bad phone for a refurb unit or something!'\n",
      " 'i have two more years left in this contract and i hate this phone.'\n",
      " 'product was excellent and works better than the verizon one and boy was it cheaper!'\n",
      " 'excellent bluetooth headset.' 'was not happy.'\n",
      " 'i really recommend this faceplates since it looks very nice, elegant and cool.'\n",
      " \"it didn't charge for me.\" 'useless phone, simply deaf.'\n",
      " 'my sanyo has survived dozens of drops on blacktop without ill effect.'\n",
      " 'this blueant supertooth hands-free phone speaker is awesome.'\n",
      " 'i highly recommend these and encourage people to give them a try.'\n",
      " 'this product is great... it makes working a lot easier i can go to the copier while waiting on hold for something.'\n",
      " 'this one works and was priced right.'\n",
      " 'nice docking station for home or work.'\n",
      " 'excellent product, i am very satisfied with the purchase.'\n",
      " 'trying to make a call on these is an exercise in frustration.'\n",
      " 'due to this happening on every call i was forced to stop using this headset.'\n",
      " \"i've missed numerous calls because of this reason.\"\n",
      " 'comfortable in my hand.'\n",
      " 'my father has the v, and the battery is dying.'\n",
      " 'it would take too long to describe how bad my customer service experience has been with amazon.'\n",
      " 'its well-designed and very sharp -- the blue is a very nice color.'\n",
      " 'i also didn\\'t like the \"on\" button, it felt like it would crack with use.'\n",
      " 'none of the new ones have ever quite worked properly.' \"doesn't work.\"\n",
      " 'plus, i seriously do not believe it is worth its steep price point.'\n",
      " 'much less than the jawbone i was going to replace it with.'\n",
      " 'awkward to use and unreliable.' \"don't make the same mistake i did.\"\n",
      " 'the design might be ergonomic in theory but i could not stand having these in my ear.'\n",
      " 'in my house i was getting dropped coverage upstairs and no coverage in my basement.'\n",
      " 'great price also!' 'poor product.'\n",
      " 'terrible.. my car will not accept this cassette.'\n",
      " 'reaching for the bottom row is uncomfortable, and the send and end keys are not where i expect them to be..'\n",
      " \"it's been my choice headset for years.great sound; good volume; good noise cancellation.\"\n",
      " 'other than that, the leather is nice and soft; the fit is very tight; the cut out for the face is a good shape.'\n",
      " 'i cannot make calls at certain places.'\n",
      " \"you can't beat the price on these.\"\n",
      " 'this case has passed the one year mark and while it shows signs of wear, it is % functional.'\n",
      " 'leopard print is wonderfully wild!.'\n",
      " \"it quit working after i'd used it for about  months, so i just purchased another one because this is the best headset i've ever owned.\"\n",
      " 'really pleased with this product so far.'\n",
      " 'it feels more comfortable than most headsets because i wear glasses and that gets in the way sometimes.'\n",
      " 'i bought it for my mother and she had a problem with the battery.'\n",
      " 'you can even take self portraits with the outside (exterior) display, very cool.'\n",
      " 'i would recommend this.'\n",
      " 'provides good protection and looks classy, too.'\n",
      " 'the bt headset was such a disapoinment.'\n",
      " 'i was sitting in my vehicle, with the cradle on my belt, and the headset lost signal.'\n",
      " 'this phone is slim and light and the display is beautiful.'\n",
      " \"doesn't work at all.. i bougth it for my lc and its not working.\"\n",
      " \"i've had this for nearly  years and it has worked great for me.\"\n",
      " 'i used to talk on it for  -  / hours and the battery would be literally drained and dying.'\n",
      " 'its a total package.' 'avoid this one if you can.'\n",
      " \"this phone tries very hard to do everything but fails at it's very ability to be a phone.\"\n",
      " 'linked to my phone without effort.' 'good , works fine.'\n",
      " 'the headset fulfills my requirements so i am happy with my purchase.'\n",
      " 'really ugly.' 'piece of junk.' 'looks great and is strong.\"'\n",
      " 'however-the riingtones are not the best, and neither are the games.'\n",
      " 'could not get strong enough signal.' 'kind of flops around.'\n",
      " 'the holster that arrived did not match the photo in the ad.'\n",
      " 'i have had this phone for over a year now, and i will tell you, its not that great.'\n",
      " 'it is well made, easy to access the phone and has a handy, detachable belt clip.'\n",
      " 'what possesed me to get this junk, i have no idea...'\n",
      " 'i searched the internet, and found this one to be the best value.'\n",
      " 'it is very comfortable on the ear.' 'poor voice clarity.'\n",
      " 'great pocket pc / phone combination.'\n",
      " 'sound quality on both end is excellent, i use headset to call my wife and ask my wife to use headset to call me !.'\n",
      " 'after  months, screen just went black all of a sudden.'\n",
      " 'i had absolutely no problem with this headset linking to my  blackberry curve!'\n",
      " 'no real improvement.'\n",
      " 'the phone can also take great pictures and even video clips.'\n",
      " 'do not buy do not buyit sucks' 'we would recommend these to others.'\n",
      " 'this is a great phone!.' 'voice recognition is tremendous!'\n",
      " '.... item arrived quickly and works great with my metro pcs samsung sch-r slider phone and sony premium sound in ear plugs.'\n",
      " 'good product - incredible value.'\n",
      " 'i was very impressed with the price of the cases.'\n",
      " 'i was able to do voice dialing in the car with no problem.'\n",
      " \"don't buy this product - it fails!.\" 'internet is excrutiatingly slow.'\n",
      " \"i'm glad i found this product on amazon it is hard to find, it wasn't high priced.\"\n",
      " \"i'll be looking for a new earpiece.\" 'sprint charges for this service.'\n",
      " 'it clicks into place in a way that makes you wonder how long that mechanism would last.'\n",
      " 'amazon sucks.'\n",
      " 'works great, when my cat attacked the phone he scratched the protective strip instead of destroying the screen.'\n",
      " 'love this headset!'\n",
      " \"i usually don't like headbands but this one is very lightweight & doesn't mess up my hair.\"\n",
      " 'dont waste your money...' 'mobile phone tools are a must have.'\n",
      " 'the only very disappointing thing was there was no speakerphone!!!!'\n",
      " 'however, bt headsets are currently not good for real time games like first-person shooters since the audio delay messes me up.'\n",
      " 'good product, good seller.' \"i wouldn't recommend buying this product.\"\n",
      " 'i got the car charger and not even after a week the charger was broken...i went to plug it in and it started smoking.'\n",
      " 'never got it!!!!!'\n",
      " 'i would recommend purchasing the jabra jx- series  which works flawlessly with my moto q, go figure.'\n",
      " 'i recently had problems where i could not stay connected for more than  minutes before being disconnected.'\n",
      " 'display is excellent and camera is as good as any from that year.'\n",
      " 'i was very pleased to see that i could replace my well travled swivel holster for my blackberry flip.'\n",
      " 'sweetest phone!!!' 'reversible plug works great.'\n",
      " 'at first i thought i was grtting a good deal at $., until i plugged it into my phone (vc razr).'\n",
      " \"however, after about a year, the fliptop started to get loose and wobbly and eventually my screen went black and i couldn't receive and place calls.\"\n",
      " 'the sound quality is good and functionality is awesome.'\n",
      " 'it is unusable in a moving car at freeway speed.'\n",
      " 'i wear it everyday and it holds up very well.'\n",
      " \"as many people complained, i found this headset's microphone was very weak.\"\n",
      " 'after the first charge kept going dead after  minutes.'\n",
      " 'he was very impressed when going from the original battery to the extended battery.'\n",
      " 'i only hear garbage for audio.' 'this phone works great.'\n",
      " 'this is a great deal.' 'it has kept up very well.'\n",
      " 'echo problem....very unsatisfactory'\n",
      " 'i have been very happy with the  and have had no complaints from any one regarding my sound quality on their end.'\n",
      " 'i was not impressed by this product.'\n",
      " 'treo and t-mobile refused to replace it again and forced me to buy another phone without any kind of upgrade discount.'\n",
      " 'i highly recommend this device to everyone!'\n",
      " \"all in all, i'm quite satisfied with this purchase.\"\n",
      " 'we are sending it back.'\n",
      " 'items stated as included from the description are not included.'\n",
      " 'i great reception all the time.' 'better than new.' 'bad quality.'\n",
      " 'we have tried  units and they both failed within  months.. pros'\n",
      " \"i've had no trouble accessing the internet, downloading ringtones or performing any of the functions.\"\n",
      " \"best i've found so far .... i've tried  other bluetooths and this one has the best quality (for both me and the listener) as well as ease of using.\"\n",
      " 'this is a very average phone with bad battery life that operates on a weak network.'\n",
      " \"it is so small and you don't even realize that it is there after a while of getting used to it.\"\n",
      " 'comfortable fit - you need your headset to be comfortable for at least an hour at a time, if not for an entire day.'\n",
      " \"i have yet to run this new battery below two bars and that's three days without charging.\"\n",
      " 'just really good.. so far, probably the best bt headset i have ever had.'\n",
      " \"it's aggravating!\"\n",
      " \"i'll be drivng along, and my headset starts ringing for no reason.\"\n",
      " 'the sound is clear and the people i talk to on it are amazed at the quality too.'\n",
      " 'after trying many many handsfree gadgets this is the one that finally works well.'\n",
      " 'all in all i think it was a good investment.' 'love this phone.'\n",
      " 'i give wirefly  star.i will contact cingular/at&t; and inform them of this practice.'\n",
      " \"it doesn't work in europe or asia.\" 'the best phone in market :).'\n",
      " 'it was a waste of my money.' 'product is exactly as described.'\n",
      " 'excellent phone.' 'the nokia ca- usb cable did not work with my phone.'\n",
      " 'the cable looks so thin and flimsy, it is scary.'\n",
      " 'overall, i would recommend this phone over the new walkman.'\n",
      " 'bad choice.' 'will order from them again!'\n",
      " 'also makes it easier to hold on to.' \"i'm returning them.\"\n",
      " 'one thing i hate is the mode set button at the side.'\n",
      " 'verizon tech support walked my through a few procedures, none of which worked and i ended up having to do a hard re-set, wiping out all my data.'\n",
      " 'i received it quickly and it works great!!'\n",
      " \"muddy, low quality sound, and the casing around the wire's insert was poorly super glued and slid off.\"\n",
      " 'these products cover up the important light sensor above the ear outlet.'\n",
      " \"the worst phone i've ever had.... only had it for a few months.\"\n",
      " 'if you simply want a small flip phone -- look elsewhere as the extra bells & whistles are mediocre.'\n",
      " 'i own  of these cases and would order another.'\n",
      " \"i'm a bit disappointed.\"\n",
      " 'this does not fit the palm tungsten e and it broke the first time i tried to plug it in.'\n",
      " 'very disappointed.' 'used and dirty.' \"don't buy this product.\"\n",
      " 'the item received was counterfeit.'\n",
      " 'the accompanied software is almost brilliant.'\n",
      " 'i had to purchase a different case.'\n",
      " 'after arguing with verizon regarding the dropped calls we returned the phones after two days.'\n",
      " 'excellent sound, battery life and inconspicuous to boot!.'\n",
      " 'i have - bars on my cell phone when i am home, but you cant not hear anything.'\n",
      " 'the best electronics of the available fm transmitters.'\n",
      " 'this item worked great, but it broke after  months of use.'\n",
      " 'you get what you pay for i guess.' 'disapointing results.'\n",
      " \"clipping this to your belt will deffinitely make you feel like  cent's up-and-coming.\"\n",
      " 'sounds good reasonably priced and effective, its that simple !'\n",
      " 'great phone.' 'made very sturdy.'\n",
      " 'well packaged, arrived on time, and works as intended.'\n",
      " 'fantastic earphones.'\n",
      " \"after my phone got to be about a year old, it's been slowly breaking despite much care on my part.\"\n",
      " \")setup couldn't have been simpler.\" 'works fine.' \"don't buy it.\"\n",
      " 'i even dropped this phone into a stream and it was submerged for  seconds and it still works great!'\n",
      " 'i ordered this product first and was unhappy with it immediately.'\n",
      " 'great earpiece.'\n",
      " \"don't trust their website and don't expect any helpful support.\"\n",
      " 'otherwise, easy to install and use, clear sound.'\n",
      " 'they made this case too small and is very difficult to install.'\n",
      " 'nice headset priced right.' 'however i needed some better instructions.'\n",
      " 'it worked very well.'\n",
      " \". long lasting battery (you don't have to recharge it as frequentyly as some of the flip phones).\"\n",
      " 'i gave it  stars because of the sound quality.'\n",
      " \"it's very convenient and simple to use - gets job done & makes the car ride so much smoother.\"\n",
      " 'then i exchanged for the same phone, even that had the same problem.'\n",
      " \"i love the camera, it's really pretty good quality.\"\n",
      " 'terrible phone holder.'\n",
      " \"this phone is pretty sturdy and i've never had any large problems with it.\"\n",
      " '[...] down the drain because of a weak snap!'\n",
      " 'i kept catching the cable on the seat and i had to pull the phone out to turn it on an off.'\n",
      " \"i've only had my bluetooth for a few weeks, but i really like it.\"\n",
      " 'great price, too!'\n",
      " 'people couldnt hear me talk and i had to pull out the earphone and talk on the phone.'\n",
      " 'i have this phone and it is a thorn in my side, i really abhor it.'\n",
      " 'the only good thing was that it fits comfortably on small ears.'\n",
      " 'horrible phone.'\n",
      " 'this frog phone charm is adorable and very eye catching.'\n",
      " \"can't upload ringtones from a third party.\"\n",
      " 'audio quality is poor, very poor.'\n",
      " 'it defeats the purpose of a bluetooth headset.' 'not worth it.'\n",
      " \"don't buy this product.\"\n",
      " 'sucked, most of the stuff does not work with my phone.'\n",
      " 'disappointing accessory from a good manufacturer.'\n",
      " \"it doesn't make you look cool.\" 'battery life is real good.'\n",
      " 'poorly contstruct hinge.'\n",
      " \"i'm using it with an iriver spinn (with case) and it fits fine.\"\n",
      " 'good show, samsung.' 'its reception is very very poor.'\n",
      " 'it felt too light and \"tinny.\".'\n",
      " \"a good quality bargain.. i bought this after i bought a cheapy from big lots that sounded awful and people on the other end couldn't hear me.\"\n",
      " \"however, my girl was complain that some time the phone doesn't wake up like normal phone does.\"\n",
      " 'i put the latest os on it (v.g), and it now likes to slow to a crawl and lock up every once in a while.'\n",
      " \"i'm very disappointed with my decision.\" 'it looks very nice.'\n",
      " 'the screen size is big, key pad lit well enough, and the camera quality is excellent for a camera phone.'\n",
      " 'it finds my cell phone right away when i enter the car.'\n",
      " 'very unreliable service from t-mobile !'\n",
      " 'appears to actually outperform the original battery from china that came with my vi.'\n",
      " 'then a few days later the a puff of smoke came out of the phone while in use.'\n",
      " 'the keyboard is a nice compromise between a full qwerty and the basic cell phone number keypad.'\n",
      " 'while i managed to bend the leaf spring back in place, the metal now has enough stress that it will break on the next drop.'\n",
      " 'worthwhile.' 'great choice!' 'stay away from the q!.'\n",
      " 'small, sleek, impressive looking, practical setup with ample storage in place.'\n",
      " 'truly awful.' 'i bought two of them and neither will charge.'\n",
      " 'i am going to have to be the first to negatively review this product.'\n",
      " 'the file browser offers all the options that one needs.handsfree is great.'\n",
      " 'the construction of the headsets is poor.'\n",
      " 'thank you for such great service.' 'its not user friendly.'\n",
      " 'i love this cable - it allows me to connect any mini-usb device to my pc.'\n",
      " 'the battery is unreliable as well as the service use antena.'\n",
      " 'using all earpieces, left or right, this thing will not stay on my ear.'\n",
      " \"their research and development division obviously knows what they're doing.\"\n",
      " 'disappointment.. i hate anything that goes in my ear.'\n",
      " \"mic doesn't work.\"\n",
      " 'for the price on amazon, it is an excellent product, which i would highly recommend.'\n",
      " \"yes it's shiny on front side - and i love it!\"\n",
      " \"the camera on the phone may be used as a dustpan when indoors... i'd rather be using a disposable then this.\"\n",
      " 'very good phone.'\n",
      " 'think it over when you plan to own this one!this sure is the last moto phone for me!'\n",
      " 'what a waste of money and time!.'\n",
      " 'the microphone also works well, but (according to people i have called) it applifies everything.'\n",
      " 'this is cool because most cases are just open there allowing the screen to get all scratched up.'\n",
      " 'this particular model would not work with my motorola q smartphone.'\n",
      " 'the calls drop, the phone comes on and off at will, the screen goes black and the worst of all it stops ringing intermittently.'\n",
      " 'lately they have been extremely nice and helpful on the phone.'\n",
      " 'i recommend igo to anyone with different brand cell phones/mp players in the family.'\n",
      " \"none of it works, just don't buy it.\" 'chinese forgeries abound!.']\n",
      "<class 'numpy.str_'>\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"amazon_cells_labelled.txt\", delimiter='\\t', header=None)\n",
    "data.columns = ['Sentence', 'Class']\n",
    "data['index'] = data.index                                          # add new column index\n",
    "columns = ['index', 'Class', 'Sentence']\n",
    "data = preprocess_pandas(data, columns)                             # pre-process\n",
    "training_data, validation_data, training_labels, validation_labels = train_test_split( # split the data into training, validation, and test splits\n",
    "    data['Sentence'].values.astype('U'),\n",
    "    data['Class'].values.astype('int32'),\n",
    "    test_size=0.10,\n",
    "    random_state=0,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(training_data)\n",
    "print(type(training_data[0]))\n",
    "\n",
    "# vectorize data using TFIDF and transform for PyTorch for scalability\n",
    "#word_vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,2), max_features=50000, max_df=0.5, use_idf=True, norm='l2')\n",
    "#training_data = word_vectorizer.fit_transform(training_data)        # transform texts to sparse matrix\n",
    "#training_data = training_data.todense()                             # convert to dense matrix for Pytorch\n",
    "#vocab_size = len(word_vectorizer.vocabulary_)\n",
    "#validation_data = word_vectorizer.transform(validation_data)\n",
    "#validation_data = validation_data.todense()\n",
    "\n",
    "#train_x_tensor = torch.from_numpy(np.array(training_data)).type(torch.FloatTensor)\n",
    "train_y_tensor = torch.from_numpy(np.array(training_labels)).long()\n",
    "#validation_x_tensor = torch.from_numpy(np.array(validation_data)).type(torch.FloatTensor)\n",
    "validation_y_tensor = torch.from_numpy(np.array(validation_labels)).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, datasetA, datasetB):\n",
    "        self.datasetA = datasetA\n",
    "        self.datasetB = datasetB\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        inp = self.datasetA[i]\n",
    "        tokens = tokenizer(inp)\n",
    "        embed = torch.tensor(vocab(tokens), dtype=torch.long)\n",
    "        label = F.one_hot(self.datasetB[i],num_classes=2)\n",
    "        return embed,label\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.datasetA),len(self.datasetB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900 900\n"
     ]
    }
   ],
   "source": [
    "def collate_fn_padd(batch):\n",
    "    '''\n",
    "    Padds batch of variable length\n",
    "\n",
    "    note: it converts things ToTensor manually here since the ToTensor transform\n",
    "    assume it takes in images rather than arbitrary tensors.\n",
    "    '''\n",
    "    ## get sequence lengths\n",
    "    lengths = torch.tensor([ t.shape[0] for t in batch ]).to(device)\n",
    "    ## padd\n",
    "    batch = [ torch.Tensor(t).to(device) for t in batch ]\n",
    "    batch = torch.nn.utils.rnn.pad_sequence(batch)\n",
    "    ## compute mask\n",
    "    mask = (batch != 0).to(device)\n",
    "    return batch, lengths, mask\n",
    "\n",
    "\n",
    "train_ds = ConcatDataset(training_data,train_y_tensor)\n",
    "val_ds = ConcatDataset(validation_data,validation_y_tensor)\n",
    "print(len(training_data),len(train_y_tensor))\n",
    "train_loader = DataLoader(train_ds,batch_size=1)\n",
    "val_loader = DataLoader(val_ds,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[  8, 602,   2,   0,  15,   0,   0,  16,   2, 162, 746,   4,  89,  63,\n",
       "            2, 120,   1]]),\n",
       " tensor([[1, 0]])]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = model\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Starting epoch {epoch+1} of {num_epochs}\")\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch_nr, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device).to(torch.float)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            #print(inputs)\n",
    "            #print(outputs.unsqueeze(0))\n",
    "            #print(labels)\n",
    "            loss = criterion(outputs.unsqueeze(0), labels)\n",
    "            loss = loss.to(device)\n",
    "\n",
    "            if (batch_nr%80 == 0):\n",
    "                print(f\"Processing batch number {batch_nr+1} of {len(train_loader)}\")\n",
    "                print(outputs)\n",
    "                print(labels)\n",
    "                print(\"current loss\",loss.item())\n",
    "                \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        train_loss /= len(train_loader.dataset)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device).to(torch.float)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs.unsqueeze(0), labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    # Save the best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Starting epoch 1 of 30\n",
      "Processing batch number 1 of 900\n",
      "tensor([0.3510, 0.6490], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.8532605171203613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anton\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch number 81 of 900\n",
      "tensor([0.2835, 0.7165], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.9328535795211792\n",
      "Processing batch number 161 of 900\n",
      "tensor([0.5033, 0.4967], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.6898790597915649\n",
      "Processing batch number 241 of 900\n",
      "tensor([0.6800, 0.3200], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.889215350151062\n",
      "Processing batch number 321 of 900\n",
      "tensor([0.3645, 0.6355], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.5668364763259888\n",
      "Processing batch number 401 of 900\n",
      "tensor([0.5506, 0.4494], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.7450413703918457\n",
      "Processing batch number 481 of 900\n",
      "tensor([0.8466, 0.1534], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 1.0986812114715576\n",
      "Processing batch number 561 of 900\n",
      "tensor([0.1867, 0.8133], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.4281196892261505\n",
      "Processing batch number 641 of 900\n",
      "tensor([0.6137, 0.3863], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.5859413146972656\n",
      "Processing batch number 721 of 900\n",
      "tensor([0.6099, 0.3901], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.8090506792068481\n",
      "Processing batch number 801 of 900\n",
      "tensor([0.8414, 0.1586], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.40890195965766907\n",
      "Processing batch number 881 of 900\n",
      "tensor([0.8540, 0.1460], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 1.1085617542266846\n",
      "Starting epoch 2 of 30\n",
      "Processing batch number 1 of 900\n",
      "tensor([0.8711, 0.1289], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.38940757513046265\n",
      "Processing batch number 81 of 900\n",
      "tensor([0.2275, 0.7725], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 1.0023280382156372\n",
      "Processing batch number 161 of 900\n",
      "tensor([0.7026, 0.2974], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.5109056830406189\n",
      "Processing batch number 241 of 900\n",
      "tensor([0.8708, 0.1292], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 1.1311991214752197\n",
      "Processing batch number 321 of 900\n",
      "tensor([0.1234, 0.8766], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.38585779070854187\n",
      "Processing batch number 401 of 900\n",
      "tensor([0.4937, 0.5063], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.6868575811386108\n",
      "Processing batch number 481 of 900\n",
      "tensor([0.9963, 0.0037], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 1.307889699935913\n",
      "Processing batch number 561 of 900\n",
      "tensor([0.3179, 0.6821], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.5274993777275085\n",
      "Processing batch number 641 of 900\n",
      "tensor([0.6433, 0.3567], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.5600665211677551\n",
      "Processing batch number 721 of 900\n",
      "tensor([0.7328, 0.2672], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.9528231620788574\n",
      "Processing batch number 801 of 900\n",
      "tensor([0.8985, 0.1015], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.37205684185028076\n",
      "Processing batch number 881 of 900\n",
      "tensor([0.6801, 0.3199], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.8893216848373413\n",
      "Starting epoch 3 of 30\n",
      "Processing batch number 1 of 900\n",
      "tensor([0.9703, 0.0297], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.3295957148075104\n",
      "Processing batch number 81 of 900\n",
      "tensor([0.0886, 0.9114], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 1.1869205236434937\n",
      "Processing batch number 161 of 900\n",
      "tensor([0.4229, 0.5771], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.7732623815536499\n",
      "Processing batch number 241 of 900\n",
      "tensor([0.3673, 0.6327], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.5692092180252075\n",
      "Processing batch number 321 of 900\n",
      "tensor([0.1813, 0.8187], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.42441925406455994\n",
      "Processing batch number 401 of 900\n",
      "tensor([0.1118, 0.8882], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.37846601009368896\n",
      "Processing batch number 481 of 900\n",
      "tensor([0.9884, 0.0116], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 1.2963086366653442\n",
      "Processing batch number 561 of 900\n",
      "tensor([0.7938, 0.2062], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 1.029500961303711\n",
      "Processing batch number 641 of 900\n",
      "tensor([0.9762, 0.0238], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.32628050446510315\n",
      "Processing batch number 721 of 900\n",
      "tensor([0.8738, 0.1262], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 1.135286808013916\n",
      "Processing batch number 801 of 900\n",
      "tensor([0.9981, 0.0019], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.31425952911376953\n",
      "Processing batch number 881 of 900\n",
      "tensor([0.8752, 0.1248], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 1.1372044086456299\n",
      "Starting epoch 4 of 30\n",
      "Processing batch number 1 of 900\n",
      "tensor([0.9657, 0.0343], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.332193523645401\n",
      "Processing batch number 81 of 900\n",
      "tensor([0.0013, 0.9987], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 1.3113031387329102\n",
      "Processing batch number 161 of 900\n",
      "tensor([0.5934, 0.4066], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.6041054725646973\n",
      "Processing batch number 241 of 900\n",
      "tensor([0.0691, 0.9309], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.3523729145526886\n",
      "Processing batch number 321 of 900\n",
      "tensor([0.0506, 0.9494], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.3414917588233948\n",
      "Processing batch number 401 of 900\n",
      "tensor([0.3828, 0.6172], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.5828182697296143\n",
      "Processing batch number 481 of 900\n",
      "tensor([0.9979, 0.0021], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 1.310198187828064\n",
      "Processing batch number 561 of 900\n",
      "tensor([0.2299, 0.7701], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.459116131067276\n",
      "Processing batch number 641 of 900\n",
      "tensor([0.8917, 0.1083], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.3762465715408325\n",
      "Processing batch number 721 of 900\n",
      "tensor([0.9554, 0.0446], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 1.2488088607788086\n",
      "Processing batch number 801 of 900\n",
      "tensor([0.8857, 0.1143], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.3800627589225769\n",
      "Processing batch number 881 of 900\n",
      "tensor([0.3845, 0.6155], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.5842961072921753\n",
      "Starting epoch 5 of 30\n",
      "Processing batch number 1 of 900\n",
      "tensor([0.9949, 0.0051], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.31601089239120483\n",
      "Processing batch number 81 of 900\n",
      "tensor([0.0357, 0.9643], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 1.261579990386963\n",
      "Processing batch number 161 of 900\n",
      "tensor([0.9801, 0.0199], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.3241465389728546\n",
      "Processing batch number 241 of 900\n",
      "tensor([0.0107, 0.9893], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.31904587149620056\n",
      "Processing batch number 321 of 900\n",
      "tensor([0.4675, 0.5325], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.661179780960083\n",
      "Processing batch number 401 of 900\n",
      "tensor([0.7277, 0.2723], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.9465898275375366\n",
      "Processing batch number 481 of 900\n",
      "tensor([0.9849, 0.0151], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 1.2913378477096558\n",
      "Processing batch number 561 of 900\n",
      "tensor([0.9521, 0.0479], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 1.2441942691802979\n",
      "Processing batch number 641 of 900\n",
      "tensor([0.9988, 0.0012], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.31390511989593506\n",
      "Processing batch number 721 of 900\n",
      "tensor([0.3405, 0.6595], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.5463529825210571\n",
      "Processing batch number 801 of 900\n",
      "tensor([0.9980, 0.0020], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.31435513496398926\n",
      "Processing batch number 881 of 900\n",
      "tensor([0.0151, 0.9849], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.3214636743068695\n",
      "Starting epoch 6 of 30\n",
      "Processing batch number 1 of 900\n",
      "tensor([9.9966e-01, 3.4176e-04], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.313445508480072\n",
      "Processing batch number 81 of 900\n",
      "tensor([0.0099, 0.9901], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 1.298771858215332\n",
      "Processing batch number 161 of 900\n",
      "tensor([0.9916, 0.0084], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.31779271364212036\n",
      "Processing batch number 241 of 900\n",
      "tensor([0.3175, 0.6825], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.5271804332733154\n",
      "Processing batch number 321 of 900\n",
      "tensor([3.1599e-05, 9.9997e-01], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.3132787346839905\n",
      "Processing batch number 401 of 900\n",
      "tensor([0.0729, 0.9271], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.3546334505081177\n",
      "Processing batch number 481 of 900\n",
      "tensor([9.9981e-01, 1.9052e-04], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 1.3129831552505493\n",
      "Processing batch number 561 of 900\n",
      "tensor([0.0100, 0.9900], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.31865864992141724\n",
      "Processing batch number 641 of 900\n",
      "tensor([0.0232, 0.9768], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 1.2796118259429932\n",
      "Processing batch number 721 of 900\n",
      "tensor([0.8287, 0.1713], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 1.0749071836471558\n",
      "Processing batch number 801 of 900\n",
      "tensor([9.9994e-01, 5.8006e-05], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.31329286098480225\n",
      "Processing batch number 881 of 900\n",
      "tensor([0.5944, 0.4056], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.7919963598251343\n",
      "Starting epoch 7 of 30\n",
      "Processing batch number 1 of 900\n",
      "tensor([0.9980, 0.0020], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.31433647871017456\n",
      "Processing batch number 81 of 900\n",
      "tensor([2.6006e-05, 9.9997e-01], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 1.3132237195968628\n",
      "Processing batch number 161 of 900\n",
      "tensor([0.8161, 0.1839], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.4261772036552429\n",
      "Processing batch number 241 of 900\n",
      "tensor([0.9929, 0.0071], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 1.3028337955474854\n",
      "Processing batch number 321 of 900\n",
      "tensor([2.1168e-05, 9.9998e-01], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.3132730722427368\n",
      "Processing batch number 401 of 900\n",
      "tensor([0.0366, 0.9634], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.3335041105747223\n",
      "Processing batch number 481 of 900\n",
      "tensor([0.9980, 0.0020], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 1.3103889226913452\n",
      "Processing batch number 561 of 900\n",
      "tensor([0.9946, 0.0054], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 1.3054347038269043\n",
      "Processing batch number 641 of 900\n",
      "tensor([9.9987e-01, 1.3005e-04], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.31333163380622864\n",
      "Processing batch number 721 of 900\n",
      "tensor([0.9300, 0.0700], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 1.2128570079803467\n",
      "Processing batch number 801 of 900\n",
      "tensor([9.9956e-01, 4.3777e-04], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.31349727511405945\n",
      "Processing batch number 881 of 900\n",
      "tensor([0.0023, 0.9977], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.3144983947277069\n",
      "Starting epoch 8 of 30\n",
      "Processing batch number 1 of 900\n",
      "tensor([9.9998e-01, 1.6818e-05], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.3132707178592682\n",
      "Processing batch number 81 of 900\n",
      "tensor([0.0424, 0.9576], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 1.2519251108169556\n",
      "Processing batch number 161 of 900\n",
      "tensor([1.0000e+00, 4.3428e-06], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.31326401233673096\n",
      "Processing batch number 241 of 900\n",
      "tensor([0.2376, 0.7624], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.4647519588470459\n",
      "Processing batch number 321 of 900\n",
      "tensor([1.9741e-07, 1.0000e+00], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.31326183676719666\n",
      "Processing batch number 401 of 900\n",
      "tensor([0.0012, 0.9988], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.3138813376426697\n",
      "Processing batch number 481 of 900\n",
      "tensor([9.9924e-01, 7.5952e-04], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 1.3121514320373535\n",
      "Processing batch number 561 of 900\n",
      "tensor([0.6584, 0.3416], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.8640381097793579\n",
      "Processing batch number 641 of 900\n",
      "tensor([0.9886, 0.0114], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.31946080923080444\n",
      "Processing batch number 721 of 900\n",
      "tensor([0.4932, 0.5068], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.6863645911216736\n",
      "Processing batch number 801 of 900\n",
      "tensor([0.5252, 0.4748], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.6682350635528564\n",
      "Processing batch number 881 of 900\n",
      "tensor([8.5279e-04, 9.9915e-01], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.3137206435203552\n",
      "Starting epoch 9 of 30\n",
      "Processing batch number 1 of 900\n",
      "tensor([9.9998e-01, 1.9758e-05], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.31327229738235474\n",
      "Processing batch number 81 of 900\n",
      "tensor([0.0014, 0.9986], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 1.3112599849700928\n",
      "Processing batch number 161 of 900\n",
      "tensor([9.9999e-01, 8.9339e-06], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.3132665455341339\n",
      "Processing batch number 241 of 900\n",
      "tensor([3.0130e-05, 9.9997e-01], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.31327787041664124\n",
      "Processing batch number 321 of 900\n",
      "tensor([0.5395, 0.4605], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.7334431409835815\n",
      "Processing batch number 401 of 900\n",
      "tensor([0.0288, 0.9712], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.32906022667884827\n",
      "Processing batch number 481 of 900\n",
      "tensor([1.0000e+00, 3.3025e-08], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 1.31326162815094\n",
      "Processing batch number 561 of 900\n",
      "tensor([0.1751, 0.8249], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.420105904340744\n",
      "Processing batch number 641 of 900\n",
      "tensor([1.0000e+00, 2.1791e-06], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.3132628798484802\n",
      "Processing batch number 721 of 900\n",
      "tensor([0.9515, 0.0485], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 1.2432177066802979\n",
      "Processing batch number 801 of 900\n",
      "tensor([0.9936, 0.0064], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.3167111575603485\n",
      "Processing batch number 881 of 900\n",
      "tensor([0.0806, 0.9194], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.3592439889907837\n",
      "Starting epoch 10 of 30\n",
      "Processing batch number 1 of 900\n",
      "tensor([9.9997e-01, 3.3867e-05], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.3132799565792084\n",
      "Processing batch number 81 of 900\n",
      "tensor([1.7431e-04, 9.9983e-01], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 1.31300687789917\n",
      "Processing batch number 161 of 900\n",
      "tensor([1.0000e+00, 9.4841e-09], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.31326165795326233\n",
      "Processing batch number 241 of 900\n",
      "tensor([9.9995e-01, 5.1867e-05], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 1.3131858110427856\n",
      "Processing batch number 321 of 900\n",
      "tensor([4.8496e-05, 9.9995e-01], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.31328779458999634\n",
      "Processing batch number 401 of 900\n",
      "tensor([2.2008e-07, 1.0000e+00], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.31326183676719666\n",
      "Processing batch number 481 of 900\n",
      "tensor([9.9993e-01, 6.8618e-05], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 1.3131613731384277\n",
      "Processing batch number 561 of 900\n",
      "tensor([0.0011, 0.9989], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.31385791301727295\n",
      "Processing batch number 641 of 900\n",
      "tensor([0.0114, 0.9886], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 1.296712040901184\n",
      "Processing batch number 721 of 900\n",
      "tensor([0.8934, 0.1066], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 1.1620564460754395\n",
      "Processing batch number 801 of 900\n",
      "tensor([9.9999e-01, 8.3364e-06], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.31326618790626526\n",
      "Processing batch number 881 of 900\n",
      "tensor([0.0341, 0.9659], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.33205360174179077\n",
      "Starting epoch 11 of 30\n",
      "Processing batch number 1 of 900\n",
      "tensor([9.9999e-01, 7.3730e-06], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.3132656514644623\n",
      "Processing batch number 81 of 900\n",
      "tensor([0.0602, 0.9398], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 1.2266530990600586\n",
      "Processing batch number 161 of 900\n",
      "tensor([9.9981e-01, 1.9456e-04], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.3133663237094879\n",
      "Processing batch number 241 of 900\n",
      "tensor([9.5029e-05, 9.9990e-01], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.3133127987384796\n",
      "Processing batch number 321 of 900\n",
      "tensor([4.2287e-07, 1.0000e+00], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.3132619261741638\n",
      "Processing batch number 401 of 900\n",
      "tensor([0.0083, 0.9917], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.31775885820388794\n",
      "Processing batch number 481 of 900\n",
      "tensor([9.9936e-01, 6.4264e-04], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 1.3123222589492798\n",
      "Processing batch number 561 of 900\n",
      "tensor([0.0604, 0.9396], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.3471958041191101\n",
      "Processing batch number 641 of 900\n",
      "tensor([9.9992e-01, 7.7126e-05], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.313303142786026\n",
      "Processing batch number 721 of 900\n",
      "tensor([0.0181, 0.9819], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.32311612367630005\n",
      "Processing batch number 801 of 900\n",
      "tensor([9.9996e-01, 3.7689e-05], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.31328195333480835\n",
      "Processing batch number 881 of 900\n",
      "tensor([0.0130, 0.9870], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.32029661536216736\n",
      "Starting epoch 12 of 30\n",
      "Processing batch number 1 of 900\n",
      "tensor([1.0000e+00, 1.8595e-11], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.31326165795326233\n",
      "Processing batch number 81 of 900\n",
      "tensor([5.1824e-04, 9.9948e-01], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 1.3125040531158447\n",
      "Processing batch number 161 of 900\n",
      "tensor([9.9999e-01, 6.8431e-06], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.313265323638916\n",
      "Processing batch number 241 of 900\n",
      "tensor([0.3170, 0.6830], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.5267923474311829\n",
      "Processing batch number 321 of 900\n",
      "tensor([3.3162e-08, 1.0000e+00], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.31326165795326233\n",
      "Processing batch number 401 of 900\n",
      "tensor([0.0260, 0.9740], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.327497661113739\n",
      "Processing batch number 481 of 900\n",
      "tensor([1.0000e+00, 1.2038e-06], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 1.3132599592208862\n",
      "Processing batch number 561 of 900\n",
      "tensor([0.0024, 0.9976], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.3145444393157959\n",
      "Processing batch number 641 of 900\n",
      "tensor([0.9872, 0.0128], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.32020607590675354\n",
      "Processing batch number 721 of 900\n",
      "tensor([9.2793e-05, 9.9991e-01], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.3133115768432617\n",
      "Processing batch number 801 of 900\n",
      "tensor([0.9551, 0.0449], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.3382156789302826\n",
      "Processing batch number 881 of 900\n",
      "tensor([2.0321e-06, 1.0000e+00], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.31326279044151306\n",
      "Starting epoch 13 of 30\n",
      "Processing batch number 1 of 900\n",
      "tensor([0.8537, 0.1463], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.40075191855430603\n",
      "Processing batch number 81 of 900\n",
      "tensor([7.1018e-07, 1.0000e+00], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 1.3132606744766235\n",
      "Processing batch number 161 of 900\n",
      "tensor([9.9999e-01, 1.3608e-05], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.3132689595222473\n",
      "Processing batch number 241 of 900\n",
      "tensor([9.3941e-06, 9.9999e-01], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.31326669454574585\n",
      "Processing batch number 321 of 900\n",
      "tensor([4.7973e-05, 9.9995e-01], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.3132874369621277\n",
      "Processing batch number 401 of 900\n",
      "tensor([3.7369e-06, 1.0000e+00], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.31326374411582947\n",
      "Processing batch number 481 of 900\n",
      "tensor([9.9993e-01, 6.7024e-05], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 1.3131637573242188\n",
      "Processing batch number 561 of 900\n",
      "tensor([1.3258e-04, 9.9987e-01], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.31333303451538086\n",
      "Processing batch number 641 of 900\n",
      "tensor([1.0000e+00, 6.3108e-07], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.3132619857788086\n",
      "Processing batch number 721 of 900\n",
      "tensor([0.5395, 0.4605], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.7334022521972656\n",
      "Processing batch number 801 of 900\n",
      "tensor([9.9996e-01, 4.2617e-05], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.31328466534614563\n",
      "Processing batch number 881 of 900\n",
      "tensor([9.2499e-05, 9.9991e-01], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.31331148743629456\n",
      "Starting epoch 14 of 30\n",
      "Processing batch number 1 of 900\n",
      "tensor([1.0000e+00, 4.3897e-08], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.31326165795326233\n",
      "Processing batch number 81 of 900\n",
      "tensor([3.3502e-04, 9.9967e-01], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 1.3127719163894653\n",
      "Processing batch number 161 of 900\n",
      "tensor([1.0000e+00, 3.8385e-07], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.3132619261741638\n",
      "Processing batch number 241 of 900\n",
      "tensor([9.9997e-01, 3.3398e-05], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 1.3132128715515137\n",
      "Processing batch number 321 of 900\n",
      "tensor([3.1631e-09, 1.0000e+00], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.31326165795326233\n",
      "Processing batch number 401 of 900\n",
      "tensor([0.0095, 0.9905], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.31838351488113403\n",
      "Processing batch number 481 of 900\n",
      "tensor([1.0000e+00, 6.3850e-09], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 1.31326162815094\n",
      "Processing batch number 561 of 900\n",
      "tensor([0.0068, 0.9932], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.3169482350349426\n",
      "Processing batch number 641 of 900\n",
      "tensor([0.9539, 0.0461], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.3388952612876892\n",
      "Processing batch number 721 of 900\n",
      "tensor([0.8836, 0.1164], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 1.1486153602600098\n",
      "Processing batch number 801 of 900\n",
      "tensor([0.9981, 0.0019], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.3142774701118469\n",
      "Processing batch number 881 of 900\n",
      "tensor([8.4282e-04, 9.9916e-01], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.3137153387069702\n",
      "Starting epoch 15 of 30\n",
      "Processing batch number 1 of 900\n",
      "tensor([1.0000e+00, 1.0782e-06], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.3132622539997101\n",
      "Processing batch number 81 of 900\n",
      "tensor([0.0033, 0.9967], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 1.3084590435028076\n",
      "Processing batch number 161 of 900\n",
      "tensor([9.9910e-01, 9.0090e-04], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.3137466013431549\n",
      "Processing batch number 241 of 900\n",
      "tensor([0.0055, 0.9945], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.31623128056526184\n",
      "Processing batch number 321 of 900\n",
      "tensor([2.1561e-11, 1.0000e+00], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.31326165795326233\n",
      "Processing batch number 401 of 900\n",
      "tensor([1.3198e-04, 9.9987e-01], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.3133326768875122\n",
      "Processing batch number 481 of 900\n",
      "tensor([1.0000e+00, 4.0930e-06], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 1.313255786895752\n",
      "Processing batch number 561 of 900\n",
      "tensor([0.0036, 0.9964], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.3152124881744385\n",
      "Processing batch number 641 of 900\n",
      "tensor([0.9963, 0.0037], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.3152427673339844\n",
      "Processing batch number 721 of 900\n",
      "tensor([0.7098, 0.2902], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.9247523546218872\n",
      "Processing batch number 801 of 900\n",
      "tensor([0.9430, 0.0570], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.3452429473400116\n",
      "Processing batch number 881 of 900\n",
      "tensor([3.7711e-07, 1.0000e+00], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.3132619261741638\n",
      "Starting epoch 16 of 30\n",
      "Processing batch number 1 of 900\n",
      "tensor([1.0000e+00, 3.7572e-08], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.31326165795326233\n",
      "Processing batch number 81 of 900\n",
      "tensor([6.1087e-06, 9.9999e-01], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 1.3132528066635132\n",
      "Processing batch number 161 of 900\n",
      "tensor([0.9797, 0.0203], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.]])\n",
      "current loss 0.3243509829044342\n",
      "Processing batch number 241 of 900\n",
      "tensor([0.0720, 0.9280], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.35404276847839355\n",
      "Processing batch number 321 of 900\n",
      "tensor([0.0240, 0.9760], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.32640257477760315\n",
      "Processing batch number 401 of 900\n",
      "tensor([8.5684e-05, 9.9991e-01], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.31330785155296326\n",
      "Processing batch number 481 of 900\n",
      "tensor([1.0000e+00, 3.4353e-06], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 1.3132566213607788\n",
      "Processing batch number 561 of 900\n",
      "tensor([3.5253e-07, 1.0000e+00], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 1.]])\n",
      "current loss 0.3132619261741638\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[115], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model2\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mLEARNING_RATE)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[114], line 28\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, criterion, optimizer, train_loader, val_loader, num_epochs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent loss\u001b[39m\u001b[38;5;124m\"\u001b[39m,loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     27\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 28\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     31\u001b[0m train_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\optim\\optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m             )\n\u001b[1;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\optim\\adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    158\u001b[0m         group,\n\u001b[0;32m    159\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    164\u001b[0m         state_steps)\n\u001b[1;32m--> 166\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\optim\\adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 316\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\optim\\adam.py:441\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    439\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m--> 441\u001b[0m     \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(params[i]):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.0001\n",
    "EPOCHS = 30\n",
    "\n",
    "model2 = FFF(len(vocab),model)\n",
    "\n",
    "print(device)\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_model(model2, criterion, optimizer, train_loader, val_loader, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.228735831090518e-12, 1.0]\n",
      "You seem to like this thing\n",
      "[0.9993926286697388, 0.0006073883851058781]\n",
      "You seem to dislike this thing\n",
      "[nan, nan]\n",
      "You seem to like this thing\n",
      "[1.8667834638108616e-06, 0.9999980926513672]\n",
      "You seem to like this thing\n",
      "[nan, nan]\n",
      "You seem to like this thing\n",
      "[0.0009728256263770163, 0.9990271329879761]\n",
      "You seem to like this thing\n",
      "[nan, nan]\n",
      "You seem to like this thing\n",
      "[1.0, 1.9274128959523296e-09]\n",
      "You seem to dislike this thing\n",
      "[nan, nan]\n",
      "You seem to like this thing\n",
      "[1.0985942109362559e-13, 1.0]\n",
      "You seem to like this thing\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    inp = input(\"Leave a review:\")\n",
    "    if inp==\"exit\":\n",
    "        break\n",
    "    tokens = tokenizer(inp)\n",
    "    embed = torch.tensor(vocab(tokens), dtype=torch.long)\n",
    "    pred = model2(embed).tolist()\n",
    "    print(pred)\n",
    "    if(pred[0]>0.5):\n",
    "        print(\"You seem to dislike this thing\")\n",
    "    else:\n",
    "        print(\"You seem to like this thing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
