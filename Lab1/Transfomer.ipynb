{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, dataset\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "from data_loading_code import preprocess_pandas\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, classification_report\n",
    "\n",
    "import math\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Tuple\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device is\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.embedding = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.linear = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            src: Tensor, shape ``[seq_len, batch_size]``\n",
    "            src_mask: Tensor, shape ``[seq_len, seq_len]``\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
    "        \"\"\"\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        if src_mask is None:\n",
    "            \"\"\"Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\n",
    "            Unmasked positions are filled with float(0.0).\n",
    "            \"\"\"\n",
    "            src_mask = nn.Transformer.generate_square_subsequent_mask(len(src)).to(device)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.linear(output)\n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([1280,  194,    9,  167,   16,  243,    2,  551,    3,  254,   16,   10,\n",
      "          26,   68,   12]), tensor([   2, 1303,    7,  566,  435,    4,  182,    5,  106, 1295,    6,  419,\n",
      "           1]), tensor([  3,  61, 140,  48,   8,  29,   1]), tensor([   8,   59,   18,  121,    2,  333, 1315,  412,    4,    5,  170,    2,\n",
      "         120,   89,    3,  187,    9,  126,    5,   24,    1]), tensor([  3, 140,  48,   8, 117,   9, 816,  12]), tensor([   5,   30,  119,   21,   14,  124,  674,    6,    4,   10,  209,   51,\n",
      "          47,    2, 1365,   31,  138,    1]), tensor([  73,   11,   25,  123,    2,  162,  327,   22,    3,  138,    4, 1102,\n",
      "          73,   11,   25,   72,    8,   13,    1]), tensor([   3,   92,    2,  606,   49,   16,   14,   94,  333, 1335,    4,    5,\n",
      "          78,   60, 1346,   46, 1127,   14,  892,  257,   38,  155,    1]), tensor([147,  29,   6,   4, 130,  15,  93,  12]), tensor([ 23,  26, 542,  16,   2,  68,   1]), tensor([365,   5,  30,  62, 365,   5,   7, 104,   1]), tensor([  83,   18,   72,   16,  137,    1,    1,    1, 1368,  371,   16,  137,\n",
      "           1]), tensor([ 95,  45, 831,  12]), tensor([996,  29,   1]), tensor([466,  81,  32,   7,  10,  12,   1]), tensor([   2,  490,   15,    2,  112,  763,   20,   38,  106,    5,  379,  557,\n",
      "          10,  234,  211,  228,    3,  268, 1004,  172,   19,    2,  166,  404,\n",
      "         239,    1]), tensor([ 26,  29,   6,  26, 525,   1]), tensor([1340,   62,   83,   18,   72,   12,   12,    1]), tensor([ 88,  27, 102, 196,  16, 193,   4, 669,   2, 899,  41,  15, 613, 128,\n",
      "        502,  22,   3,  27, 177,   9, 746,   1]), tensor([  3,  84,   8, 117,   1]), tensor([   3,  489,   10,  158,  798,    4,   20,   23,  107,   19,    5,    6,\n",
      "          34,    2,   36,   41,    6,  304, 1066,    6,   21,    8,    7,   51,\n",
      "           1]), tensor([   2,  841,  650, 1054,   71,    2, 1061,   22,   33, 1045,    1,  439,\n",
      "           7,   17,    1]), tensor([407,   7,  52,   4, 199,   7,  39,  26,  39, 115,  53,  22, 275,   1]), tensor([   2,  329, 1205,    7,  931,    1]), tensor([   3,  138,   18,  386,  719,    2,  291,   16,  176,  778,   29,   34,\n",
      "           3,  971,    2,  974,   22,    3,  348,   18,   27,   82,    8,  433,\n",
      "        1056,  614,    1]), tensor([  33,   15,   14,  838, 1132,  139,    1]), tensor([ 658,  636,    6,   28,   98,  850,   93,  127,  235,    2, 1297,    1]), tensor([ 28,  64, 231, 270, 494,  19,   5,  24,   2,  49,  90,   2, 972,   7,\n",
      "        735,   1]), tensor([   2,  742,    4,  150,   45, 1095,  257,    1]), tensor([1065,    6,  106,    9,  451,    4,   42,    6,  153,   36,    1]), tensor([109,   7, 147,   4, 864,  15, 538,   1]), tensor([166, 511,   7, 559,  12]), tensor([ 83,  18,  72,  83,  18, 660, 352]), tensor([ 43, 446, 345,   1]), tensor([   3,   11,  464,   65,  784,  604,    6,    4,   14,   32,  536, 1174,\n",
      "          16,   77,  508,    1]), tensor([   2, 1103,   49,  105,   11,   25,  121,    1]), tensor([  3,  61, 598,   9,  18, 260,   8,  74,   5, 255,  78,  23,  43,   1]), tensor([ 51,  47, 238,   1]), tensor([147,  13, 902,   1]), tensor([  2, 150,  16,  21,   4, 161,  45, 134,   1]), tensor([167, 352,   1]), tensor([ 54,  28, 495,   9,  42,   8,  24,  10, 116, 432,  79,   5,   1]), tensor([  64,  142, 1226,  175,    9,  121,  206,   14,  600,  697,  408,  539,\n",
      "         192, 1159,    2,   49,    1]), tensor([   8,   13, 1311,   23,  242,    9,   83,  154,   34,  834,  133,    5,\n",
      "          11,   37,   23,  368,    9,   65,   10,   13,    1]), tensor([  31,  159,  146,   18,  160,  175,   24,  124, 1137, 1330,    1]), tensor([23, 26, 13,  1]), tensor([30, 26,  1]), tensor([  2, 217, 960,   7,   2, 149,   6,   4,  15, 732,   5, 648,   1]), tensor([ 54,  28,  73,  11,  25, 204,   5,   6,  69, 134,   6,  39, 194,   2,\n",
      "        164, 700,  19,  33, 795,  67,   1,   3,  61,  18,  48,   8,   9, 195,\n",
      "          1]), tensor([  3,  73,  11,  25,  56,   8, 477, 302,   1]), tensor([ 14, 422,  20, 147,   1,   1,   1,   1,   1,   8,  20,  14, 856,  81,\n",
      "         32,   6,   4, 191,   5,  20, 143,  75, 104,  47,  14, 248, 158,  70,\n",
      "        101,   3, 895,  12,  12,  12]), tensor([  5, 524, 233, 523,   6, 197, 903,  21,   9,  14, 230,   6,   4, 954,\n",
      "          2, 319, 450,   1]), tensor([  67,   35,    8,   10, 1031,   34,    5,   11,   37,   78,  429,   38,\n",
      "         155,    1]), tensor([  5,  59, 154,   2, 297, 518,   5,  61,   1]), tensor([   3,   27,  579,    9, 1181,    8,   94,   31,  382,  112,  227,    4,\n",
      "          22,   11,   37,  353,  174,  192,  288,    1]), tensor([   8,    7,   10,  185,  325,   13,    9,   42,    6,   34,    2,  647,\n",
      "           7, 1318,    1]), tensor([157,   3, 474, 220,  51, 940,   1]), tensor([ 4,  2, 36, 41,  7, 17,  1]), tensor([   5,   11,   37,  321,   15,  417,    9,   42,   90,   15,  141,    5,\n",
      "         208,    4, 1033,    5,   11,   37,  417,  141,  685,    2,  291,    7,\n",
      "           1]), tensor([ 495,   21, 1062,   53,  148,  194,    4,  194,    1]), tensor([  30,   17,    6,  100,   14,  671,  624,    2,   13,  441,  346,    2,\n",
      "        1128, 1257,  452,   15,  760,    2,  128,    1]), tensor([  3,  84,   8, 111,  12]), tensor([493,  14,  13,   7, 753,   6,  34, 125, 231, 122,  50, 100,   3,  42,\n",
      "          8,   1]), tensor([273,  45, 264,   5, 168,   1]), tensor([336, 399,   1]), tensor([   3,   27,    9,   42,    2, 1228,  300, 1129,    6,   34,    5, 1249,\n",
      "          21,  215,   43,    1]), tensor([  3,  84,   2, 251,   4, 427,  15, 344, 847,   1]), tensor([  5,  11,  37, 102,  14, 394,  32,  16, 193,   1,  17,  36,  26, 149,\n",
      "         26, 329, 665,   1]), tensor([472, 105,  11,  25,  44,   1]), tensor([73, 11, 25, 72,  8, 29,  1]), tensor([  3,  84,  14,  32,   1,   1,  14, 158,  81,  32,   7,  17,   6,   2,\n",
      "        109,   7,  23,  26,   4,   2,  40, 144,   7,  10, 104, 121,   1]), tensor([  3,  35, 165, 193, 277,   4,  60, 978, 221, 129,   1]), tensor([1245,   62,  147,  295,  129,    1]), tensor([ 136,   11,   25,   42,    2,  164,   19, 1265,    6,   18,   26,   24,\n",
      "        1277,   12]), tensor([52, 13,  1]), tensor([   8,   74,    7,   17,    6,  938,    5,    6,  459,    5,  391,  113,\n",
      "        1072,    4,    5,   46,  102,  132,   26,  237,  266,   19,   77,  216]), tensor([  2, 414,  21,   8,   7,  69, 964,  86,  69, 897,   1,   1,   1,   5,\n",
      "        955, 835,  87,  15,  14,  40,   1]), tensor([576,  29,   1]), tensor([   3,   92,    2,  116,   91,    4,   18,  118,   63,   10,  274,    2,\n",
      "          91,   20,  387,    1,    1,    1,    3,  222,    9,  126,    5,   24,\n",
      "           4,    5,  268, 1229,    1]), tensor([ 39, 253, 125, 708,   6,   3, 177,   8,  32,  11,  37, 473,  20,  23,\n",
      "        360,   1]), tensor([   2,  149,   16,    2, 1173,    7,  217,   26,   70,   28,   27,  686,\n",
      "         141,  210,   57,    1]), tensor([  10,  274,  324,   63,    3,  593,    5,    6,    5, 1264,  764,    1]), tensor([1169,  126,   30,   17,    1]), tensor([  5, 240,  75, 104,  47, 183, 442,  90,   3, 361, 882,   4,  22, 178,\n",
      "         24,   2, 190, 530,   1]), tensor([  3, 569,  14,  93, 168,   1]), tensor([   2,   49,    7,   10,  430,  144,   15,  497,    4,   46,   77,  436,\n",
      "          86,  527,  501, 1348,    1]), tensor([   2,   67,  111,   22,  768,   50,    7,    2,  936,  512, 1108,   70,\n",
      "         946,   57,    1]), tensor([ 18, 575,   5,   1]), tensor([ 392,   34,  898,    5,   30,    1,    1,   20, 1101, 1270,  880,    2,\n",
      "         252,  726,   15,    8,   74,    1]), tensor([  2, 126, 138,  18,  44,  23,  43,   1]), tensor([ 162,  145,   39,  488,   27, 1012,    1]), tensor([1109,    4,    5,   30,    1]), tensor([   3,  418,  113, 1225,    5,   21,    2,  800,   15,   14, 1081,   86,\n",
      "         168, 1106,  452,    1]), tensor([364,  13, 139,   1]), tensor([  5,   7, 185,   9,  42,   4,   3,  56,   5,   1]), tensor([   3, 1358,    3,   98,  263,    2,  164,    4,   66,  168,   14,   93,\n",
      "           1]), tensor([1363,   44,  127,   86,  622,   16,   50,    1]), tensor([ 29,  20,  52,   4,  30,  51,  47,   2, 165,  33,   4, 645,  20,   5,\n",
      "        682,  12]), tensor([ 72,  10, 200,  13,  62,  34,  18,   8,   1]), tensor([ 18,  39,  26,  39,   3,  35, 906,   1]), tensor([   5, 1134,  132,   63,    3,   11,  137,  188,    5,   16,   79,  212,\n",
      "           6,   38,    3,  108,  337,  610,   33,   90,    8,    7,    2,  103,\n",
      "          32,    3,   11,   96,  139, 1074,    1]), tensor([ 30,  56,  10, 289,   1,   1,  30,  39, 371,   1]), tensor([   3,  956,  672,    2,  198,   21,    2, 1191,    4,    3,   35,    9,\n",
      "         503,    2,   13,   87,    9,  561,    5,   21,   97,  161,    1]), tensor([   2,  198,  208,   38,  550,    4,  430,    6,    5,    7, 1186,    1]), tensor([ 446, 1271,    8,    7,   10,   26,   41,  116,   91,    4,  110,   11,\n",
      "          37,   18,  143,  515,   79,    5,    1]), tensor([   3,  718,    2,  291,    4,   88,  556,   50,   22,    6,  373,    2,\n",
      "         164,   20,  146,  354, 1341,    6,   54,    3,  136,   11,   25, 1123,\n",
      "          14, 1145,    3,   20,   87,   15,  998,    1]), tensor([   3,  437, 1357,  535,    1,    3,  114,  717,  690,    4,  935,  148,\n",
      "          15,    8, 1115,    1]), tensor([ 157,    6,    2,  959,   45,   38, 1291,   22,    3,  530, 1139,    2,\n",
      "         578,  150,    1]), tensor([105,  11,  25, 248, 160,   1]), tensor([ 973, 1121,    7, 1362, 1352,   12,    1]), tensor([ 77, 217, 927,   1]), tensor([  88,  455,  878,   51,    4,   51,   70,    8,    7,   14,  553,   33,\n",
      "           4,    3,   11,   96,   35,  480, 1080,   69,   57,    1]), tensor([110,  20,  38, 143, 916, 184,   8,  13,  22,   3, 621,   5,  20,   2,\n",
      "        103,   6,  14, 327,   1]), tensor([ 29,   7, 420,  39, 756,   1]), tensor([ 8, 13, 30, 17,  1]), tensor([252,  41,   1]), tensor([   3, 1148,   35,  216,  362,    3,   98,   18,  269,  398,   16,   75,\n",
      "          47,  211,  228,  229,  771,    1]), tensor([   5,  156,   38, 1192,   22,    2,   40,  905,   59,   18,  118,  254,\n",
      "           9,   65,  188,    4,    2,   36,    7,   51,  766,  554,   55,   40,\n",
      "         664,    1]), tensor([17, 49,  4, 68, 12]), tensor([   2,  166,  511, 1287,    2,  893,    7,   52,    1]), tensor([52, 29, 16,  2, 68,  1]), tensor([1069,    6,    3,   80, 1130,    9,   27,   10,   13,  101,   46,   71,\n",
      "          14,  618,    4,  293,   24,    4,  178,   17,  109,    1]), tensor([ 158,   40,  874, 1135,    2,  103,   12,    1]), tensor([ 26,  29,  62, 449, 357,   1]), tensor([ 625,  458,   19,   76, 1207,    4,  701,    1,    3,   48,   28,    8,\n",
      "          49,   12,   12]), tensor([213,  15,   5,  30,   6, 108,  73,  11,  25,  72,   5,   1]), tensor([   5, 1100,  994,   47,  115,  256, 1241,   15,    8,  349,    2,   68,\n",
      "           7,   38,  252,   22,  183,   61,  552,    2,   41,    7,  456,    6,\n",
      "         157,    6,    5,   11,   37,   18,    1]), tensor([   3, 1190,    2,  246,    6,    4,  177,    8,   33,    9,   65,    2,\n",
      "         103,  357,    1]), tensor([ 76,  49,   6, 240,  26,  24,  55, 311,   1]), tensor([ 26,   6,  30, 119,   1]), tensor([ 16,   2,  68,  21, 167,   6,   5,   7,  97,  52,  29,   6, 101,   3,\n",
      "         61, 140,  48,   1]), tensor([  3,  82, 112,  15, 148,   4, 475, 114, 152,   1]), tensor([ 31, 159,   7, 217,  26,   1]), tensor([   2,  128,   59,   66, 1232,  301,   90,    5, 1302,   55,   40,    4,\n",
      "         305,    1]), tensor([342,  31,  62,  26,  72,   1]), tensor([ 154,   79,    5,    7,  119,    4, 1143,   16,    2,   68,    3,    1,\n",
      "         412,    1]), tensor([  3,  11,  96, 187, 218, 200, 300,  16,  14, 135,  13,   4,   8, 158,\n",
      "         33,   7,   2, 120,  33,   3,  11,  96, 177,  22, 156,  14,  40, 290,\n",
      "          1]), tensor([  3,  61, 140,  48,   8,   1]), tensor([ 17, 499, 491, 582,  13, 699,   1]), tensor([ 17, 394,  12]), tensor([52, 81, 32,  1]), tensor([ 76, 314,  16,   2,  68,   4,  88,  44,  17,  12]), tensor([  14,   13,  105,   11,   25, 1223,  196,   14,  116,   99,    4,    2,\n",
      "         889, 1117,   14,   13,   53, 1227,   87,   15,   14,  891,    1]), tensor([ 3, 84,  8, 81, 12]), tensor([134, 109,   1]), tensor([  14,   13,  532,  483,   70,   18,   17,   62,  483,   57,    6,   34,\n",
      "          14,  223,   11,   37,   13,   20,  372, 1300, 1320,    6,  265,  136,\n",
      "          11,   25,  563,   10, 1364,  229,  518,   21,    5,    1]), tensor([925, 297,   1,   1,   1,   1,   3,  35,   9, 263,   5,   1]), tensor([   5,  752,    2, 1133,   15,   10,   81,   32,    1]), tensor([  3,  27, 102,  23, 345,  19,   8, 135,  13,  53, 747,  33,   1]), tensor([  3, 487,   8,  29, 120,   4,  20, 564,  19,   5, 923,   1]), tensor([  88,   83,   18,  248,  310,    6,   34,    7,   18, 1070,  826,    9,\n",
      "         262,    1,  106,    9,  486,    4,    2,   36,    7,  143,   51,   47,\n",
      "         488,    3,   27,  187,    1]), tensor([  4,   3, 108,  84,   2, 698,  12]), tensor([ 62,  57, 482,   6,   2,  91, 524,   9,  44, 119,   1]), tensor([  14, 1183,   46, 1272,  780,   15,  299,   21,  641,  192,  919,  416,\n",
      "           1]), tensor([  3,  27, 187,  95, 662,  19,  14, 397,   4,  14, 453,   4,   5,  30,\n",
      "        108, 119,   1]), tensor([388,  66,   2, 186,   9,  44,  19,  14, 397,   1]), tensor([ 88,  83,  18, 389,  79,   2, 400,  33, 384,   1]), tensor([  5, 105,  11,  25, 123,  28, 251, 173,   1]), tensor([ 814,   28,   58,  432, 1020,   11,   37,  548,  544,    1]), tensor([  3,  80, 490,   8,  19,  14, 319,   6,   4,   3,  98,  18,  65, 312,\n",
      "         19,   5,  38, 155,   1]), tensor([ 131, 1111,   50,    9,   66,    8,  207,    6,    3,   27,   77,  917,\n",
      "           1,    1,    1]), tensor([  3,  60, 358,   2, 496,   9,  65,   2, 127,  33,   6,  34,   5,  46,\n",
      "         69, 253, 454,  16,  50,   1,   2,  26]), tensor([  3,  11,  85, 335,   1]), tensor([ 31,   7, 147,   1]), tensor([61, 48,  8, 74,  1]), tensor([143, 250,  47,   2, 320,   3,  20, 241,   9, 262,   5,  19,   1]), tensor([295, 129,  20, 147,   1]), tensor([30, 43,  1]), tensor([  2,  31,   7, 356,  39,  43,  39,   2, 129,  42, 612,   1]), tensor([  8,   7,  10,  26,  31,   6,   4,   5,  92, 315,  60, 308,   1]), tensor([  64,    6,   54,   55,   13,    7,  236,    6,    8,   49,    7,   18,\n",
      "         241,    9,  519,    5,    6, 1242,  100,  236,  305,  235,    1]), tensor([  3,  11, 137,  56,   9, 263,   5,   1]), tensor([ 36,  41,  21, 197, 303,   7,  52,   6,   3,  42,  32,   9, 151,  14,\n",
      "        223,   4, 376,  14, 223,   9,  42,  32,   9, 151,  50,  12,   1]), tensor([   2, 1278, 1017,  239,    7,   60, 1310,    9,   42,    1]), tensor([142,  18, 566, 435,   1]), tensor([134, 121,   6, 190,  69, 169,   1]), tensor([   8,   29,   35,   10,  541, 1180,  529,   22,   20, 1319,   63,   10,\n",
      "         191,    4,  390,   50,    9,  263,    5]), tensor([  17,   16,  189,   19,   55,  317, 1251,    1]), tensor([17, 68,  6, 69, 12]), tensor([1313,  378,    1]), tensor([215, 144,  15, 207,   1]), tensor([142, 424, 350,   4, 547, 310,   9,  83, 279,  19,   5,   1]), tensor([   8,    7,   10,   23,  627,   13,   19,  134,   31,  159,   22, 1059,\n",
      "          21,   10,  360,  328,    1]), tensor([205,   6,  35,   9, 546, 555,   1]), tensor([269, 377,  53,   2, 506,  12,   1]), tensor([  14,   32,   30,  108, 1088,    1]), tensor([   2,  995, 1060,    7,   17,    6,    2,  656,   19,    2,  977,    7,\n",
      "          23,    1,    1,    1,  616,    1]), tensor([226, 117,   1]), tensor([  2, 472, 110,   7,  10, 951,   6,   4,   2, 149,   7, 340, 252,   1]), tensor([   1,    1,    1,    1,   74,  280,  339,    4,   30,   17,   19,   14,\n",
      "        1019, 1086,  344, 1187, 1224,   13,    4,  531, 1116,   36,   24,   40,\n",
      "        1105,    1]), tensor([ 76, 458,   1]), tensor([60, 26, 29,  1]), tensor([ 309,    6,   63,  353,   86,  855,  555,    2,  534,   15,    2,  965,\n",
      "         170,    4,    3,   98,   18,   42,    5,  115,  467,   21,    2, 1333,\n",
      "           1]), tensor([1326,   13,    6,  528,  748,    1]), tensor([   2,  261,    7,   23,  403,    6,    3,   11,   96,  102,  276,    9,\n",
      "        1176,  196,   14,  908,   19,    2,   13,   24,    2,  984, 1178,   19,\n",
      "          77, 1149,   41,  454,    1]), tensor([496,  81, 818,  72,   1]), tensor([  2, 109,   7,  52,  12]), tensor([  26, 1306,  225,    1]), tensor([  3,  20, 374, 133,   2, 507, 619,  15,   2, 112, 331, 460, 135,  13,\n",
      "        381,   4,   4, 133,  10, 857,  15,   2,  68,   1]), tensor([  3,  84,   8,  13,   6,   5,   7,  23, 440,   4,  46,  10, 209,  15,\n",
      "        839,   1]), tensor([105,  11,  25, 444, 152,   1]), tensor([ 71,   5, 557,  20,  33, 411,  53,  79, 928, 584,   2, 961, 729,   4,\n",
      "          5,  20, 736,   1,   3,  80,  18, 180,   4,   3,  80,  18, 968,   1]), tensor([ 83,  18,  72,  54,  28, 569,   9,  42,   2, 316,   1]), tensor([  3,  27, 337,  95,  16, 197, 307,   4, 861,   6,   4,  71, 808, 221,\n",
      "        232,   4, 415,  15,  42]), tensor([205,   6, 205, 502,   1]), tensor([ 144,   15, 1308,    1]), tensor([ 17, 357,   1]), tensor([30, 16, 50,  1]), tensor([126,  20,   2, 578, 349,   1]), tensor([629,   9,  42,   4, 356,   1]), tensor([   2,   33,  169,  783,   15,    2, 1038, 1098,    7,   22,    2,  150,\n",
      "          21,    2,   13,   11,   37,  436,  733,   22,  459,   28, 1084,    4,\n",
      "        1221, 1237,  465,   87,   63,   10,  176,  522,    1]), tensor([ 789,  218,  200, 1025,  172,    6,    3,  827,  299,    4,    5,   20,\n",
      "          24,   97,  375,   22,   35,  227,    1]), tensor([124, 309,  92,   2, 166,  41,  15,  10,  81,  32, 127,   1]), tensor([26, 49, 12,  1]), tensor([  77,   40,  987,  474,    6,    5,   11,   37, 1292,    4,    2,   36,\n",
      "           7,   17,    1]), tensor([  2,  31,   7, 132,  43,  39,  10, 513,  16,   2, 331,  22, 286,  19,\n",
      "          2,  13, 184, 193, 277,   1]), tensor([ 59, 154,   5, 348,   4,  75,   1]), tensor([  5,   7,  23, 104,   9, 361,  39,  43,   6, 101,   7, 500,   2, 183,\n",
      "        245, 620,  79, 189,  10,  49,   1]), tensor([ 141, 1258,    7,   22,  224]), tensor([   3,   82,    8,   13,   39,   10,  513,   16,   14, 1247,    4,   27,\n",
      "        1156,    5,  266,    1]), tensor([  3, 343,   5, 339,   4,   5,  30,  17,  12,  12]), tensor([ 273,   27,  187, 1321,    4,   88,  197,  306,  363,  212,    1,    1,\n",
      "        1126]), tensor([   5,    7,    2,  103,   91,    3,   27, 1196,   21,    2,  470,  579,\n",
      "           1]), tensor([202,  19,  31,   1]), tensor([17, 13,  1]), tensor([ 17,  16,   2, 320,   1]), tensor([1262,    6,  183,   15,    2,  542,   59,   18,   44,   19,   14,   13,\n",
      "           1]), tensor([571, 949,  21,   2,  13,   1]), tensor([ 145,    7,   22,    2,   40,  988,   45,  181,   15,  360, 1007,    4,\n",
      "         285,  301,    1]), tensor([  81,  261,    7,   26,   62,   10,  176,  174,  277,    3,  249,   14,\n",
      "          13,   24,    2, 1314,    6,   92,   10,  151,    6,    4,  667,    2,\n",
      "         401,  192,   10,  901,    1]), tensor([364, 295, 129,   1]), tensor([321,  15, 849, 196,   1]), tensor([ 165,   11,   37,  638,    6,  157,    6,   45,  201,    9,  563,  118,\n",
      "        1283,  221, 1119, 1096,   45,  185,    1]), tensor([   3,   58,  379,  139,  122,   21,    5,    4,   80,  715, 1185,  131,\n",
      "         224]), tensor([  3,  27, 112,  75, 193, 249,  24,   8, 720,   4,   3, 313,   8,  13,\n",
      "          1]), tensor([   8,   29,    7,  918,   16,  125,   56,   50, 1350,  203,   45,   23,\n",
      "        1198,    1]), tensor([131,  10, 169, 130,  15,  89,   1]), tensor([   3,   42,    8,   29,   24,   10, 1036,  721,  677,  362,  110,    7,\n",
      "          10,  209,   15,  244, 1334,  913,   53,    2,  810,    6,    4,    5,\n",
      "          30,   17,   12]), tensor([966,  88,  27, 102, 424,  76,   4, 443,  21,   2,  13,   1]), tensor([  5, 156, 290,  24, 302,  40,   6,   2,  36,   7, 153,   4, 210,   6,\n",
      "          4,   2, 152, 323,  10, 234,  15, 174,   1]), tensor([1289,    9,   91,   16,  402,  457,   75,   47,  211,    1, 1002,  216,\n",
      "          12,   12]), tensor([ 31, 323,  67,  10, 176, 318,   1]), tensor([ 23,  26,  29,   6,  43, 181,   1]), tensor([   3,  187, 1274,  217,  210,   34, 1211,   21,    2, 1276,  178,  484,\n",
      "           4,    3,   20,  146,  556,    5,  359,   11,   25,   17,    1]), tensor([ 46, 102, 132,  17,   1]), tensor([   3, 1131,    8,   16,    2,  116,   91,    4,    5,   59,   18,   44,\n",
      "           1]), tensor([426, 797,   1]), tensor([  8,   7,  10,  17, 325,  74,   1]), tensor([  8,  74,  78,  17,   6,  34,   5, 170,  63, 212,  15,  42,   1]), tensor([  3, 577,  11,  25,  48, 659,   8,  29,   1]), tensor([ 43, 332,   6, 280,  21,  89,   6,   4,  30,  39, 942,   1]), tensor([1079,   21,    2,   40,    1]), tensor([  3,  58,  11,  25,  42,   8,  49,  90,   2, 529,   7, 772,   1]), tensor([  8,  29,   7,  23, 244,  41, 393, 294,  12,  12,  12,  12,  12,  12]), tensor([533,  26, 509, 259,   4, 801,   6, 142,  22, 185,  12]), tensor([  3,  20, 276,   9,  83, 166, 404,  24,   2, 116,  19,  77, 145,   1]), tensor([  3,  80,  18, 180,  19,   8,   4,   3,  61,  18,  48,   8,  74,   9,\n",
      "        195,   1]), tensor([ 191,    3, 1005,    9,  635,    2,  969,  534,  168,   24,  258,    6,\n",
      "           2, 1018,   99,   46,  175, 1256,   22,    5,  114,  285,   21,    2,\n",
      "         476,  411,    1]), tensor([107,  38, 155,  12,   1]), tensor([  3,  27,   9, 950,   2, 126,   9,  66,   5,   9, 463, 113, 127,   9,\n",
      "         66, 403, 149,   1]), tensor([ 213,   15,    2,  353, 1219,   88, 1199,   19,    2,   32,   61,  269,\n",
      "          24,   14,  203,    1]), tensor([ 2, 31, 30, 17, 12]), tensor([   3,  177,    8,   29,    9,   65, 1336,   69,  169,    1]), tensor([1168,  318,  324,    1]), tensor([120, 161,   2, 109, 352,   6,   3,  27, 255,  35,  75,  47, 227,   6,\n",
      "        139,   1]), tensor([  2,  74, 343,  20, 730,   1]), tensor([366, 235,   2, 781,   1]), tensor([ 17, 311, 434, 117,   1]), tensor([   3, 1112,   75,  762,  703,  354,    2,  888,   86,  284,   13,    6,\n",
      "          34,    3,   27,    2,  845,  512,    4,    5,   11,   37,   10,   17,\n",
      "         395,   12]), tensor([  28,   58,  118,  270, 1197, 1110,   19,    2, 1068,   70,  829,   57,\n",
      "         407,    6,   23,  173,    1]), tensor([   8, 1087,  517,    1]), tensor([   5,   46,  102,   10, 1354,   16,  565,    1]), tensor([   8,   20, 1328,  712,  133,  120,    6,  101,  390,   50,    9,  992,\n",
      "          10,  234,   15,   23,    6,   23,  245,  293,    1]), tensor([  5,  11,  37, 156,  56,  10, 883,   4,   7, 541,   6, 523,   6,   4,\n",
      "        788,   1]), tensor([  3,  35,   9, 179,   9,  10, 351,   4,  82,  10,  94, 477,  13, 101,\n",
      "          7, 132,  17,   1]), tensor([131,  10, 130,  15,  93,   4,  89,  12,   1]), tensor([ 28,  11, 464,  84, 141, 550,   5,   7,   1]), tensor([  3,  80,  75,  47, 107,  19,   8,  29,   1]), tensor([ 18,  10,  26, 282,   1]), tensor([  5,  20, 205,  12,   1]), tensor([ 64, 201,   9, 338,  21,   1,   3,  11, 137,  48, 628,   8,  29,   1]), tensor([ 10,  26,  41, 282,   1,   1,   3,  82,   8,  63,   3,  82,  10, 684,\n",
      "         53, 169, 993,  22, 532, 378,   4, 125,  21,   2, 256, 303, 136,  11,\n",
      "         25, 122,  50,   1]), tensor([ 83,  18, 123,   2, 162, 327,  39,  50,   1]), tensor([17, 13,  1]), tensor([  54,   28,  313,  794,    6,  281,    8,   13,  171,   71, 1008,    1]), tensor([ 355,    5,  114,   18,  510,   14,  319,   37,    6,  758,  714,    5,\n",
      "          53, 1039, 1113, 1240,   70,  920,    6,  830,   31,    6, 1339, 1067,\n",
      "           6,  419,   57,    1]), tensor([  2, 473,  64,  30,  43,   6,  34,  70, 591,   9, 125,   3,  27, 663,\n",
      "         57,   5, 617, 154,   1]), tensor([110,  11,  37,  60, 330, 134,   3,  58, 520,  79,   8,  32,   1]), tensor([  3,  11,  85,  10, 384, 202,   1]), tensor([  5,   7,  10, 952,   9,  42,   1]), tensor([   3,   27,   35, 1023,   16,   79,   10,  275,    4,    8,  687,    3,\n",
      "          82,  220,   16,    2, 1165,   15,    2,  307,    1]), tensor([846, 535, 498,   6, 498,   1]), tensor([  8,  31,   7,  97,  52, 282,  12]), tensor([336, 166, 232,   1]), tensor([   5,    7,  461,    6,  106,    9,   42,    6,    4,   46,   23,  153,\n",
      "         109,    4, 1305,    1]), tensor([  3, 204,   8, 933,   4,  38, 114, 500,  65, 514,   8,  13,   4, 493,\n",
      "        679, 668,   1]), tensor([ 157,    6,   63,   79,   10,  275,    6,    2,  848,  268,    9,   66,\n",
      "         989,    4, 1360,    4,  815,   14,  128,  222,  284,    4,    3,  136,\n",
      "          11,   25, 1146,    4,  258,  172,    1]), tensor([  5,  11,  37,  60, 106,   1]), tensor([  54,   28,   45,  342, 1075,    1,    1,    1,   28, 1041,   27,    8,\n",
      "          12]), tensor([   8,   13,    7,   23,  308,   19,  264,  115,  321,   15, 1016,    4,\n",
      "        1343,  651,    7, 1214,  836,   47, 1118,  214,    3,   27,  188,    1]), tensor([   3,   11,   96, 1027,  480,  172,   90,   15,    8,  508,    1]), tensor([ 23, 773,   1]), tensor([  63,  288, 1071,    6,   95,  381,   44,   17,    1]), tensor([ 482,    4,    3,  854,    9,   64, 1011,    2, 1345,  395,  416,    5,\n",
      "          46,   21,   55,   13,    1]), tensor([  8,   7,  10,  17, 749,   1]), tensor([  3,  35,   9, 260,  10, 200,  49,   1]), tensor([  23,  153,    6,   41,   36,    4,   28,   73,   11,   25,   27,    9,\n",
      "        1015,   19,    2,   36,   21,   55,  453,  266,   28,   27,    2,   36,\n",
      "         150,   21,    2,   32,    1]), tensor([   2,   31,  159,    7,  140, 1317,    1]), tensor([ 28,  66, 423, 211,  38,  22,  28,  58, 670,  87,   2, 151,   4,  18,\n",
      "         66, 740, 161,   1]), tensor([ 17,   5,  20,  94, 332,  76,  30,  26,   6,  77, 216,   4,   5, 286,\n",
      "         24, 250,  89, 163,   3, 238,  12,  12,  12,  12]), tensor([51, 47, 94,  1]), tensor([455, 113,   2,  26,  44, 167,  12,  12]), tensor([ 165,  548,  544, 1338,   14,  554,   10,  176, 1122,    6,  213,   15,\n",
      "         101,   78,    4,    3,  418,  113,  243,    9,   83,   10,  242, 1138,\n",
      "           6, 1355,   87,   71,   14,  296,    1]), tensor([   8,    7,  173,   90,  183,  287,   45,  108, 1057,  110,  601,    2,\n",
      "         128,    9,   66,   71,  346,  113,    1]), tensor([   2,  128,  349,    7,  169,    6,  957, 1077,  982,   43,  175,    6,\n",
      "           4,    2,  199,   41,    7,   52,   16,   10,  199,   13,    1]), tensor([   3,  118,  865,  391,    5,  228,    3,  222,    9,  633,    4,  562,\n",
      "         161,  385,  558,    4, 1351,    4,  479,   22,    5,   67,   35,  580,\n",
      "         249,   24,    2, 1032,    1]), tensor([  3,  27,  35,   8,  13,  16, 184,  10, 275,  99,   6,   4,   3, 114,\n",
      "        549,  28,   6, 142,  18,  22,  17,   1]), tensor([104,   6,  76, 261,   6,  26,  31, 159,   1]), tensor([  5,  46, 154,   3, 254,   4,   3, 136,  11,  25, 376,  16,  75,   1]), tensor([17, 68, 64, 12]), tensor([  3, 337,   8,   4, 363, 174,   5,  20,  77, 467, 132,  12,  12,  12,\n",
      "         12,  12,  12,  12,  12,  12]), tensor([948, 537,  39, 448,  53,   2, 297,  45,  18, 448,   1]), tensor([1234, 1210,  944,    8, 1239,    1]), tensor([  49,   20,   75,   86,  250,   97,  423,   22,    3, 1063,  338,   21,\n",
      "          34,  324,  770,   90,    5,  346,   14,   40,    1]), tensor([1366,    1]), tensor([  3,  61,  18,  48,   8,  74,   9, 195,   1]), tensor([ 10, 144,  15, 207,  22, 170,  63, 229,  21,  14,  13,  16, 174,  12,\n",
      "         12,  12]), tensor([ 10, 406,   1]), tensor([   5,  323,  250,   47, 1052,  211,    6,   54,    3,  370,  272,    9,\n",
      "          42,    2,   13,    1,   14,  223,   46,    2,  162,   13,   19,    2,\n",
      "         162,  145,    1]), tensor([ 12,   3, 755,  48,  12,  12]), tensor([462,   4,  30,  43,   1]), tensor([   2,  939, 1006,    7,  456,    1]), tensor([  8,   7, 528,   2, 103,  81,  32,  16,  36,  41,  12]), tensor([   3,   48,  445,    9,  195,   19,  200,  646,  135, 1092, 1099,   24,\n",
      "           2,  307,    1]), tensor([  38,  110,    7,   77,  190,   16,   50,    9,  126,    5,   24,  315,\n",
      "          24,    2,  565, 1323,    3,  179,  171,   10,  723,    1]), tensor([273,  27, 885,  10, 209,  15, 710,  21,   5,   1]), tensor([  38,  195, 1044,   28,  114,  122, 1082,   15,   55,  401,    1]), tensor([560,   9, 123,  10, 151,  21,  95,   7,  97, 823,  24, 862,   1]), tensor([   3,   20,   23,  335,    9, 1193,   22,    3,   98,  262,   14,   43,\n",
      "        1309, 1273,  316,   16,   14,  640,  431,    1]), tensor([ 355,    5,   11,   37,  106,    9,  369,  592,  148,   19,    2,  876,\n",
      "         150,   54,   28,  369, 1301,    2,   13,    9,   55,  305,  191,  981,\n",
      "           1]), tensor([   2,  149,  546, 1177,   87,   15,    2,  164,    9,   10,  759, 1322,\n",
      "           1]), tensor([ 73,  11,  25, 130,  55,  93,   4,  89,   1]), tensor([141,  58,  22,  65, 224,   2, 225,  41,   7, 336,   1]), tensor([ 17, 225,  12,   1]), tensor([  3, 140,  48,  95,   4, 806, 125,   9, 437, 148,  10, 272,   1]), tensor([68,  7, 26, 69,  1]), tensor([ 142,   10, 1299, 1076,    1]), tensor([  3,  84, 229, 276,   9,  42,  33,  32,  16, 197, 171, 963,   4, 135,\n",
      "          1]), tensor([ 16,  10,  29,  22, 727,  39, 143,  39,   8,  33,  59,   6,   3, 421,\n",
      "          5,   9,  44, 155,  51,   4,  19, 887, 415,  47,   8, 111,  59,   1]), tensor([   3,   56,    2,  833,   22,    5, 1167,  976,  599,   55,   40,    6,\n",
      "         341,   47,  450,    1]), tensor([  3,  11,  96,  35,   8, 642,  32,  16, 220,  89,  99,   4, 146,  18,\n",
      "        104,  19,   2, 190,   5, 156,  21,   2,  40,   1]), tensor([281,   8,  33,  54,  28,  58,   1]), tensor([  5,  20, 340, 104,  24,   2,  40,   1]), tensor([   8,  117,    7,   17,   24,  218, 1218,    1,   57]), tensor([73, 11, 25, 72,  5,  1]), tensor([  2, 590, 186,   7, 372, 649,   1]), tensor([264,   5, 168,   1]), tensor([  64,    2,  375,  362,   14,  164,  170,   57,    1,   62,    3,   11,\n",
      "          85,   18,   69,  852,   15,    2,  999, 1253,    1]), tensor([ 909, 1172,   45,   18,    2,  103,    6,    4,  475,   45,    2,  871,\n",
      "           1]), tensor([  2,  13, 547, 310,   9, 152,  56,   9, 318, 983,   1]), tensor([   3,   11,   85,  189,    5,   19,   97,  947, 1244,   70,   19,   49,\n",
      "          57,    4,    5,  156,  119,    1]), tensor([  5,  30,  17,  19,  10, 116,  91,   6, 304,  54,  28, 231, 126,  24,\n",
      "        112, 595, 133,   2, 162,  89,   1]), tensor([   2, 1003,   15,    2,  466,  793,  442,  306,    1]), tensor([  3,  11,  85, 438,   3, 177,   8,  29,  21, 167,   5,   7, 242,   9,\n",
      "        204,   6,   5, 359,  11,  25, 244, 259,   1]), tensor([   3,  146, 1001,   22, 1030, 1209,   11,   25,  123,  314,    6,  273,\n",
      "         108,  481,   73,   11,   25, 1203,  175,  777,    9,  725,    2,  298,\n",
      "         184,    9,  912,    1]), tensor([125, 728, 122,  50, 271,   4,   3,  35,   9, 503,  87,   2, 796,   4,\n",
      "        271,  21,   2,  13,   1]), tensor([  2, 109,  46, 102, 875,  26,   1]), tensor([   3,   27,   35,  216, 1359,  900,  786,  219,    4,   75,    1]), tensor([ 5, 78, 23, 43,  1]), tensor([   3,   27, 1140,  256,   11,   37,  515,  315,   34,    3,  896,   11,\n",
      "          25,   35,  115,  145,   19,    5,    1]), tensor([  33,  111,    3,  313,    7,    2, 1028, 1202,  657,  133,    2,  527,\n",
      "           1]), tensor([   3,   84,    8,  198,   62,    5,  602,   50,    9,  713,  115, 1024,\n",
      "         117,    9,   14,  491,    1]), tensor([131,  10, 130,   1]), tensor([   2,  445,  681,    4, 1293,   45,   60,   17,    1]), tensor([ 54,  28,  27, 218, 779,  86, 218, 914, 293,   6, 163, 922,   2, 866,\n",
      "         15, 264, 792,  15, 148,  33, 171,  33,   1]), tensor([ 426,   72,    4,  114,   66,  194,   16, 1347,   14,  476,   13,    7]), tensor([  5, 904,  10, 152,  16,  10, 160,  89,   6,   7, 509, 104, 354, 986,\n",
      "        711,   4,   2,  41,  15,  36,   7, 559,   1]), tensor([ 76, 408, 539,  16, 317,  86,  44,   1]), tensor([  26, 1212,    6,  344,    1]), tensor([  2, 158, 413, 121,  14, 203,  23,  43,   1]), tensor([   3,   20,   23,  821,    9,   66,    8,   32,   90,    3, 1284,    5,\n",
      "          20,   60,  741,    1]), tensor([   3,   92,    8,   13,   21, 1144,   53,   10, 1157,    4,    3,   11,\n",
      "          85,  438,    3,  138,    1]), tensor([17, 13,  1]), tensor([ 60, 335,  19,   8,  29,  38, 155,   1]), tensor([858, 859,   1]), tensor([173,  13,   1]), tensor([   2,  644,  329,  666,    7,  607,    6,  101,    7,   23,  245,   16,\n",
      "          10, 1051,  705,    1]), tensor([  32,   30,   17,  367,   20,  332, 1047,    9,  281,  115,  744,    1]), tensor([  3,  11,  85,  60, 202,  71,   3,  27,  99,   7,  10,  91,  22, 105,\n",
      "         11,  25,  44,   1]), tensor([ 52,  36,   6,  31, 159,   4, 929,   9, 643,  12,   1]), tensor([449,  12,   1]), tensor([  3,  20,  18, 180, 171,   8,  29,   1]), tensor([   2,   13,   58,   64,  270,   17,  494,    4,  118, 1331,  696,    1]), tensor([   2,  199,    6,  373, 1136,  133,   97,  447,    1, 1010,    6, 1160,\n",
      "         921,   22,  425,   43,  382,  825,   15,  543,   10, 1158,  244, 1163,\n",
      "           1]), tensor([  3,  67, 188,   5, 112, 174,   6,   4,   5, 359,  11,  25, 278, 106,\n",
      "          9, 122,  19,   1]), tensor([ 23, 202,   1]), tensor([  3, 304,  84,   2, 160,  31, 159,   1]), tensor([ 88, 181,   8,  49,  69, 267,   4,   7,  23, 201,   9, 451,   1]), tensor([   2, 1042,  537,    5,    1,   14, 1236,   20,  775,    1]), tensor([ 74,  59,  18, 326, 334,   1]), tensor([   3,   82,    5,   16,   14, 1034,    4,  265,   35,   10,  145,   19,\n",
      "           2,   31,    1]), tensor([ 23, 202,  24, 589,   1]), tensor([ 213,   15,    2,   94, 1055,   27,  139,  340,   78, 1125,    1]), tensor([  88, 1154,    9, 1153,   86,  262,    1]), tensor([ 28, 254, 112, 311,   9, 486,   2, 128,   1,   8, 186, 943,   7, 750,\n",
      "        484,   4, 231, 706,  19,  94, 186, 757,   1]), tensor([   2,  298, 1021,   65,  812,   24, 1281,   34,    3,   98,   18, 1246,\n",
      "         243,   95,   24,   14,   40,    1]), tensor([  5, 990, 292,   9,   2,  13, 118, 100,   5,   7, 363,  10, 234, 428,\n",
      "        383,   2, 112,   1]), tensor([  5,  20,  97, 934, 144,   6,  34,   3,  61, 146,  27, 238,  51,  41,\n",
      "          1]), tensor([ 26, 501,   4,  59,  18, 123,  13,  69, 655,   1]), tensor([   2,  316,   22,  280,  138,   18,  326,    2, 1093,   24,    2,  594,\n",
      "           1]), tensor([   3,   80,   23,  180,   19,    2,  247,   22,  124,  138,   21,    2,\n",
      "        1259,   15,    8,   13,    1]), tensor([143,  51,  47,   2, 242, 497, 287,   1]), tensor([  3,  82,  95, 907,   3,  98, 123,  14,  81,  32, 121,  51,  34,  95,\n",
      "        551, 181,   5, 924,   9, 361,   1]), tensor([ 22,  33, 405,  11,  25,  44, 302,   1]), tensor([178,  10, 219, 100, 256, 165, 214, 574,  11,  25,   1]), tensor([  58,   11,   25,  351,  279,   34,   13, 1050,    9, 1215,    1]), tensor([   8,   29,    7,  691,   18, 1142,   16, 1120,   89,    6,    3,   73,\n",
      "          11,   25,  389,  141,  173,    5,  208,    6,   54,   28,   58,   11,\n",
      "          25,  549,   10,  151,    7,  702,   24,    5,   11,   37,  576,    1]), tensor([410,  72,   5,   1]), tensor([   2,   13,    7, 1260,    4, 1342,    1]), tensor([ 23, 107,  19,   8,  29,   1]), tensor([30, 17, 12,  1]), tensor([   2,   13,  738,  233,    4,   99,    3,   27,    9,   66,    5, 1161,\n",
      "           1]), tensor([  28,  574,   11,   25, 1155,    5,   12]), tensor([17, 29,  1]), tensor([   3,  140,   48,    8, 1029,  259,  675,   13,    1]), tensor([  3,  35, 487,  10, 124, 296, 198,   6,  92,  10,  23,  43, 844,   4,\n",
      "        132,  29,   1]), tensor([   2,   31, 1182,  235,  339,    1]), tensor([  3,  60,  48,   8, 832, 266,   5, 208,  23,  76,   6, 804,   4, 173,\n",
      "          1]), tensor([   3,  188,  639,   70,   10,  434, 1124,   28,   58,  204,   21,    2,\n",
      "         246,   57,    9, 1304,  296,    9,    2,   13,    1,    2,   68,   15,\n",
      "           2,  198,   20,   52,    1]), tensor([356,  62,   3,  11,  85, 881, 113,   1]), tensor([17, 13, 12,  1]), tensor([ 76, 298,   4,  41,   1]), tensor([  52, 1248,  573,   32,    1]), tensor([1290, 1208,    4,   41,   29,    1]), tensor([ 103,   15,   71,    7,    2, 1179,  239,    6,   23,  443,    1]), tensor([  54,    2,  112,  571, 1200,  171,   10, 1014,  581,  863,    3,  268,\n",
      "           9,  478,  819,  538,    4,  873,   36,   53,    2,   32,    1]), tensor([  2,  36,  41,   7,  26,   4, 867,   7, 226,   1]), tensor([84,  8, 13,  1]), tensor([ 243, 1312,   19,  149,    1]), tensor([   2,   91,   78,   16,   79,   10,  274,    4,  163,  233, 1252,  288,\n",
      "          14,   13,    1]), tensor([ 71,  24,  71,   6,   3,  11, 137, 238,  10,  51, 400, 422,  53, 124,\n",
      "          1]), tensor([364, 139,   1]), tensor([   3, 1104,    5,   24,   67,    9,  204,   87,   18,   10,  745,  111,\n",
      "          78,    1]), tensor([1327,  294,    1,    1,   36,   41,    7,  147,    1]), tensor([  77, 1206,    6,   77,  652,    6,   77, 1089,    6,   18,  118,   10,\n",
      "        1188,    6,  330,   12,    3,  136,   11,   25,   65,   75,  312,   19,\n",
      "          14,   94,   33,   16,    2,  785,    1]), tensor([ 166,   41,    6,  219, 1255,    6,  236,  172,    1]), tensor([  28,   66,  131,   28, 1085,   16,    3,  890,    1]), tensor([   3,  398,   14,  223,   11,   37,   81,    6,   70,  124,  910,   57,\n",
      "           9,   14,   13,    4,    5,   78,   56,   10,  289, 1349,    2,   13,\n",
      "          20,   24,   14,  499,   86,    2,   49,    1]), tensor([ 3, 98, 18, 48, 95, 75,  1]), tensor([  5,   7,  43, 181,   6, 106,   9, 588,   2,  13,   4,  46,  10, 440,\n",
      "          6, 761, 230, 694,   1]), tensor([   5,  240, 1107,  716,    6,    2, 1013,   45,  201,    9, 1043,    6,\n",
      "           4,    2,  150,   45,   38, 1150,   22,    5,    7,  201,    9,  505,\n",
      "         148,    1]), tensor([  14, 1216,   46,   33,   64,    4,  265,  997,    5,    1]), tensor([ 52,  29,   6,   3,  80,  23, 345,  19,   2, 260,   1]), tensor([ 30,  56,  10, 289,   5,  44,   2, 162,  39,   2,  33,   3,  92,  19,\n",
      "          2,  13,   1]), tensor([246,   7, 822, 350,   1]), tensor([ 18,  67, 138,   2, 186,  83,  10,  17, 247,  15,   8,   6,   3,  98,\n",
      "         64, 123,  14, 489, 516, 433,  14, 824, 673, 192,  97, 246, 292,   1]), tensor([ 100,    5, 1058,    6,    2,   31,  292,    7,  387,    4,    2,  117,\n",
      "           7,  562,  161,    1]), tensor([ 526,  222,   23, 1231,    1]), tensor([  8,   7,  38, 805,   4,  64,  14, 203, 915,  54,   3, 272,   9, 505,\n",
      "          2,  40, 126, 206,  14,  40,   1]), tensor([320, 811,   7, 226,  69,  12]), tensor([  8,   7,  10, 632,  13,   1]), tensor([ 17,  29,   6, 308, 347,  12,   1]), tensor([ 8,  7, 10, 17, 13, 12,  1]), tensor([  3,  20,  23, 180,  19,   2,  68,  15,   2, 287,   1]), tensor([420, 131,   3, 358,   1]), tensor([   3,  343,   14, 1269,   94,  124,    4,  615,  110,   20,   18,   10,\n",
      "         326,  383,    2,   13,    4,    2,   91,    1]), tensor([586,  17,   1]), tensor([  8,   7,   2, 120,  13,   3,  11,  96,  35,  22,  46, 102,  38, 683,\n",
      "        181,   1]), tensor([  60, 1316,    1]), tensor([  76, 1233,  958,    1]), tensor([   3,  286,  184,   53,  165,   90,  688,   46, 1048,  135,  214,    1,\n",
      "           1,    1,    1,    2,  120,  111,    3,  479,   20,    2,   60,  134,\n",
      "         129,    1]), tensor([78, 17, 12,  1]), tensor([255,  92,   5,  12,  12,  12,  12,  12]), tensor([   2,   13,  985, 1266,   12]), tensor([481, 110,   7,  10, 145,  19,   2, 596,   1]), tensor([  5, 405,  11,  25,  44,   6, 125,  58,  18, 122,  50, 100,   3, 271,\n",
      "          1]), tensor([  5,  64,  35,  10,  94, 145,   1]), tensor([ 59,  18, 152,   2, 689,  70, 623,  57,  13,   1]), tensor([  8,   7, 937,   1]), tensor([106,   9,  42,   1]), tensor([ 695,    8,    9,   55,  230,  114,  754,  123,   28,  427,   56,  676,\n",
      "          11,   37, 1324,    1]), tensor([410, 130,  55,  93,   1,   1,   1]), tensor([441,  20,  23, 180, 100, 241,  53,   2, 331,  31,   9,   2, 828,  31,\n",
      "          1]), tensor([189,  71, 300,   6, 249,  86, 127,   6,   8, 111, 114,  18, 269,  21,\n",
      "         14,  40,   1]), tensor([  38,    3,  380, 1286,   14,   93,   87,    2, 1353,   16,  330,    1]), tensor([  3,  67, 122, 872,  16, 225,   1]), tensor([ 76,  32, 259, 127,   1]), tensor([1175, 1053,  184,  152,  347,    1]), tensor([   2,  251,   15,    5,    7,   23, 1204,    4,    2,  128,    7,   76,\n",
      "           4,  153,    6,   19,   17,  886,    1]), tensor([519,  55,  93,   1,   1,   1,   1,   3,  11,  96,  35,   8,  74,  16,\n",
      "        212,  99,   1]), tensor([ 52, 787,  32,   1]), tensor([ 10, 215,  26,  29,   1]), tensor([   5,   11,   37,   23,  722,    4,  185,    9,   42,   62,  178,  247,\n",
      "         409,  367,  182,    2,  116, 1171,   38,  143, 1230,    1]), tensor([ 941, 1189,   19,    2,  124,  342,   13,    1]), tensor([237, 111,  21,  13,  44, 257,   6, 265,  56,   5,   1]), tensor([   5,   67, 1152,    2,   13,   39,  142,  540,  117,    1]), tensor([   3,   61,   48,  504,    2,  158,  953, 1201,  101,   30,  429,   19,\n",
      "          14, 1035,  506,    6,  179,  840,    1]), tensor([ 97, 226,  94, 251,  16, 425,  12,   1]), tensor([ 30, 119,   1]), tensor([  2,  67,  26, 111,  20,  22,   5, 156, 290,  21, 267, 203,   1]), tensor([   4,  213,   15,    2, 1298,    7,  587,    1]), tensor([   3,  338,    2,  967, 1064,   21,    5,   70,  567,    1,  869,   57,\n",
      "           6,    4,    5,   99,  979,    9,  350,    9,   10,  739,    4,  465,\n",
      "         113,  237,  485,   24,   10,  191,    1]), tensor([ 51,  47,  28,  11, 137, 421,   1]), tensor([  38,    3,   82,   79,   15,   95,    4, 1184,  605,   15,   93,    1]), tensor([  3,  82,   8,  31,  19,  10, 731,  53, 167,   4,   3,  11,  85,  23,\n",
      "        107,  19,  14, 260,   1]), tensor([   3,  820,    2,  531,  813, 1369,   16,    8,    4,    3,   11,   85,\n",
      "         215,  107,   19,   22,  751,    1]), tensor([393, 853, 583,  12,   1]), tensor([ 63, 560, 253, 253, 439, 870,   8,   7,   2,  33,  22, 309,  30,  43,\n",
      "          1]), tensor([ 108, 1141,   21,    2, 1243,  603,  182,   28,  520, 1367,    1]), tensor([769,   1]), tensor([ 17, 414,   1]), tensor([   2,  413,  680,    2,   36,  767,  206,   55,   40,    4, 1194,    9,\n",
      "         930,    2,   36,  149,    4,  232,    1]), tensor([  1, 160, 457,  31,  70,  28,  73,  11,  25,  27,   9, 510,   5,  39,\n",
      "        860,  39, 220,  15,   2, 431, 214,  57,   1]), tensor([140,  48,  16, 115,  33, 572,  46,  10, 385, 558,  13,   1]), tensor([  3,  27,  62, 227,  21,  14, 135,  13, 100,   3,  80, 317,   6,  34,\n",
      "         28, 388,  18, 122, 279,   1]), tensor([ 2, 36, 41,  7, 52, 39, 43,  1]), tensor([185,   6, 462,   4,  17, 121,   1]), tensor([  54,   28,   56,   10,  210,  661,    9, 1073,   71,   55,  402,    6,\n",
      "         163,    8,   13,    7,   16,   28,   12]), tensor([   3,   80,  241,    9,   27,    9,   65,    2,  120,    9, 1046, 1170,\n",
      "           8,   29,    1]), tensor([   2,  704,   45,    2,  183, 1026,    1]), tensor([  2,  67, 111,  22,   3, 552,  98, 926,   7,   2,  36, 970,  87,  53,\n",
      "          2,  32,   1]), tensor([76, 36,  1]), tensor([  5,  20,  10, 130,  15,  14,  93,   1]), tensor([52, 36, 41,  1]), tensor([  5, 842,  14, 135,  13, 127, 377, 100,   3, 809,   2, 116,   1]), tensor([1279,   28,   16,  543,   17,  129,    1]), tensor([ 14, 837,  46,   2, 567,   6,   4,   2,  31,   7, 791,   1]), tensor([   2,  103,  803,   15,    2,  626,  851, 1307,    1]), tensor([ 73,  11,  25, 386,  62, 179,   9,   2, 351,   1]), tensor([   2,   40,  654,   67, 1097, 1040,   24,   33,   40,    1]), tensor([1332,  573,  517,    4,   38,   59,    8,  392,  325,   13,   12]), tensor([ 17,  16, 945,  69,   1]), tensor([  8,   7,  10,  17,  29,   1,   1,   1,   1,   1, 545, 631, 189,  55,\n",
      "        843,  12,   1]), tensor([  3,  92,   5,  90,   5,  20,  38, 267,   4, 597,   1]), tensor([  5,  11,  37,  18, 131,   5, 521,   5,   7,   1]), tensor([ 34,   5,  59,  66,  51, 109,   4, 232,  47, 115,  13,   3,  11,  96,\n",
      "         35, 228,   1]), tensor([   3,  322,   22,  533,  868,    6,   34,    9,   50,    5, 1195,   56,\n",
      "        1220, 1275,   22,  577,   11,   25,   44,   43,    1,   43,    6,    8,\n",
      "          33,   30,   17,    1]), tensor([84,  8, 29,  1]), tensor([   8,   29,    7,   17,    1,    1,    1,    5,  182,  132,   10,  209,\n",
      "         799,    3,   58,  179,    9,    2,  724,  191,  568,   21,  444,   16,\n",
      "        1235,    1]), tensor([ 59,  18, 121,   1]), tensor([130,  15, 653,   1]), tensor([ 78, 257,  12]), tensor([103,  81,  21,   2, 470,   1]), tensor([ 355,    2,  368,    9,  370,  322,   28,   45, 1147,   10,  151,    7,\n",
      "          10,  341,  245,  239,    4,    8,   13,    7, 1094,   24,   22, 1164,\n",
      "           1]), tensor([  5,   7,  23, 104,  21,   2,  40,   1]), tensor([  3,  11,  85, 514, 148,   1]), tensor([  2,  36,   7, 153,   4,   2, 125,   3, 271,   9,  21,   5,  45, 374,\n",
      "        133,   2,  41,  69,   1]), tensor([  2, 347,  89,  20,  64,  23, 507,  12]), tensor([980,   9,  14,  13, 192, 802,   1]), tensor([380,   2, 129,  20,  23, 134,   1]), tensor([  3,  92,   8,  13, 196,   2, 303,  15, 471,   4,   3,  11,  85, 233,\n",
      "        564,  19,   5,   1]), tensor([  17,  186,   16, 1037,    1]), tensor([   2,  637,  709,    3,   27,    7,    6,    2,   31,  782, 1268,    1]), tensor([   3,  118,  236,    8,   13,  206,   10, 1254,    4,    5,   20, 1261,\n",
      "          16,  522,    4,    5,  146,   30,   17,   12]), tensor([  3,  27,  67,  35,   5,  16,  10, 176, 570,   6,  34,  38, 155,   6,\n",
      "         38,  26,   1]), tensor([  5, 278, 743,  87,   4, 182,  10, 283, 283, 283,  36, 163, 521, 219,\n",
      "        306,   1]), tensor([  3,  20, 468,  16,   8,  32,  16,  10, 160,  89,   4,  99,  22,   3,\n",
      "         11,  96,  92,   5,   3, 136,  11,  25,  65, 312,   1]), tensor([ 73,  11,  25, 130,  55,  93,  12,   1]), tensor([ 572,   24,  221,  127, 1022,    7,  884,   72,    8,   31,  224,    1]), tensor([   3,   20, 1217,   24,   14, 1329,    6,   19,    2,  737,   21,   14,\n",
      "         230,    6,    4,    2,   32,  469,  219,    1]), tensor([ 461, 1344,    6,    3,  894,  478,    5,    7,  110,    1]), tensor([ 28,  58,  18, 611, 172,  19,   2, 164,   6, 255,  78, 485,  12]), tensor([ 157,    6,    2,   40, 1078,  396,  161,  301,    4,   63,   67,   33,\n",
      "         274,    3,  469,   33,    1]), tensor([962, 278,  26,   1]), tensor([585, 207,   1]), tensor([1166,   14,   13,    9,   56,   94,  492,    1]), tensor([  58,   11,   25, 1325,  516,   53,   10,  553, 1083,    1]), tensor([154,  78,  21,   2, 120, 272,   1,   2, 117,  20, 678, 807,  24,  10,\n",
      "        692, 190,   4,   2, 399, 240,  26,   1]), tensor([ 20,  18, 107,   1]), tensor([ 157,    6,   14,  879,   20,  707,   22,  220,   89,    2,   13,  105,\n",
      "          11,   25, 1337,  113,   56, 1049,   13,   59,    1]), tensor([  63,  212,    6,  128,  108,  222,  284,   71,   15,   10, 1263,    1]), tensor([178,   2, 247, 409,   1]), tensor([ 88,  44,  79, 570, 163, 285,   1]), tensor([ 18, 210, 175,   4, 105,  11,  25, 561,  21,  56,   5, 348,   1]), tensor([221, 328, 734,  24, 991, 609,   7, 205,   1]), tensor([34,  5,  7, 17,  6,  3, 61, 60, 48,  5]), tensor([  3,  20,  18, 107,  19,   8,  74,   1]), tensor([   3,   27,   35, 1294,  129,   16,   86,  193,   99,    6,    4,    3,\n",
      "          80,  215,  107,   19,    5,    1]), tensor([ 146,  568,    1,    1,    1,    1,    1,    1,    3,   11,   85,  545,\n",
      "           8,   74,   61,   44,   43,    1,    1,   54,    3,  139, 1151,    5,\n",
      "          12]), tensor([ 23, 776, 492,   1]), tensor([  3,  80,  64,  23, 107,  19,   2,  68,   1]), tensor([   5,  693,  206,  258,   24,   10,  190,   22,  182,   28, 1361,  141,\n",
      "         160,   22, 1009,   61,  248,    1]), tensor([   3,   27,   10,  165,  460,   13,    4,   88,   44,   43, 1296,    6,\n",
      "          26,  109,    4,  261,   22,  817,  428,  463,   15, 1213,    1]), tensor([ 18, 131,   3, 358,   1]), tensor([1288,  113,    9,    8,  525]), tensor([30, 17,  1]), tensor([ 267,    6, 1222,    6,  447,  468,    6, 1114,  526,   19,  608,  540,\n",
      "          24,  258,    1]), tensor([1250,  153,   15,    8,   29,    4,  179,   19,    2,  877,  333, 1162,\n",
      "        1091,    6,  101,  396,   24,   10, 1285,    1]), tensor([  2,  49,   7,  17,   4,  30, 119,  19,   2,   1]), tensor([1267,   13,   21,   10,   17,  328,    1]), tensor([ 575,  237, 1090,    1]), tensor([105,  11,  25,  44,   1]), tensor([208,  26,  24,   2, 334,   6,  34,   8,  49,  20,  10, 911, 406,  12,\n",
      "         12]), tensor([  2, 199,  21,   2,  13, 471,  65, 188,  39,  10, 790, 100, 932,   1,\n",
      "          1,   1,   3,  11, 137, 341,  65, 189,  10, 774, 163,   8,   1]), tensor([ 54,   3, 270,  10, 334,   6,   2,  31, 299,  10, 630,   6,   4, 536,\n",
      "        634,   6, 975,  50, 322, 142, 765,   1]), tensor([   3,   11,   96,   82,  366, 1356,  314,   22,   36,   51,   47,   95,\n",
      "           1]), tensor([144,  15, 207,   1]), tensor([1282,  214,   45,  132,  108,  119,   99,    1]), tensor([  5,  11,  37,  10, 144,  15, 294,  12]), tensor([  82, 1000,   16,    2,   91,    6,  101,  170, 1238,   63,  504,    1])]\n",
      "[tensor([ 38, 155,  38,  26,  12,   1]), tensor([   5,  138,   18,   44,   24,   14,  135,   13,  126,    3,   80,   23,\n",
      "         113, 1202,   19,    2,   91,   12,    1]), tensor([  2,   0,   7, 146, 456,  24,   0,   1]), tensor([336,  41,   4, 129,   1]), tensor([   2,  439, 1082,   30,  119,    6,   34,  163,    2,  116, 1311,    9,\n",
      "           0,    2,    0,    0,    6,    4,    2,    0,    0,    1, 1069,    6,\n",
      "           3,  146,    0,    8,  117,  244,    1]), tensor([  5,  11,  37, 215, 106,   1]), tensor([  2, 513, 764,  24,  10, 176, 570,   1]), tensor([  2, 958,   7,  10,  76,   0, 383,  10, 864,   0,   4,   2,   0, 135,\n",
      "         13,   0,   0,   1]), tensor([336,  29,   1]), tensor([1069,    6,    3,   61,   48,    8,   13,  184,    2,   94,    0,    1]), tensor([73, 11, 25, 72,  8, 29,  1]), tensor([ 99,   3, 322,  22,   3, 181,  10,   0, 751,   1]), tensor([142,   2, 103,  32,   3,  27, 188,   1]), tensor([336,  41,   1]), tensor([ 552,    5,  184,  100,   28,  495,    9,  489,    8,   33,   12,    8,\n",
      "         545,    7,    2,  248, 1035,   13,   16,   50,   12]), tensor([106,   9,   0,  19,  14, 344, 135,   1]), tensor([ 18, 175, 149,   1]), tensor([   3,  343,   14,    0,   43,  363,    2,  347,    0,    6,  154,   20,\n",
      "          24,   26,  132,    0,    4, 1069,    6,    3,   80,   23,  821,    9,\n",
      "          27,    8,    0,    1]), tensor([  0,  59,  18,   0, 175, 288,   0,   1]), tensor([  5,  46,  71,   2, 839,   3, 569]), tensor([525,   0, 339,   4, 143, 682,  47,   2,   0,   1]), tensor([  8,  74,   7, 426,   4,  30, 257,  12]), tensor([  8,   7,  97,  52,   0,   6, 304, 100,   0,  19,  55,  13,  11,  37,\n",
      "          0,   1]), tensor([1279,   28,   16,    0,   14,   93,    1]), tensor([  3, 598, 816,  83,  18,  65,   0,  12]), tensor([ 28, 255, 322,  54,  28,   0,   5, 242, 175,  86,   2, 127,   0,  15,\n",
      "        555,  16,   2,   0,  28, 569,  86,  18,   1]), tensor([  8,  49, 524,  43, 181,   1]), tensor([52, 29,  1]), tensor([ 10, 209,  15,   0,  27, 102,   0,   8,  10,  23,  26,  13,   4,  38,\n",
      "         83,   3,   1]), tensor([   0,   13,    0,   45,   10, 1041,   27,    1]), tensor([23,  0, 19, 22,  1]), tensor([   2,  958,    7,   60, 1366,   24,    0,    4,    7, 1260,  175,    3,\n",
      "          73,   11,   25,  421,  115,  216,    1]), tensor([  3,  80,  23, 180,  19,   8,  32,  53, 496,   1]), tensor([ 5, 20, 10, 17, 13,  1]), tensor([52, 12,  1]), tensor([ 95,  45, 678,  23, 104,   4, 867,   7, 403,   1]), tensor([   3,   11,   96, 1074,    8,   13,   16,  212,   99,    4,   58,  520,\n",
      "          22,    5,   11,   37,    2,  103,    0,   13,    3,   11,   96,   35,\n",
      "           1]), tensor([   8,    7,    2,   13,    9,   66,   16,    1,    1,    1,    1,    3,\n",
      "         108,   82,   14,    0,    4,   71,    3,   58,  520,    7, 1367,   12]), tensor([  0, 117,  30,  17,  12,  12,   1]), tensor([ 23, 143, 202,  19,   8, 291,   1]), tensor([ 5,  0, 20, 18, 39, 26, 39, 14, 37,  1]), tensor([  3, 487,   8,  16, 531,   0,   0,  34,   3, 552,   5,  67,  78, 485,\n",
      "         70,   0, 100,   3, 120, 188,   5,  57,   1]), tensor([336, 271,  89, 492,   1]), tensor([  3, 231, 123, 172, 133,   0,   0,   1]), tensor([  18,   10,   26,   74,    1,    1,    5,   78,   16,   10,  191,  163,\n",
      "         268,  243,  216,   24,   14,    0,    0,    0, 1098,    1]), tensor([208,  17,   4,   7, 541,   1]), tensor([ 632,    0, 1283,    1]), tensor([  5, 208,  23,  76,   1]), tensor([ 38, 108, 636,   1]), tensor([  2,  67,  23, 769, 111,  20, 110,  20,  77,   0,  12,  12,  12,  12]), tensor([ 18, 180,   1]), tensor([104,  24,  14, 891,   1]), tensor([  34,    6,   24,  115,   49,    6,    2,  103, 1082,    7,    6,   28,\n",
      "          58,    0,   95,  494,    9,   55,    0,  189,    0,    6,   86,  118,\n",
      "           0,  494,   53,   55,    0,    9,    2,   13,    1]), tensor([  3,  61,  27, 880,  77, 535,  54,   3,  20, 276,   1]), tensor([   3,   11,   85,  560,    9,  263,    5,   16,   10, 1153,    1]), tensor([355,   5, 138,  18,  44,   1]), tensor([  0,   9,   8,   0,  21, 237, 151,   3,  20,   0,   9,   0, 189,   8,\n",
      "         32,   1]), tensor([   8,  291,  152,   50,   10,    0,    0,    4,  146,   18,  880,   50,\n",
      "          14, 1153,  168,    1]), tensor([  8,   7, 814,  10,   0,   0,  22,  59,  18,   0,   1]), tensor([  3,  58, 122, 191,   3,  11,  85,   0,  24,   2, 116,   6,   4,   0,\n",
      "         73,  11,  25, 118,  27,   9, 338,   5,  21,   5,  11,  37,   0,   0,\n",
      "          1]), tensor([  3,  35, 586,  77, 145,  19,   8,  32,   0,   9,  14, 640,   0,  12]), tensor([ 16,   2,  68,   8,  20,  10,  17, 749,   1]), tensor([202,   1]), tensor([163,  10, 176, 174, 324,   2,  10,   0,  15,   0, 286,  87,  15,   2,\n",
      "         13, 191,  24,  42,   1]), tensor([   8,   13,    7, 1226,    4,  461,    4,    2,  407,    7,  632,    1]), tensor([   3,    0,   73,   11,   25,   56,    0,   34,    8,   33,    7,   23,\n",
      "         462,  367,  105,   11,   25, 1015,  113,   14,    0,    1]), tensor([  69,  134,   28,   27,    9, 1085,  113,    9,    0,   10, 1031,   16,\n",
      "           2,  129,   12]), tensor([  2,  68,  20,  23,  26,   4,  19,   2, 434, 347,   4,  71,   5,  20,\n",
      "         10,  26, 260,   1]), tensor([ 73,  11,  25, 123,   2, 162, 327,   3, 138,   1]), tensor([  2, 497,   0,  60, 106,  21,   8, 694,   1]), tensor([  5,   7, 461,   6,  46,   0,  15,  31,   0,   6,   4,   7,  23,   0,\n",
      "          9, 361,  16,   0, 828,   0,  15,  89,   1]), tensor([ 31,   7, 903, 113,  43,   1]), tensor([  3,  80,   0,   3, 181,   8, 260,   1]), tensor([  2, 120, 111,  22,   0,  20,  22,   2,   0,  20, 161,   1]), tensor([ 57, 526, 136,  11,  25,  27, 102,   0,   1]), tensor([   8,   13,    7,  215, 1260,    4,    3,   11,   96,  255,   35,  115,\n",
      "         964,  216,   19,    5,    1]), tensor([225,  41,   7, 336,   6,  23, 336,   1]), tensor([ 23, 335,  19,   8,  32,   1]), tensor([  5,   7, 106,   9, 561,  21,   4, 161, 100,  28,  45,  24,   2, 116,\n",
      "          4,   2, 149,   0,  45, 340,   0,   1]), tensor([  3,  11,  96,  35,   8,  16,   0, 193,   4,   5,  46,  78,  17,  16,\n",
      "         50,   1]), tensor([ 271,   79, 1326,  295,  129,    1]), tensor([  29,    7, 1326,    6,  266,    5,   59,   18,   27,  175,  288,    0,\n",
      "           9,  152,    2,    0,    3,   20,    0,    9,   42,    5,   19,    1]), tensor([108, 131,   3, 358,   1]), tensor([  0,   9, 520,   6,   3,   0,  14,  93,   1]), tensor([  2, 109, 554,   8,  32,   7,  52,   1]), tensor([  5,  46, 956, 113,  23,  43,   1]), tensor([ 14,   0, 367,   3,  99,  66,  17, 109,   1,  10, 325, 826,   6,  34,\n",
      "        492,   7,  17,   1]), tensor([   3,  177,    2,   29,    9,   65,  106,    9, 1202,  113,    4,   42,\n",
      "           1]), tensor([  3,  84,  71,   2, 839,   4, 433,   0,   1]), tensor([26, 68,  1]), tensor([269, 377,  53,   8, 351,   6,  65,   0,   1]), tensor([  8,   0,  71,   2, 216,   1]), tensor([498,   6,   3,   0,  83,  18,   0,   5,   7, 575, 142,   0,  68,   0,\n",
      "          1]), tensor([  0,  46,   2, 103, 295, 129,   0,   1]), tensor([ 156,  290,    6,  286,   19,  353, 1219,   15,  793, 1293,    1]), tensor([30, 17,  1]), tensor([  5, 105,  11,  25,  44,  24,   0,  86,   0,   1]), tensor([  8,  33,  30,   4,  20, 259, 127,   1]), tensor([ 63,  10, 275,   2,  31, 222, 233,   0,  21,  14,  32,   1]), tensor([  3,  20, 907,  16,  75,   1]), tensor([  5,  11,  37,  10,  17,   0,  16,   0,   6,   0,   6,   4, 296,   0,\n",
      "          1, 482,   6,  65, 545,   9,  42,   0,   1,  16,   0, 296,   0,   0,\n",
      "         12]), tensor([   0,  378,    0,    0,    6, 1291,    0,   36,    4,    0,    0,   16,\n",
      "        1282,   21,    2,  256,  303,   15,    2,  151,    1]), tensor([   5,   20,   22,  210,    1,  438,    9,  520,   22,    2,  496,    0,\n",
      "          10,    0,  292,    9,   14,  135,    4,   19,   77,  538,  789, 1049,\n",
      "          42,    1]), tensor([  3,  84,   2, 199,   6,   5,  11,  37,  60, 215,  26,  41,   1]), tensor([  3, 405,  11,  25, 569,   2, 694, 241, 184,   2,   0,  15,  14,  40,\n",
      "          6,   0,   0,   1]), tensor([163,   3, 820,  16,   2, 162,  13,   6, 118,  22,  35,   2, 162, 145,\n",
      "          1]), tensor([142, 109,   7,  23,  23, 336,   1]), tensor([131,  10, 130,  15,  89,  12]), tensor([  26,  357,    6,   30,  119,   62, 1113,    0,    0,    6,  116,    6,\n",
      "          86, 1339, 1067,    1]), tensor([  8,   0,  15, 314,   7,   2, 364,  22,   3,  27, 139,  35,   0,   1]), tensor([  3,  17, 109,  71,   2,  89,   1]), tensor([   5,    0,   69,  461,    4, 1291,    1,    1]), tensor([  8, 325, 117,  46,   0,  14,   0,   0,   4, 181,  14, 159,  10,   0,\n",
      "        209, 799,   1]), tensor([202,  12,   1]), tensor([ 26,  74,   6, 252,  68,   1]), tensor([298,   0, 224,   1]), tensor([134,  41,   1]), tensor([  95,    0,  733,  113,    2,  245,  461,    0,  584,    2,   40, 1067,\n",
      "           1]), tensor([  5,   7, 392,   6,   4,   5, 427,   4, 251, 108,  39, 392,   1]), tensor([ 23, 202,   1]), tensor([  3,  60,  56,   8,  29, 184,   2, 124,  90,   5,   7,   0,   0,  21,\n",
      "          2,  40, 144,   4,   2, 472,   1]), tensor([134, 394,   1]), tensor([  5, 156,  14,  40,  43,   4,   7, 104,  21,   1]), tensor([701,  16,   0,   0, 307,   1]), tensor([ 83,  18, 260,   8,  13,   1]), tensor([131,  10, 406]), tensor([  0,  23, 335,  19,  14, 260,   1]), tensor([  3,  27,   8,  13,   4,   5,   7,  10,   0,  24,  14, 527,   6,   3,\n",
      "         60,   0,   5,   1]), tensor([  5, 182,  23,   0,   0,   0, 228,   5,   0,   2, 151,   1]), tensor([  3, 188,   9, 271,  21,   5,  16,  62, 582, 318,   4,   2,  31,  61,\n",
      "         65, 983,   0,   4, 791,   1]), tensor([52,  0,  0, 13, 49,  1]), tensor([  3,  27, 188, 218,  13,  24, 112, 193,   6,  34,   8,  33,   7,   2,\n",
      "        103,   1]), tensor([  0,   1,   1,   1,   0, 235,   2, 781,  90,  15,  10, 360,   0,  12]), tensor([ 24,   0,   5, 240,   0, 208,  39,  54,   2,  13,   7,  71, 462, 392,\n",
      "        497,   1]), tensor([ 34, 758,  95, 176,   0,   6,   8,  49,   7,  15,   0,  41,   4,  43,\n",
      "        575,   2,   0, 727,  15,   0,  97,   0,   0,  29,   1]), tensor([   2,  172,  411,    6,    2,   13,  700,   21,    4,  161,  133,  114,\n",
      "           6,    2,  128,    0,  284,    4,    2,  364,   15,   71,    5,    0,\n",
      "        1174,    0,    1]), tensor([103,  32, 139,  12,  12,  12,   1]), tensor([  2,   0,  32,  20, 543,  10,   0,   1]), tensor([157,   6,   0, 442,  45,   0,  18,  26,  16, 217,  89, 871,  56,   0,\n",
      "          0, 266,   2, 225,   0,   0,  50, 113,   1]), tensor([52, 29, 16,  2, 68,  1]), tensor([  3,  73,  11,  25, 552,   5,  61, 444,   5,  69,   0,  21,  55, 230,\n",
      "          1]), tensor([ 63,   2, 120, 152, 956, 241,   0,  63, 211,   1]), tensor([  0,  26, 501,   4, 208,   0,   6,  69,   1]), tensor([136,  11,  25, 840,   5,  87]), tensor([  3,  11,  85, 146,   0,  19,   8,  13,   1]), tensor([23,  0,  1]), tensor([  38,    3,   35,    9,  270,    2,   31,   87,   15,    2,   13,  338,\n",
      "           5,   71,  168, 1296,    4,  163,    0,    5,    1]), tensor([385,   0,   7, 106,   9,  42,   1]), tensor([ 133,  120,    3, 1284,    3,   20,    0,   10,   26,  749,  133,  366,\n",
      "           1,    6,    0,    3, 1104,    5,  206,   14,   13,   70,    0,  342,\n",
      "          57,    1]), tensor([  0,   6,   0, 144,  15, 207,   1]), tensor([ 71,   2, 256, 287,   3,  11,  96, 187,   0, 425,   0,  63,  10, 176,\n",
      "        212,  34,   8,  33, 524,   9,  65,  24,  16,   2, 160,   0,   1]), tensor([26, 41,  1]), tensor([61, 18,  0,  1]), tensor([336,  36,  41,   1]), tensor([793, 144,   0, 301,   1]), tensor([  39,   97,    0, 1170,    0,    6,  126,   24,    8,   91,    4,  330,\n",
      "           0,    1]), tensor([ 5, 11, 37, 10, 17, 74,  1]), tensor([ 3, 61, 48,  5,  1]), tensor([   3, 1021,   27,  885,   10,    0,    6,   34,    3,   61,   18,    0,\n",
      "         659,    5,  194,   90,   15,    2,    0,   41,  603,    1]), tensor([ 30,  39, 756,   1]), tensor([ 142,   10,  325,    0,   34,    3,  552,    0,  142,    0,   21,    0,\n",
      "           4,    5,  517,    4,    0,    0,  127,  235,    9,  142,   31,    0,\n",
      "        1222,    0,  458,   49,    1]), tensor([153,   0,  36,   1]), tensor([ 73,  11,  25, 130,  55,   0,  21,   8,  33,   1]), tensor([108,  60,  26,   1,   1,  38, 155,   6, 500,   2, 103,   0,  32,   3,\n",
      "         27, 139,  35,   1]), tensor([1245,    0,   16,    8,  129,    1]), tensor([  10, 1041,    0,   16,  195,    0,   24,    2,  364,    0,   15,    0,\n",
      "         298,    1]), tensor([  2, 364,  13,   3,  11,  96, 139,  35,   1,   1,   1,   1,  67,  35,\n",
      "          5,  16,  10, 176, 212,   1]), tensor([ 28,  98,  67, 270,   0, 133,  10,  89,   4,   2,  41,  20,  23, 336,\n",
      "          1]), tensor([147,   1,   1,  14, 116, 114,  18,   0,   8,   0,   1]), tensor([ 17,  36,   4, 129,   1]), tensor([  8, 602,   2,   0,  15,   0,   0,  16,   2, 162, 746,   4,  89,  63,\n",
      "          2, 120,   1]), tensor([ 81,  59,  18,  44,   6,  13,   0, 113,   6,   0, 108,   0, 113,   4,\n",
      "         99,   5, 108, 182, 172,   0, 191,  24,  14, 499,   0,   1]), tensor([   8,    7,    0,   10, 1041,   27,   54,   55,    0,   59,   18,    0,\n",
      "         135,   13,    0,  191,    0,    1]), tensor([ 100,    3, 1095,   14,    0,  206,    2,   49,    6,   18,   67,   20,\n",
      "           5,   18,    0,    6,   34,  110,   20,   10,  209,   15,  423, 1178,\n",
      "          21,    2,    0,    1]), tensor([  3,  11, 464,  65, 468,  16,  10,  94, 414,   1]), tensor([154,  79,   8,  29,   7, 578,   1, 120]), tensor([   0,    9,  370,    0,    2,  331,   31,   53,    0,   22,  286,   19,\n",
      "          14, 1330,    1]), tensor([  2,  13, 178, 424,   0,  12]), tensor([ 52, 311, 434,   0,   1]), tensor([  54,   28,   56,   10,  210,  661,    9, 1073,   71,   55,  402,    6,\n",
      "         163,    8,   13,    7,   16,   28,   12]), tensor([   8,   13, 1021,   43,   65,    2,  364,    3,   11,   96,  139,   35,\n",
      "          24,  115,  646,    1]), tensor([  3, 138,  18,  27, 115, 145,  19,   8,  74,   4,  61,   0,   5, 194,\n",
      "         54, 474,   1]), tensor([120,  15,  71,   6,   5, 105,  11,  25, 361,  43,   1]), tensor([ 0,  2, 13, 21, 71,  0,  1]), tensor([0, 0, 1]), tensor([110,  11,  37,  10, 205,   0,  36,  24,   2,   0,  21,  71,  14, 172,\n",
      "         22,   3,  27, 255, 827, 228,   1]), tensor([  5, 405,  11,  25, 152,  16,  50,   1]), tensor([ 31,  46,  77, 159,   1]), tensor([406,   1,   1,   3, 313, 279,  22,   0,  24,  14,  40,   1]), tensor([ 26, 225,  41,   1]), tensor([   2,   36,   41,   16,    2,  117,    7, 1317,    1, 1323,   28,   45,\n",
      "          24,   10,   60,    0,  375,    6,   28,  372,  122,  330,    1]), tensor([769,   0,  53,  10,  26,   0,   1]), tensor([ 13,   0,  87, 301,   1]), tensor([182,   5, 799,   9, 455, 113,  19,  14,  81, 100,   3,  11,  85,  18,\n",
      "          0,   5,   1]), tensor([ 18,   0,  39,  26, 468,  39,   2, 167, 334, 182,   5, 251,   1]), tensor([ 78,  23,  43,   0,  19,  10,   0,   0,   4,  39,  10,   0,  32,  21,\n",
      "         14, 491,  70, 189,  10,   0,  81,   0,  57,   1]), tensor([108,  59,  18,  44,   1]), tensor([  54,  110,    7,   10,    0,    6,    5,    7,  233, 1326,    1]), tensor([   0,  348,   27,  220,  190,    9,    0,   10,  134,   13,   16,   10,\n",
      "           0,  164,   86, 1235,   12]), tensor([188,   4,   0,   1])]\n",
      "[tensor([17, 29, 16,  2, 68, 12,  1]), tensor([84,  5,  1,  1, 17,  0,  1]), tensor([ 22, 291,   7,  10, 951,   1]), tensor([ 0, 16,  2,  0,  1]), tensor([  2, 442,  45, 106,   9,  42,   4, 816, 997, 148,   1]), tensor([364, 295, 129, 139,   1]), tensor([ 90, 197, 203,  45,   0,   6,   0,   7,  18,   0, 133,  71,   1]), tensor([   0,    7,  108, 1267,    1]), tensor([30, 17, 12,  1]), tensor([ 28, 254, 133,   0,   0,   9,  66,   9,  55,  13,   0,  53,   2,  89,\n",
      "         28, 120, 561,  21,   2,  13,   1,  31, 159,   7,   0,   1]), tensor([  3,  80, 438,   3, 337,   5,   1]), tensor([ 52, 129,  12,  12,  12,  12,  12,  12,  12,  12,   1]), tensor([  2,  91, 280, 363,   2,   0,   0,   6,  34,   5, 138,  18,  44,   1]), tensor([   2, 1241,    7,   15,  252,   41,   38,   39, 1004,    2,    0, 1298,\n",
      "          36,   23,  392,    1]), tensor([70,  5, 30, 12, 57]), tensor([ 23, 356, 129,  53,   0,  12]), tensor([   3,   59,   18, 1001,   10,  292,   19,    2,  397,  191,    5,    7,\n",
      "          21,   14,    0,    1]), tensor([280, 339,   4, 143, 250, 826,  47, 488, 229,   0,   1]), tensor([114,   0,  53, 148, 194,  12]), tensor([ 54,  28,  45, 468,  16,  10,  26,  41, 124,  32, 455, 468,   6,   8,\n",
      "          0,  11,  25,   5,   1]), tensor([169, 406,  19,   0,   0,   1]), tensor([  5,   0,  18,  44, 183,  15,   2,  89,  19,  14, 477,   1]), tensor([   3,   11,   96,   35,   77, 1312,    0,    2,  246,    6,    0,  516,\n",
      "          86,    0,  115,   15,    2,    0,    1]), tensor([  2, 477,   0,   0, 198, 138,  18,  44,  19,  14,  13,   1]), tensor([  0,  31, 207,  12,   1]), tensor([  2, 525,   7,  23,   0,   4,   0,   6,   3,  61,   0,  72,  53, 148,\n",
      "        194,   1]), tensor([ 73,  11,  25,   0, 221,   0,   4,  73,  11,  25, 421, 115, 443, 544,\n",
      "          1]), tensor([   2,    1,    0,    0,  199,    6,  229,   10, 1082,   15,   10,   13,\n",
      "           6,    7,  509,   26,    1]), tensor([  8,   7,  10,  17, 749,   1]), tensor([  2, 246, 588,  20, 119,   6,   5,   2,   0,   0,  22,   5,  78,   1]), tensor([17, 29,  4, 68,  1]), tensor([  34,   99,   22,    5,    7,   87,   15, 1341,    2,  162,  216,    0,\n",
      "           1,    0,  463,    1,    1,    1,  338,   55,   93,    0,    0,    1,\n",
      "           1,    1,  689,  114,   18,  544,    5,    1]), tensor([ 18,  26, 175,  16,   2,  68,   1]), tensor([  3,  64, 405,  11,  25,  56,   2,  21, 657,   6,   5,   0,  56,   5,\n",
      "         61,   0,  19,  42,   1]), tensor([  2, 472,   7,  17,   1]), tensor([17, 29,  1]), tensor([ 481,   88,   27,   10,  147,  295,  129,    6,   38,   28,   66,  131,\n",
      "          28, 1085,   16,    1]), tensor([  39,    3,  518,  584,    1,    1,    1,    1,  215, 1326,   12]), tensor([  3,  56, 298,   4, 251,  15, 158,   0,   2,  40, 442,   4,   7, 215,\n",
      "          0,   9, 361, 318,  10, 747, 192,   0,  24,   2,  40,   1]), tensor([  2, 298,   7,  23,  26,   1,   1]), tensor([364, 186, 139, 188,   1,   1,   1,   1,  54,   3,  98, 437,   8,   0,\n",
      "          0,   3,  61,   1]), tensor([  3,  27, 278, 188,   0, 442,   4,   2,   0,  53,   2, 573,   7,  23,\n",
      "        443,   1]), tensor([  3,  84,   2, 516,  90,  88,  45,  38,   0,  12]), tensor([103,   3,  11,  96, 177,  38, 155,   1,   1,   1,   1,   3,  11,  96,\n",
      "        187, 256,   0,   4,   8,  33,  46,   2, 103,  41,  70,  16, 197,  50,\n",
      "          4,   2,   0,  57,  39,  43,  39, 415,  15, 189,   1]), tensor([  0,   5,  11,  37,   0,  21, 436, 527,  62,   4,   3,  84,   5,  12]), tensor([256,  47,  22,   6,   2, 458,   7,  76,   4,   0,   2, 121,   7,  23,\n",
      "          0,   2, 740,  87,  16,   2, 305,   7,  10,  26,   0,   1]), tensor([  3,  80,  23, 107]), tensor([   5,   11,   37,   38, 1258,    9,   27,    9,  455,  659,   94,  681,\n",
      "           6,  116,  681,    6,    0,    6,  314,    4,  116,    0,  237,   89,\n",
      "          10,   94,   13,  700,   87,    1]), tensor([753, 294,   1]), tensor([  63,    0,   19,  165,    0,    2,  236,  172,  273, 1168,    2,  214,\n",
      "          63,  112,  174,    1]), tensor([  5,   7,   0,  24,  10,   0, 116, 133,   0,   0,   1]), tensor([  63,   14,   13,   92,    9,   65,   79,   10,  275,  484,    6,    5,\n",
      "          11,   37,  102,    0,  648,  758,  143,  389,   21,   14, 1082,    1]), tensor([  3,  27,  82,   8, 477, 135,  13,  10, 176, 570, 277,   4,   5,  20,\n",
      "         10,   0,   1]), tensor([   2,  395,    7,  118,    0,   47,    3, 1284,    5,   61,   65,    6,\n",
      "           4,    2,  886,   45,    0, 1204,    1]), tensor([ 0, 23,  0, 12, 12, 12, 12, 12,  1]), tensor([   3,  405,   11,   25,  552,   22,    2,  940, 1129,  571,  443,    9,\n",
      "          50,    1]), tensor([   2,    0,  463,    1,    1,    1,  610,  576,    6,  392,    0,   53,\n",
      "        1245,    1]), tensor([   2,   31,    7,  233, 1326,    9,   50,    1]), tensor([  8,   0,  24,   2,  13, 229, 302,   0, 133,   0, 149,  86,   0,   1]), tensor([  3,  66, 586, 205, 109,  24,  14,   0,   6, 362,  19, 256, 214,   3,\n",
      "         27,  18,  35,   8, 145,   1]), tensor([153,   0, 172,   6, 160,  31, 159,   6, 160, 261,   1]), tensor([1107,    0,    0,    1]), tensor([623,   7,  18, 153,   6,  36,   7,  23,   0,   4,  28,  27,   9,   0,\n",
      "        100,  28, 271,   1]), tensor([163,   3,  35,   9,   0, 490,   5,   0, 266,   5,   0, 956, 786,   1]), tensor([   2,  334, 1163,    7,  155,  382,  131,  256,    0,  214,   45,    0,\n",
      "           0,    1]), tensor([ 88,  45,  38, 173,  12]), tensor([134, 260,   1]), tensor([ 24,  14, 908,   3,  20, 878, 236, 734,   0,   4,  77, 734,  24,  14,\n",
      "          0,   1]), tensor([ 95, 314, 571,  10,  17, 204,  62,   4,   3, 552,  88,  45, 493,   2,\n",
      "        103, 260,   3,  11,  96, 181,  24,   2, 248, 218, 193,  62,   0,   1]), tensor([  2,   0,   0,   7, 201,   4,   0,   1]), tensor([17, 13,  1]), tensor([  3,  11,  96,  67,  35,  14,  81,  16,  10, 176, 570,   6,  34,   3,\n",
      "         60,  56,   5,   1]), tensor([ 0, 13, 12, 12, 12]), tensor([ 14,  67, 709,   7,   2,   0,  36, 149,   7,  10, 325, 252, 118, 100,\n",
      "        562, 113,   9,  70,  15,  57]), tensor([  0,  16,   2,   0,   0,   7,   0,   6,   4,   2,   0,   4, 303,   0,\n",
      "         45,  18, 362,   3, 421, 148,   9,  65,   1,   1]), tensor([ 64, 182,   5, 799,   9, 444,  21,   9,   1]), tensor([  28,   27,    9,  444,    2,   13,  133,   10,    0,    0,   16,    2,\n",
      "         256, 1083,    9,  122,   28,  691,    1]), tensor([ 31,   0,   7, 340, 160,   1]), tensor([  2, 298,   7,  23,   0,   6,  39,   2,  40, 694,   7,  18,  23, 104,\n",
      "        133,  71,   1]), tensor([ 76,  41,   0,   6,   0, 220, 392,   0,  87, 110,   1]), tensor([   5,   46,   10,   17,  199,    0, 1038,    6,    4,    2,    0,   45,\n",
      "          76,    4,  153,   19,   17,  334,   41,    1]), tensor([   3,   61,    0,   48,    2,  158,    0,   16, 1282,  572,   45,  468,\n",
      "          16,  701,    6,  232,    4,   10,   17,   68,   12]), tensor([   0, 1275,  352,    1]), tensor([17, 13, 12,  1]), tensor([  8,   0,   0,  61,  18,  44,  19,  14, 124, 506,   0,   1]), tensor([ 3, 61, 48,  8,  1]), tensor([  3, 489,  15,  95, 287,   4,  61,   0, 610,   1]), tensor([  10,    0,  958,  370,    0,   10, 1087,  206,   10,    0,    0,    0,\n",
      "         452,   15,  108,   10,    0,    0,    1]), tensor([  59,   18,   44,   16,  981,    9, 1040,   19,    2,  689,    1]), tensor([ 23, 769,   1]), tensor([ 59,  18, 121,   1]), tensor([ 54,  28, 528, 569,  10, 267, 431,  13,   0, 251,   0,  39,   2, 423,\n",
      "          0, 367,   0,  45,   0,   1]), tensor([ 24,   2,   0,  15,  97,   0,   6,   3,  35, 112, 125,   0,   0,  62,\n",
      "          7,  22,   2,  94,  13,  21,   0, 224,  12, 224]), tensor([  8,   7, 311, 235,   2, 103,  13,   3,  11,  96, 139,  35,   1]), tensor([28, 58, 11, 25,  0,  2, 68, 21, 95,  1]), tensor([1052,   14,    0,    2,  103,   13,    3,   27,  139,   35,    1]), tensor([  8,  49,  46,   0,   2,  33, 275,   0,   4, 191,   5,   0,   0,  15,\n",
      "        361,   6,   5,   7, 580,   0,   1]), tensor([273,  61,  48,  95,   9, 488,   1]), tensor([  18,   26,  100,    0,   10,    0,   86, 1265,    1]), tensor([131,  10, 144,  15, 207,   1,   1,   3, 992,  75, 172,  21,   8,  13,\n",
      "          1]), tensor([  3,   0,   0,   4, 110,   7,  77, 239,  16,   5, 101,   7,  60, 769,\n",
      "          1]), tensor([ 38, 155,   5,  46,  78,  56,  10, 289,   1]), tensor([84,  8, 32, 12]), tensor([ 71,  24,  71,   3, 552,   5,  20,  10,  26,   0,   1]), tensor([17, 91,  1]), tensor([ 5, 11, 37,  0, 12]), tensor([  63, 1147,    4,  189,    2,   29,   16,  108,  174,    5,  170,    1]), tensor([ 23,   0, 129,   1]), tensor([  0, 700,  19,  10, 541, 461,  22,  28,  58,  42,   9, 461, 113,  55,\n",
      "        199,   0,   6,   4, 118,   0,   0,   0,  70,   0,  12]), tensor([   0,   24,    2,   40,    6,   73,   11,   25,   42,   19,  460, 1335,\n",
      "          70,    0,   57,    1]), tensor([  22,  229,  518,    6,   16,   10,   13,    6,    2,  199,    7,   23,\n",
      "          76,  253,  173,    0,    9, 1097,   19,    6,    4, 1331,    7,  403,\n",
      "          39,   43,    1]), tensor([ 23, 392, 497,   6,   0,  56,  97, 484,   0,   0,   1]), tensor([1126,    0,  199,   62,   23,   76,  494,    6,   64,   46,  173,    0,\n",
      "          56,  284,    4,    0,    6,    4,   75,    1]), tensor([0, 0, 1]), tensor([  2, 291,   0,  14,  29,  23,   0,   4,   5,  30,  43,   1]), tensor([ 26,  49,   6,  52, 357,   1]), tensor([  2,  32,   0,  14,   0,  38,   3,  80, 107,  19,  14, 260,   1]), tensor([ 73,  11,  25, 130,  55,  93,   1]), tensor([1102,  410,  130,   55,   93,    1]), tensor([  3, 361,   5,   0,   4,   5, 904, 113,  23,  43,   1]), tensor([ 17,  32,   6,  23, 180,  62,   0,   1]), tensor([ 116,   91,   39,   43,   39,    0,   91,   45,  448,    9,  123,  545,\n",
      "          28,  255, 1181,   87,   15,    0,    1,    0,    0]), tensor([17, 81, 12,  1]), tensor([ 17,   1,   1,   1,  77, 216, 133,  71,  12,   1]), tensor([610,   0,  79,   8,  13,  11,  37,   0,   7,  22,   5,  60, 208, 341,\n",
      "          0,   6, 304,  24,   2,  71, 284,   0,   1]), tensor([   3,   11,   96,   64,   35,  216,   19,    2,   13, 1141,    2,    0,\n",
      "           0,   24,  101,    3,  278,  561,    5,   21,    4,  163,  161,  194,\n",
      "           1]), tensor([130,  15,  93,   1]), tensor([ 181,   23, 1260,    1]), tensor([   5,    7,    0, 1326,    4,  138,   18,    0,  115,  321,   15,    0,\n",
      "           9,   14,  109,   63,    3,   82,    5,    1]), tensor([ 73,  11,  25,  72,   8,  29,  62,   5, 834,  12,   1]), tensor([579,   0,   0,   9,  42,   2, 162,   0,  91, 298,   1]), tensor([  3,  11,  96, 236,  14,  13,  75, 555,  47,   3,  58, 520,   6, 118,\n",
      "         21,   0,   4,  14,  13,   7, 146,  17,  70,   0,  21,   0,  12,  57,\n",
      "          1]), tensor([  3, 140,  48,   8,  49,   1]), tensor([ 2, 13, 20,  0,  4, 20, 18, 94,  1]), tensor([  23,   26,   41, 1283]), tensor([   5,   11,   37,    0,    4,    2,   36,   41,    7,  340,  336,    0,\n",
      "          19,    2,   13,   70,  342,   57,   86,   19,   14, 1118, 1356,   32,\n",
      "          70,   22, 1104,  206,   97,  460,   57,    1]), tensor([221,   0,   4,   0,   0, 481,   0, 131,  88,  11,   0,   0,   1]), tensor([  2, 103,  13,  24, 470,  57,   1]), tensor([ 3,  0,  5,  0, 90, 15,  2, 36, 41,  1]), tensor([105,  11,  25,  44, 133,  71,   1,   1,   3,   0,   5,  16,  14,   0,\n",
      "          4, 142,  18, 132,   1]), tensor([  5,  61, 270,  69, 160,   9,   0, 141, 134,  14, 295, 129, 422,  46,\n",
      "        102,  19, 167,   1]), tensor([  0,  33, 747,   4, 163,   0, 113,   1]), tensor([  13,    7, 1260,   39,   71,  477,  630,  214,   45,    1]), tensor([105,  11,  25,  83,   2, 247,   1]), tensor([ 3, 84,  8, 13, 12,  1]), tensor([  0, 145,   1,   1,   1,   1,  23,   0]), tensor([  3, 361, 882,   4,   5, 156, 119,  19, 148,   1]), tensor([ 71, 353, 170, 363, 112, 212,  15,  42,   1]), tensor([  77,    0,   40,  874, 1129,    6,    4,   77,  940, 1348,    1]), tensor([  0, 298,   1]), tensor([  8,   7, 171, 155,   2, 364, 260,   3,  11,  96, 181,  21, 167,   1]), tensor([ 71,  24,  71,   6,   3,  11,  85, 340, 345,  19,   8, 260,   1]), tensor([  2, 940, 405,  11,  25,   0,  22,  10, 473,   0,  98,  65, 188,   1]), tensor([ 34, 100,   3,   0, 166,   0, 133,   0,   6,   2,   0,   0,   0, 161,\n",
      "         10, 176, 522, 206,   2, 120,   0,   6,   4, 163,   3,  11,  85, 469,\n",
      "          1]), tensor([  0,   4,   0,   0,   9, 262,   5, 194,   4,   0,  50,   9,  72, 610,\n",
      "         13, 192, 115, 321,  15,   0,   0,   1]), tensor([104, 121,  62,  28, 254,  55,  32,   9,  65, 104,  16, 133,   0,  97,\n",
      "          0, 133,  10,  89,   6,  54,  18,  16,  97,   0, 747,   1]), tensor([ 23, 106,   9,  42,   1]), tensor([199, 395,   0,   7, 378,   1]), tensor([  8,   0,  13, 289,   7, 597,   4,  23,   0, 672,   1]), tensor([  3,  27, 102,  23, 107,  19,   2,   4,  27,  35,  77,   0,  53, 115,\n",
      "         33,   0,  14,  36,  41,  21, 221, 303,   1]), tensor([ 2,  0, 20, 21, 89,  1]), tensor([   0,    6,  252,   41,   36,    6,    4,    2,    0,  196,    2,    0,\n",
      "          11,   37,    0,   20, 1107, 1266,    0,    4,    0,  161,    1]), tensor([336,   0,   1]), tensor([   8,    0,    0,    0,   13, 1241,    7,  226,    1]), tensor([ 5, 11, 37, 23,  0,  4,  0,  9, 65, 15, 26, 41,  1]), tensor([  3,  82,   8,   9,  42,  19,  14,   0, 845,   4, 586,   0,   5,  12]), tensor([ 13,  99, 904, 152,  56,   5, 138, 100,   5,  20,  94,   1]), tensor([205,  13,   1]), tensor([ 71,   3,  58,  83,   7,   0,  21,   2, 246,   6,  38, 315,   5,   0,\n",
      "          1,   2,  75,   3,  42,   2, 111,   2, 250,   3,  56,   5,   1]), tensor([  3,   0,  14, 325,  93,  19,   8, 414,   1]), tensor([150,  45,  69, 267,   1]), tensor([ 115,    0,    0,    1,    1, 1069,   26,   13,    9,   72,    1,    1]), tensor([  2, 364, 144,  15, 294, 139, 604,  19,   2, 165, 295, 129,   1]), tensor([59, 18, 44,  1]), tensor([273, 343,  10,   0,   0,  31,  22,   0,   0,  63,  10, 234,   0, 331,\n",
      "        188,   9, 248,  10, 274,  62,  34, 146, 323, 467,  47,   0]), tensor([   5,    7, 1266,  391,  113,   16,   42,   39,   10,  267,    0,    0,\n",
      "           6,    4,    0,   24, 1282,    0,    1]), tensor([94, 31, 30, 17, 24, 13,  1]), tensor([  0,   4, 153,   1]), tensor([ 14,   0, 484, 477,  53,   0, 904,   2, 152,  10, 209,  51,  47,   8,\n",
      "          1]), tensor([  3,  11,  85, 107,  79,   8,   0,  26,  41,   4, 252,  68,   1]), tensor([  5,   7,  38, 267,   4,  28,  73,  11,  25, 118,   0,  22,   5,   7,\n",
      "        110,  63,  10, 191,  15, 878, 188,   9,   5,   1]), tensor([ 142,    0,    4,   23, 1204,    0,    2,  385,    7,   10,   23,   76,\n",
      "         395,    1]), tensor([  3, 343,  14,  32,  24,  26,  89,   4,  20, 107,  19,   5,   1]), tensor([  3,  11,  85,  23, 202,  19,  14, 751,   1]), tensor([ 31, 159,   7,  64,  17,  12]), tensor([  2, 399,  15,   2, 442,   7, 336,   1]), tensor([ 23,  43, 181,   4, 156,  14,   0,   0, 257,   1]), tensor([ 23, 104,   1]), tensor([308, 129,   1]), tensor([77,  0,  0, 21,  8, 33, 12,  1]), tensor([ 98,  18,  66, 541, 175, 219,   1]), tensor([  2, 364,  13,  53, 344,   1,   1,   1, 294,   1,   1,   1,   1,   1,\n",
      "          8,   0, 344, 412,   1,   1,   7,   2, 364,   3,  27, 139,   0,   1]), tensor([  3, 222,  21, 124,  11,  37,   0,   4,   0,  71,   0,   6,  34,  98,\n",
      "         18,  66,   5,   9,   0, 194,   1]), tensor([  64,    6,    2,   13,  105,   11,   25, 1194,    9,    0,  279,    0,\n",
      "           0,    0,    6,    0,    0,  171,    0,    0, 1098,    1]), tensor([ 118,   24,   14,    0, 1201,  101,    7,    0,    0,    6,    3,   27,\n",
      "        1312,    0,  131,    2,  256,    0,    7, 1185,    1]), tensor([  22,   11,   37,   10,  911,  298,    0,   70, 1323,    3,   11,   85,\n",
      "          18,  189,    5,    0,    6,  101,    3,   73,   11,   25,  552,    7,\n",
      "           2,   49,   57,    1]), tensor([  18,   67,  114,    5,  781,   55, 1098,    6,   34,  471,   64,    0,\n",
      "           0,    5,    1]), tensor([248,  89, 659,  53,  28,   1]), tensor([1340,   62,  269,  377,    1]), tensor([  0,  10, 282,   1])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anton\\AppData\\Roaming\\Python\\Python312\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator       \n",
    "import copy     # pre-process\n",
    "\n",
    "data = pd.read_csv(\"amazon_cells_labelled.txt\", delimiter='\\t', header=None)\n",
    "data.columns = ['Sentence', 'Class']\n",
    "data['index'] = data.index                                          # add new column index\n",
    "columns = ['index', 'Class', 'Sentence']\n",
    "data = preprocess_pandas(data, columns)   \n",
    "\n",
    "train_iter, val_iter, test_iter = \\\n",
    "              np.split(data.sample(frac=1, random_state=42), \n",
    "                       [int(.6*len(data)), int(.8*len(data))])\n",
    "\n",
    "train_iter2 = copy.deepcopy(train_iter)\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter2[\"Sentence\"]), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "# get data, pre-process and split              \n",
    "\n",
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data2 = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    print(data2)\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data2)))\n",
    "\n",
    "\n",
    "train_data = data_process(train_iter[\"Sentence\"])\n",
    "val_data = data_process(val_iter[\"Sentence\"])\n",
    "test_data = data_process(test_iter[\"Sentence\"])\n",
    "\n",
    "\n",
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"Divides the data into ``bsz`` separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Arguments:\n",
    "        data: Tensor, shape ``[N]``\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape ``[N // bsz, bsz]``\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size)  # shape ``[seq_len, batch_size]``\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)\n",
    "\n",
    "#training_data, validation_data, training_labels, validation_labels = train_test_split( # split the data into training, validation, and test splits\n",
    "#    data['Sentence'].values.astype('U'),\n",
    "#    data['Class'].values.astype('int32'),\n",
    "#    test_size=0.10,\n",
    "#    random_state=0,\n",
    "#    shuffle=True\n",
    "#)\n",
    "#\n",
    "## vectorize data using TFIDF and transform for PyTorch for scalability\n",
    "#word_vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,2), max_features=50000, max_df=0.5, use_idf=True, norm='l2')\n",
    "#training_data = word_vectorizer.fit_transform(training_data)        # transform texts to sparse matrix\n",
    "#training_data = training_data.todense()                             # convert to dense matrix for Pytorch\n",
    "#vocab_size = len(word_vectorizer.vocabulary_)\n",
    "#validation_data = word_vectorizer.transform(validation_data)\n",
    "#validation_data = validation_data.todense()\n",
    "#\n",
    "#train_x_tensor = torch.from_numpy(np.array(training_data)).type(torch.FloatTensor)\n",
    "#train_y_tensor = torch.from_numpy(np.array(training_labels)).long()\n",
    "#validation_x_tensor = torch.from_numpy(np.array(validation_data)).type(torch.FloatTensor)\n",
    "#validation_y_tensor = torch.from_numpy(np.array(validation_labels)).long()\n",
    "#\n",
    "#class TransfromerDataset(torch.utils.data.Dataset):\n",
    "#    def __init__(self, datasetA, bptt):\n",
    "#        self.source = datasetA\n",
    "#        self.bptt = bptt\n",
    "#\n",
    "#    def __getitem__(self, i):\n",
    "#        seq_len = min(self.bptt, len(self.source) - 1 - i)\n",
    "#        data = self.source[i:i+seq_len]\n",
    "#        target = self.source[i+1:i+1+seq_len].reshape(-1)\n",
    "#        return data, target\n",
    "#\n",
    "#    def __len__(self):\n",
    "#        return min(len(self.datasetA))\n",
    "#    \n",
    "#train_ds = ConcatDataset(train_x_tensor,train_y_tensor)\n",
    "#val_ds = ConcatDataset(validation_x_tensor,validation_y_tensor)\n",
    "#train_loader = DataLoader(train_ds,batch_size=5)\n",
    "#val_loader = DataLoader(val_ds,batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape ``[full_seq_len, batch_size]``\n",
    "        i: int\n",
    "\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape ``[seq_len, batch_size]`` and\n",
    "        target has shape ``[seq_len * batch_size]``\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab)  # size of vocabulary\n",
    "emsize = 200  # embedding dimension\n",
    "d_hid = 200  # dimension of the feedforward network model in ``nn.TransformerEncoder``\n",
    "nlayers = 2  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``\n",
    "nhead = 2  # number of heads in ``nn.MultiheadAttention``\n",
    "dropout = 0.45  # dropout probability\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 30.0  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 3.0, gamma=0.95)\n",
    "\n",
    "def train(model: nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 2\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(train_data) // bptt\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        output = model(data)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        loss = criterion(output_flat, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            seq_len = data.size(0)\n",
    "            output = model(data)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += seq_len * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |     2/   10 batches | lr 30.00 | ms/batch 152.95 | loss 18.82 | ppl 148837131.62\n",
      "| epoch   1 |     4/   10 batches | lr 30.00 | ms/batch 58.23 | loss 13.66 | ppl 857966.33\n",
      "| epoch   1 |     6/   10 batches | lr 30.00 | ms/batch 54.35 | loss 33.18 | ppl 257650664703349.38\n",
      "| epoch   1 |     8/   10 batches | lr 30.00 | ms/batch 78.05 | loss 39.33 | ppl 120995284583087152.00\n",
      "| epoch   1 |    10/   10 batches | lr 30.00 | ms/batch 50.38 | loss 75.33 | ppl 518211706981793670724880531718144.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time:  0.88s | valid loss 59.21 | valid ppl 51744103059810939637334016.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |     2/   10 batches | lr 30.00 | ms/batch 220.30 | loss 109.15 | ppl 253467208510786375748155564915608304138623909888.00\n",
      "| epoch   2 |     4/   10 batches | lr 30.00 | ms/batch 54.89 | loss 29.64 | ppl 7461208297805.35\n",
      "| epoch   2 |     6/   10 batches | lr 30.00 | ms/batch 70.79 | loss 19.62 | ppl 330391678.60\n",
      "| epoch   2 |     8/   10 batches | lr 30.00 | ms/batch 84.63 | loss 14.97 | ppl 3159930.65\n",
      "| epoch   2 |    10/   10 batches | lr 30.00 | ms/batch 65.95 | loss 15.99 | ppl 8830144.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time:  1.10s | valid loss 121.49 | valid ppl 57877570713687615011129132896053009064995656982069248.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |     2/   10 batches | lr 30.00 | ms/batch 131.99 | loss 68.44 | ppl 527992195434144695198387535872.00\n",
      "| epoch   3 |     4/   10 batches | lr 30.00 | ms/batch 81.48 | loss 20.70 | ppl 974428736.64\n",
      "| epoch   3 |     6/   10 batches | lr 30.00 | ms/batch 75.02 | loss 24.98 | ppl 70860448906.65\n",
      "| epoch   3 |     8/   10 batches | lr 30.00 | ms/batch 71.13 | loss 19.16 | ppl 209880586.98\n",
      "| epoch   3 |    10/   10 batches | lr 30.00 | ms/batch 51.50 | loss 13.16 | ppl 517016.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time:  0.89s | valid loss 40.97 | valid ppl 620746982263940480.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |     2/   10 batches | lr 28.50 | ms/batch 102.96 | loss 48.87 | ppl 1677656828984168546304.00\n",
      "| epoch   4 |     4/   10 batches | lr 28.50 | ms/batch 71.19 | loss 37.93 | ppl 29764325623940812.00\n",
      "| epoch   4 |     6/   10 batches | lr 28.50 | ms/batch 70.82 | loss 15.50 | ppl 5407824.32\n",
      "| epoch   4 |     8/   10 batches | lr 28.50 | ms/batch 72.67 | loss 18.36 | ppl 94053204.49\n",
      "| epoch   4 |    10/   10 batches | lr 28.50 | ms/batch 68.20 | loss 57.01 | ppl 5717319327098279684997120.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time:  0.85s | valid loss 27.01 | valid ppl 538097936627.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |     2/   10 batches | lr 28.50 | ms/batch 107.72 | loss 45.33 | ppl 48381273089440292864.00\n",
      "| epoch   5 |     4/   10 batches | lr 28.50 | ms/batch 76.22 | loss 14.36 | ppl 1723905.87\n",
      "| epoch   5 |     6/   10 batches | lr 28.50 | ms/batch 79.04 | loss 22.70 | ppl 7237559742.49\n",
      "| epoch   5 |     8/   10 batches | lr 28.50 | ms/batch 71.59 | loss 27.28 | ppl 703786115375.77\n",
      "| epoch   5 |    10/   10 batches | lr 28.50 | ms/batch 50.52 | loss 26.32 | ppl 270734049589.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time:  0.84s | valid loss 17.50 | valid ppl 39890936.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |     2/   10 batches | lr 28.50 | ms/batch 119.10 | loss 33.64 | ppl 406656871840205.38\n",
      "| epoch   6 |     4/   10 batches | lr 28.50 | ms/batch 75.97 | loss 21.61 | ppl 2431156298.06\n",
      "| epoch   6 |     6/   10 batches | lr 28.50 | ms/batch 113.38 | loss 37.44 | ppl 18243338751382084.00\n",
      "| epoch   6 |     8/   10 batches | lr 28.50 | ms/batch 70.37 | loss 19.65 | ppl 340837827.57\n",
      "| epoch   6 |    10/   10 batches | lr 28.50 | ms/batch 47.38 | loss 24.14 | ppl 30526324292.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time:  0.92s | valid loss 34.15 | valid ppl 679888820502271.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |     2/   10 batches | lr 27.07 | ms/batch 97.31 | loss 43.77 | ppl 10172504251972702208.00\n",
      "| epoch   7 |     4/   10 batches | lr 27.07 | ms/batch 63.12 | loss 42.09 | ppl 1903907060766061056.00\n",
      "| epoch   7 |     6/   10 batches | lr 27.07 | ms/batch 66.16 | loss 40.09 | ppl 257568265275120320.00\n",
      "| epoch   7 |     8/   10 batches | lr 27.07 | ms/batch 62.00 | loss 30.66 | ppl 20655372728083.81\n",
      "| epoch   7 |    10/   10 batches | lr 27.07 | ms/batch 45.22 | loss 20.67 | ppl 946210317.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time:  0.74s | valid loss 31.51 | valid ppl 48590798417660.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |     2/   10 batches | lr 27.07 | ms/batch 97.91 | loss 54.75 | ppl 601259193351293605249024.00\n",
      "| epoch   8 |     4/   10 batches | lr 27.07 | ms/batch 65.10 | loss 40.58 | ppl 419943678908439168.00\n",
      "| epoch   8 |     6/   10 batches | lr 27.07 | ms/batch 64.13 | loss 25.82 | ppl 162951754430.19\n",
      "| epoch   8 |     8/   10 batches | lr 27.07 | ms/batch 74.06 | loss 46.07 | ppl 101397912975178334208.00\n",
      "| epoch   8 |    10/   10 batches | lr 27.07 | ms/batch 45.66 | loss 27.38 | ppl 779208200253.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time:  0.76s | valid loss 26.74 | valid ppl 410793029822.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |     2/   10 batches | lr 27.07 | ms/batch 98.60 | loss 41.38 | ppl 932987926719884160.00\n",
      "| epoch   9 |     4/   10 batches | lr 27.07 | ms/batch 63.50 | loss 36.46 | ppl 6803859388314781.00\n",
      "| epoch   9 |     6/   10 batches | lr 27.07 | ms/batch 62.61 | loss 19.79 | ppl 391399268.04\n",
      "| epoch   9 |     8/   10 batches | lr 27.07 | ms/batch 60.30 | loss 23.61 | ppl 17972543903.12\n",
      "| epoch   9 |    10/   10 batches | lr 27.07 | ms/batch 48.78 | loss 26.59 | ppl 352937790582.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time:  0.74s | valid loss 20.13 | valid ppl 553748528.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |     2/   10 batches | lr 25.72 | ms/batch 97.36 | loss 25.71 | ppl 146666860377.77\n",
      "| epoch  10 |     4/   10 batches | lr 25.72 | ms/batch 64.77 | loss 14.21 | ppl 1481185.77\n",
      "| epoch  10 |     6/   10 batches | lr 25.72 | ms/batch 63.98 | loss 37.78 | ppl 25532376712920152.00\n",
      "| epoch  10 |     8/   10 batches | lr 25.72 | ms/batch 58.63 | loss 16.42 | ppl 13511157.38\n",
      "| epoch  10 |    10/   10 batches | lr 25.72 | ms/batch 43.01 | loss 20.15 | ppl 561908527.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time:  0.73s | valid loss 25.81 | valid ppl 161563330362.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |     2/   10 batches | lr 25.72 | ms/batch 88.35 | loss 28.12 | ppl 1631361717602.31\n",
      "| epoch  11 |     4/   10 batches | lr 25.72 | ms/batch 60.12 | loss 18.30 | ppl 88431857.68\n",
      "| epoch  11 |     6/   10 batches | lr 25.72 | ms/batch 57.25 | loss 19.39 | ppl 264004480.13\n",
      "| epoch  11 |     8/   10 batches | lr 25.72 | ms/batch 61.18 | loss 22.48 | ppl 5813213721.83\n",
      "| epoch  11 |    10/   10 batches | lr 25.72 | ms/batch 42.50 | loss 17.39 | ppl 35663418.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time:  0.69s | valid loss 37.17 | valid ppl 13848382578821726.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |     2/   10 batches | lr 25.72 | ms/batch 113.20 | loss 41.29 | ppl 856502478099764736.00\n",
      "| epoch  12 |     4/   10 batches | lr 25.72 | ms/batch 56.63 | loss 15.01 | ppl 3316880.95\n",
      "| epoch  12 |     6/   10 batches | lr 25.72 | ms/batch 61.64 | loss 27.13 | ppl 607072547383.35\n",
      "| epoch  12 |     8/   10 batches | lr 25.72 | ms/batch 58.64 | loss 13.30 | ppl 599645.16\n",
      "| epoch  12 |    10/   10 batches | lr 25.72 | ms/batch 44.60 | loss 16.60 | ppl 16238720.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time:  0.74s | valid loss 15.53 | valid ppl 5536313.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |     2/   10 batches | lr 24.44 | ms/batch 94.75 | loss 22.93 | ppl 9122737660.37\n",
      "| epoch  13 |     4/   10 batches | lr 24.44 | ms/batch 58.11 | loss 19.34 | ppl 249776891.61\n",
      "| epoch  13 |     6/   10 batches | lr 24.44 | ms/batch 57.74 | loss 21.78 | ppl 2878677237.62\n",
      "| epoch  13 |     8/   10 batches | lr 24.44 | ms/batch 60.02 | loss 17.50 | ppl 39649136.17\n",
      "| epoch  13 |    10/   10 batches | lr 24.44 | ms/batch 40.77 | loss 15.37 | ppl 4718563.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time:  0.69s | valid loss 19.88 | valid ppl 429067536.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |     2/   10 batches | lr 24.44 | ms/batch 85.19 | loss 23.90 | ppl 24021142620.56\n",
      "| epoch  14 |     4/   10 batches | lr 24.44 | ms/batch 55.16 | loss 40.00 | ppl 235039371707859008.00\n",
      "| epoch  14 |     6/   10 batches | lr 24.44 | ms/batch 55.45 | loss 19.67 | ppl 348718638.93\n",
      "| epoch  14 |     8/   10 batches | lr 24.44 | ms/batch 54.51 | loss 14.32 | ppl 1650544.15\n",
      "| epoch  14 |    10/   10 batches | lr 24.44 | ms/batch 41.64 | loss 15.77 | ppl 7065961.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time:  0.65s | valid loss 24.40 | valid ppl 39600367785.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |     2/   10 batches | lr 24.44 | ms/batch 78.51 | loss 42.56 | ppl 3053899852621423104.00\n",
      "| epoch  15 |     4/   10 batches | lr 24.44 | ms/batch 56.12 | loss 13.80 | ppl 986033.38\n",
      "| epoch  15 |     6/   10 batches | lr 24.44 | ms/batch 53.24 | loss 20.80 | ppl 1079119016.79\n",
      "| epoch  15 |     8/   10 batches | lr 24.44 | ms/batch 50.00 | loss 18.48 | ppl 105931472.34\n",
      "| epoch  15 |    10/   10 batches | lr 24.44 | ms/batch 42.64 | loss 21.49 | ppl 2158273028.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time:  0.64s | valid loss 14.44 | valid ppl 1869926.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |     2/   10 batches | lr 23.21 | ms/batch 94.19 | loss 22.08 | ppl 3893654232.48\n",
      "| epoch  16 |     4/   10 batches | lr 23.21 | ms/batch 49.97 | loss 21.01 | ppl 1329383715.99\n",
      "| epoch  16 |     6/   10 batches | lr 23.21 | ms/batch 49.62 | loss 17.97 | ppl 63425942.90\n",
      "| epoch  16 |     8/   10 batches | lr 23.21 | ms/batch 49.91 | loss 12.33 | ppl 226820.11\n",
      "| epoch  16 |    10/   10 batches | lr 23.21 | ms/batch 38.64 | loss 23.54 | ppl 16752908002.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time:  0.64s | valid loss 13.33 | valid ppl 615815.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |     2/   10 batches | lr 23.21 | ms/batch 83.31 | loss 22.68 | ppl 7082801672.58\n",
      "| epoch  17 |     4/   10 batches | lr 23.21 | ms/batch 56.70 | loss 21.39 | ppl 1939373088.63\n",
      "| epoch  17 |     6/   10 batches | lr 23.21 | ms/batch 53.15 | loss 17.65 | ppl 46194782.58\n",
      "| epoch  17 |     8/   10 batches | lr 23.21 | ms/batch 56.50 | loss 15.93 | ppl 8292989.50\n",
      "| epoch  17 |    10/   10 batches | lr 23.21 | ms/batch 41.12 | loss 18.55 | ppl 114174211.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time:  0.66s | valid loss 16.58 | valid ppl 15919677.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |     2/   10 batches | lr 23.21 | ms/batch 84.98 | loss 34.15 | ppl 677030816772470.25\n",
      "| epoch  18 |     4/   10 batches | lr 23.21 | ms/batch 57.15 | loss 15.78 | ppl 7136847.75\n",
      "| epoch  18 |     6/   10 batches | lr 23.21 | ms/batch 53.28 | loss 24.09 | ppl 29004541825.66\n",
      "| epoch  18 |     8/   10 batches | lr 23.21 | ms/batch 49.58 | loss 17.82 | ppl 54849187.05\n",
      "| epoch  18 |    10/   10 batches | lr 23.21 | ms/batch 45.80 | loss 19.99 | ppl 481265671.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time:  0.68s | valid loss 21.40 | valid ppl 1968652951.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |     2/   10 batches | lr 22.05 | ms/batch 105.16 | loss 20.58 | ppl 862979923.51\n",
      "| epoch  19 |     4/   10 batches | lr 22.05 | ms/batch 50.00 | loss 16.36 | ppl 12769673.21\n",
      "| epoch  19 |     6/   10 batches | lr 22.05 | ms/batch 64.30 | loss 24.99 | ppl 71558087380.61\n",
      "| epoch  19 |     8/   10 batches | lr 22.05 | ms/batch 66.03 | loss 13.26 | ppl 575655.77\n",
      "| epoch  19 |    10/   10 batches | lr 22.05 | ms/batch 39.68 | loss 16.98 | ppl 23766238.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time:  0.72s | valid loss 15.04 | valid ppl 3401798.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |     2/   10 batches | lr 22.05 | ms/batch 80.95 | loss 24.72 | ppl 54510084277.16\n",
      "| epoch  20 |     4/   10 batches | lr 22.05 | ms/batch 57.73 | loss 12.91 | ppl 404759.91\n",
      "| epoch  20 |     6/   10 batches | lr 22.05 | ms/batch 56.50 | loss 17.06 | ppl 25608734.00\n",
      "| epoch  20 |     8/   10 batches | lr 22.05 | ms/batch 55.50 | loss 18.01 | ppl 66119591.37\n",
      "| epoch  20 |    10/   10 batches | lr 22.05 | ms/batch 39.65 | loss 16.92 | ppl 22252871.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time:  0.66s | valid loss 10.57 | valid ppl 38799.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  21 |     2/   10 batches | lr 22.05 | ms/batch 79.00 | loss 32.61 | ppl 144691935232041.16\n",
      "| epoch  21 |     4/   10 batches | lr 22.05 | ms/batch 54.67 | loss 21.59 | ppl 2389210334.95\n",
      "| epoch  21 |     6/   10 batches | lr 22.05 | ms/batch 56.50 | loss 21.41 | ppl 1978785918.25\n",
      "| epoch  21 |     8/   10 batches | lr 22.05 | ms/batch 54.73 | loss 17.58 | ppl 43030911.81\n",
      "| epoch  21 |    10/   10 batches | lr 22.05 | ms/batch 42.44 | loss 16.42 | ppl 13588593.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time:  0.65s | valid loss 20.17 | valid ppl 573557309.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  22 |     2/   10 batches | lr 20.95 | ms/batch 81.13 | loss 25.12 | ppl 80833500731.54\n",
      "| epoch  22 |     4/   10 batches | lr 20.95 | ms/batch 49.67 | loss 19.24 | ppl 227268774.95\n",
      "| epoch  22 |     6/   10 batches | lr 20.95 | ms/batch 54.86 | loss 25.37 | ppl 104756019992.92\n",
      "| epoch  22 |     8/   10 batches | lr 20.95 | ms/batch 52.00 | loss 12.67 | ppl 317213.93\n",
      "| epoch  22 |    10/   10 batches | lr 20.95 | ms/batch 35.00 | loss 21.80 | ppl 2931196360.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time:  0.62s | valid loss 16.32 | valid ppl 12196113.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  23 |     2/   10 batches | lr 20.95 | ms/batch 86.85 | loss 21.59 | ppl 2373228880.02\n",
      "| epoch  23 |     4/   10 batches | lr 20.95 | ms/batch 54.00 | loss 14.15 | ppl 1399061.03\n",
      "| epoch  23 |     6/   10 batches | lr 20.95 | ms/batch 56.02 | loss 13.09 | ppl 483422.56\n",
      "| epoch  23 |     8/   10 batches | lr 20.95 | ms/batch 53.48 | loss 12.97 | ppl 427222.91\n",
      "| epoch  23 |    10/   10 batches | lr 20.95 | ms/batch 43.51 | loss 14.86 | ppl 2848175.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time:  0.66s | valid loss 18.38 | valid ppl 96298663.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  24 |     2/   10 batches | lr 20.95 | ms/batch 81.75 | loss 24.81 | ppl 59709474010.11\n",
      "| epoch  24 |     4/   10 batches | lr 20.95 | ms/batch 50.50 | loss 15.41 | ppl 4913998.35\n",
      "| epoch  24 |     6/   10 batches | lr 20.95 | ms/batch 53.72 | loss 10.55 | ppl 38239.59\n",
      "| epoch  24 |     8/   10 batches | lr 20.95 | ms/batch 51.66 | loss 20.48 | ppl 781755577.26\n",
      "| epoch  24 |    10/   10 batches | lr 20.95 | ms/batch 42.74 | loss 19.10 | ppl 196632405.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time:  0.64s | valid loss 17.95 | valid ppl 62764347.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  25 |     2/   10 batches | lr 19.90 | ms/batch 79.82 | loss 19.35 | ppl 252490156.09\n",
      "| epoch  25 |     4/   10 batches | lr 19.90 | ms/batch 57.51 | loss 12.85 | ppl 381510.78\n",
      "| epoch  25 |     6/   10 batches | lr 19.90 | ms/batch 54.99 | loss 13.81 | ppl 990394.48\n",
      "| epoch  25 |     8/   10 batches | lr 19.90 | ms/batch 54.74 | loss 14.44 | ppl 1865588.13\n",
      "| epoch  25 |    10/   10 batches | lr 19.90 | ms/batch 43.00 | loss 11.43 | ppl 92083.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time:  0.63s | valid loss 12.65 | valid ppl 311321.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  26 |     2/   10 batches | lr 19.90 | ms/batch 82.66 | loss 16.51 | ppl 14779545.62\n",
      "| epoch  26 |     4/   10 batches | lr 19.90 | ms/batch 57.09 | loss 13.76 | ppl 946727.43\n",
      "| epoch  26 |     6/   10 batches | lr 19.90 | ms/batch 55.00 | loss 15.86 | ppl 7761384.03\n",
      "| epoch  26 |     8/   10 batches | lr 19.90 | ms/batch 57.34 | loss 16.92 | ppl 22256978.56\n",
      "| epoch  26 |    10/   10 batches | lr 19.90 | ms/batch 41.14 | loss 13.96 | ppl 1149733.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time:  0.65s | valid loss 12.87 | valid ppl 387986.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  27 |     2/   10 batches | lr 19.90 | ms/batch 81.63 | loss 16.89 | ppl 21633226.27\n",
      "| epoch  27 |     4/   10 batches | lr 19.90 | ms/batch 53.85 | loss 13.76 | ppl 946415.09\n",
      "| epoch  27 |     6/   10 batches | lr 19.90 | ms/batch 54.10 | loss 15.73 | ppl 6771759.34\n",
      "| epoch  27 |     8/   10 batches | lr 19.90 | ms/batch 55.51 | loss 16.46 | ppl 14108989.59\n",
      "| epoch  27 |    10/   10 batches | lr 19.90 | ms/batch 43.68 | loss 13.19 | ppl 536313.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time:  0.64s | valid loss 13.20 | valid ppl 542063.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  28 |     2/   10 batches | lr 18.91 | ms/batch 84.78 | loss 22.88 | ppl 8624229586.98\n",
      "| epoch  28 |     4/   10 batches | lr 18.91 | ms/batch 56.10 | loss 13.37 | ppl 639431.19\n",
      "| epoch  28 |     6/   10 batches | lr 18.91 | ms/batch 55.37 | loss 13.38 | ppl 646584.63\n",
      "| epoch  28 |     8/   10 batches | lr 18.91 | ms/batch 58.67 | loss 10.34 | ppl 30939.34\n",
      "| epoch  28 |    10/   10 batches | lr 18.91 | ms/batch 43.50 | loss 10.13 | ppl 24998.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time:  0.67s | valid loss 18.11 | valid ppl 73005489.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  29 |     2/   10 batches | lr 18.91 | ms/batch 80.87 | loss 19.77 | ppl 383688384.30\n",
      "| epoch  29 |     4/   10 batches | lr 18.91 | ms/batch 56.50 | loss 11.89 | ppl 145374.61\n",
      "| epoch  29 |     6/   10 batches | lr 18.91 | ms/batch 54.20 | loss 10.32 | ppl 30292.69\n",
      "| epoch  29 |     8/   10 batches | lr 18.91 | ms/batch 52.12 | loss 12.56 | ppl 286157.92\n",
      "| epoch  29 |    10/   10 batches | lr 18.91 | ms/batch 41.13 | loss 14.25 | ppl 1551161.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time:  0.64s | valid loss 11.57 | valid ppl 105883.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  30 |     2/   10 batches | lr 18.91 | ms/batch 87.52 | loss 29.53 | ppl 6690978193163.32\n",
      "| epoch  30 |     4/   10 batches | lr 18.91 | ms/batch 55.80 | loss 22.74 | ppl 7528889976.69\n",
      "| epoch  30 |     6/   10 batches | lr 18.91 | ms/batch 53.13 | loss  9.37 | ppl 11768.85\n",
      "| epoch  30 |     8/   10 batches | lr 18.91 | ms/batch 57.62 | loss 11.01 | ppl 60756.70\n",
      "| epoch  30 |    10/   10 batches | lr 18.91 | ms/batch 39.10 | loss 11.28 | ppl 79353.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time:  0.66s | valid loss 19.38 | valid ppl 260056172.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  31 |     2/   10 batches | lr 17.96 | ms/batch 84.00 | loss 20.11 | ppl 541365813.86\n",
      "| epoch  31 |     4/   10 batches | lr 17.96 | ms/batch 55.61 | loss 11.89 | ppl 146512.01\n",
      "| epoch  31 |     6/   10 batches | lr 17.96 | ms/batch 57.11 | loss 10.32 | ppl 30328.24\n",
      "| epoch  31 |     8/   10 batches | lr 17.96 | ms/batch 52.92 | loss  8.99 | ppl  8001.76\n",
      "| epoch  31 |    10/   10 batches | lr 17.96 | ms/batch 42.59 | loss 17.79 | ppl 53365372.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time:  0.65s | valid loss 15.89 | valid ppl 7933290.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  32 |     2/   10 batches | lr 17.96 | ms/batch 87.82 | loss 17.67 | ppl 47088263.78\n",
      "| epoch  32 |     4/   10 batches | lr 17.96 | ms/batch 54.18 | loss 15.15 | ppl 3803076.62\n",
      "| epoch  32 |     6/   10 batches | lr 17.96 | ms/batch 53.20 | loss  9.97 | ppl 21451.89\n",
      "| epoch  32 |     8/   10 batches | lr 17.96 | ms/batch 55.74 | loss 15.01 | ppl 3298059.60\n",
      "| epoch  32 |    10/   10 batches | lr 17.96 | ms/batch 43.64 | loss 14.20 | ppl 1464298.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time:  0.67s | valid loss  9.70 | valid ppl 16381.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  33 |     2/   10 batches | lr 17.96 | ms/batch 81.30 | loss 14.18 | ppl 1440941.95\n",
      "| epoch  33 |     4/   10 batches | lr 17.96 | ms/batch 56.69 | loss 16.27 | ppl 11651450.74\n",
      "| epoch  33 |     6/   10 batches | lr 17.96 | ms/batch 50.98 | loss 10.67 | ppl 43182.25\n",
      "| epoch  33 |     8/   10 batches | lr 17.96 | ms/batch 52.01 | loss  9.34 | ppl 11362.10\n",
      "| epoch  33 |    10/   10 batches | lr 17.96 | ms/batch 39.99 | loss 15.40 | ppl 4865171.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time:  0.63s | valid loss 21.09 | valid ppl 1437033796.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  34 |     2/   10 batches | lr 17.06 | ms/batch 80.71 | loss 20.30 | ppl 655964757.65\n",
      "| epoch  34 |     4/   10 batches | lr 17.06 | ms/batch 53.02 | loss 17.00 | ppl 24106821.07\n",
      "| epoch  34 |     6/   10 batches | lr 17.06 | ms/batch 57.99 | loss  9.59 | ppl 14575.39\n",
      "| epoch  34 |     8/   10 batches | lr 17.06 | ms/batch 63.67 | loss 10.91 | ppl 54841.23\n",
      "| epoch  34 |    10/   10 batches | lr 17.06 | ms/batch 50.49 | loss  9.45 | ppl 12684.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time:  0.69s | valid loss  9.90 | valid ppl 19863.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  35 |     2/   10 batches | lr 17.06 | ms/batch 105.18 | loss 15.74 | ppl 6882362.27\n",
      "| epoch  35 |     4/   10 batches | lr 17.06 | ms/batch 66.51 | loss 11.02 | ppl 61330.36\n",
      "| epoch  35 |     6/   10 batches | lr 17.06 | ms/batch 70.12 | loss 11.68 | ppl 118245.88\n",
      "| epoch  35 |     8/   10 batches | lr 17.06 | ms/batch 72.15 | loss  9.83 | ppl 18675.25\n",
      "| epoch  35 |    10/   10 batches | lr 17.06 | ms/batch 54.49 | loss  9.17 | ppl  9624.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time:  0.83s | valid loss  8.81 | valid ppl  6694.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  36 |     2/   10 batches | lr 17.06 | ms/batch 106.00 | loss 21.80 | ppl 2928833796.73\n",
      "| epoch  36 |     4/   10 batches | lr 17.06 | ms/batch 70.65 | loss  8.55 | ppl  5191.48\n",
      "| epoch  36 |     6/   10 batches | lr 17.06 | ms/batch 70.62 | loss 10.50 | ppl 36274.35\n",
      "| epoch  36 |     8/   10 batches | lr 17.06 | ms/batch 70.36 | loss 10.14 | ppl 25409.31\n",
      "| epoch  36 |    10/   10 batches | lr 17.06 | ms/batch 56.46 | loss 11.46 | ppl 94760.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time:  0.84s | valid loss 12.53 | valid ppl 277211.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  37 |     2/   10 batches | lr 16.21 | ms/batch 108.00 | loss 16.98 | ppl 23684162.07\n",
      "| epoch  37 |     4/   10 batches | lr 16.21 | ms/batch 71.16 | loss 10.87 | ppl 52350.36\n",
      "| epoch  37 |     6/   10 batches | lr 16.21 | ms/batch 66.64 | loss 11.41 | ppl 90048.14\n",
      "| epoch  37 |     8/   10 batches | lr 16.21 | ms/batch 70.02 | loss 11.11 | ppl 66616.66\n",
      "| epoch  37 |    10/   10 batches | lr 16.21 | ms/batch 54.11 | loss  9.92 | ppl 20428.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time:  0.84s | valid loss 14.16 | valid ppl 1417509.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  38 |     2/   10 batches | lr 16.21 | ms/batch 130.49 | loss 15.43 | ppl 5012419.04\n",
      "| epoch  38 |     4/   10 batches | lr 16.21 | ms/batch 66.64 | loss 11.78 | ppl 131249.56\n",
      "| epoch  38 |     6/   10 batches | lr 16.21 | ms/batch 70.17 | loss  8.91 | ppl  7375.77\n",
      "| epoch  38 |     8/   10 batches | lr 16.21 | ms/batch 71.74 | loss  9.72 | ppl 16618.98\n",
      "| epoch  38 |    10/   10 batches | lr 16.21 | ms/batch 56.50 | loss 10.37 | ppl 31867.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time:  0.88s | valid loss  9.60 | valid ppl 14834.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  39 |     2/   10 batches | lr 16.21 | ms/batch 103.77 | loss 18.89 | ppl 160306497.83\n",
      "| epoch  39 |     4/   10 batches | lr 16.21 | ms/batch 68.20 | loss  9.58 | ppl 14438.84\n",
      "| epoch  39 |     6/   10 batches | lr 16.21 | ms/batch 66.00 | loss  9.28 | ppl 10744.02\n",
      "| epoch  39 |     8/   10 batches | lr 16.21 | ms/batch 70.58 | loss  8.96 | ppl  7799.59\n",
      "| epoch  39 |    10/   10 batches | lr 16.21 | ms/batch 51.21 | loss 11.04 | ppl 62010.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time:  0.81s | valid loss 11.07 | valid ppl 64081.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  40 |     2/   10 batches | lr 15.40 | ms/batch 110.16 | loss 13.88 | ppl 1068402.65\n",
      "| epoch  40 |     4/   10 batches | lr 15.40 | ms/batch 71.61 | loss 10.24 | ppl 28041.43\n",
      "| epoch  40 |     6/   10 batches | lr 15.40 | ms/batch 71.14 | loss  9.05 | ppl  8487.91\n",
      "| epoch  40 |     8/   10 batches | lr 15.40 | ms/batch 69.50 | loss 13.85 | ppl 1038857.85\n",
      "| epoch  40 |    10/   10 batches | lr 15.40 | ms/batch 55.08 | loss 11.06 | ppl 63521.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time:  0.84s | valid loss 10.11 | valid ppl 24503.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  41 |     2/   10 batches | lr 15.40 | ms/batch 102.77 | loss 15.96 | ppl 8544497.15\n",
      "| epoch  41 |     4/   10 batches | lr 15.40 | ms/batch 69.67 | loss  8.69 | ppl  5916.47\n",
      "| epoch  41 |     6/   10 batches | lr 15.40 | ms/batch 69.64 | loss  9.73 | ppl 16736.79\n",
      "| epoch  41 |     8/   10 batches | lr 15.40 | ms/batch 70.01 | loss  9.58 | ppl 14531.33\n",
      "| epoch  41 |    10/   10 batches | lr 15.40 | ms/batch 52.99 | loss  8.85 | ppl  6996.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  41 | time:  0.83s | valid loss  9.77 | valid ppl 17515.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  42 |     2/   10 batches | lr 15.40 | ms/batch 103.25 | loss 13.25 | ppl 569339.16\n",
      "| epoch  42 |     4/   10 batches | lr 15.40 | ms/batch 68.18 | loss  8.10 | ppl  3282.31\n",
      "| epoch  42 |     6/   10 batches | lr 15.40 | ms/batch 73.49 | loss  8.92 | ppl  7507.38\n",
      "| epoch  42 |     8/   10 batches | lr 15.40 | ms/batch 65.01 | loss  8.63 | ppl  5571.30\n",
      "| epoch  42 |    10/   10 batches | lr 15.40 | ms/batch 48.37 | loss  9.15 | ppl  9410.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  42 | time:  0.81s | valid loss  8.07 | valid ppl  3191.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  43 |     2/   10 batches | lr 14.63 | ms/batch 112.25 | loss 17.16 | ppl 28407567.21\n",
      "| epoch  43 |     4/   10 batches | lr 14.63 | ms/batch 66.00 | loss  8.02 | ppl  3042.20\n",
      "| epoch  43 |     6/   10 batches | lr 14.63 | ms/batch 70.14 | loss  8.87 | ppl  7139.31\n",
      "| epoch  43 |     8/   10 batches | lr 14.63 | ms/batch 68.01 | loss 10.81 | ppl 49301.13\n",
      "| epoch  43 |    10/   10 batches | lr 14.63 | ms/batch 53.75 | loss  9.20 | ppl  9920.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  43 | time:  0.84s | valid loss 10.02 | valid ppl 22572.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  44 |     2/   10 batches | lr 14.63 | ms/batch 103.06 | loss 13.93 | ppl 1120888.98\n",
      "| epoch  44 |     4/   10 batches | lr 14.63 | ms/batch 69.14 | loss 10.26 | ppl 28564.72\n",
      "| epoch  44 |     6/   10 batches | lr 14.63 | ms/batch 68.14 | loss  7.97 | ppl  2880.72\n",
      "| epoch  44 |     8/   10 batches | lr 14.63 | ms/batch 66.88 | loss  7.98 | ppl  2918.80\n",
      "| epoch  44 |    10/   10 batches | lr 14.63 | ms/batch 55.13 | loss 12.11 | ppl 182255.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  44 | time:  0.82s | valid loss 10.40 | valid ppl 32902.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  45 |     2/   10 batches | lr 14.63 | ms/batch 103.37 | loss 13.88 | ppl 1068319.11\n",
      "| epoch  45 |     4/   10 batches | lr 14.63 | ms/batch 65.50 | loss  7.26 | ppl  1428.96\n",
      "| epoch  45 |     6/   10 batches | lr 14.63 | ms/batch 70.08 | loss  8.02 | ppl  3039.81\n",
      "| epoch  45 |     8/   10 batches | lr 14.63 | ms/batch 66.65 | loss  8.88 | ppl  7212.56\n",
      "| epoch  45 |    10/   10 batches | lr 14.63 | ms/batch 51.11 | loss  7.58 | ppl  1948.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  45 | time:  0.81s | valid loss  8.26 | valid ppl  3875.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  46 |     2/   10 batches | lr 13.90 | ms/batch 103.11 | loss 11.91 | ppl 148687.87\n",
      "| epoch  46 |     4/   10 batches | lr 13.90 | ms/batch 66.37 | loss  8.27 | ppl  3911.64\n",
      "| epoch  46 |     6/   10 batches | lr 13.90 | ms/batch 68.14 | loss  7.44 | ppl  1698.11\n",
      "| epoch  46 |     8/   10 batches | lr 13.90 | ms/batch 66.88 | loss 12.94 | ppl 416869.65\n",
      "| epoch  46 |    10/   10 batches | lr 13.90 | ms/batch 51.67 | loss  7.45 | ppl  1717.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  46 | time:  0.80s | valid loss  9.33 | valid ppl 11234.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  47 |     2/   10 batches | lr 13.90 | ms/batch 104.32 | loss 12.64 | ppl 307982.89\n",
      "| epoch  47 |     4/   10 batches | lr 13.90 | ms/batch 70.11 | loss  9.00 | ppl  8063.53\n",
      "| epoch  47 |     6/   10 batches | lr 13.90 | ms/batch 67.13 | loss  8.22 | ppl  3716.27\n",
      "| epoch  47 |     8/   10 batches | lr 13.90 | ms/batch 66.13 | loss  7.53 | ppl  1858.68\n",
      "| epoch  47 |    10/   10 batches | lr 13.90 | ms/batch 51.82 | loss  8.32 | ppl  4112.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  47 | time:  0.80s | valid loss  6.85 | valid ppl   946.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  48 |     2/   10 batches | lr 13.90 | ms/batch 103.90 | loss 12.51 | ppl 271553.58\n",
      "| epoch  48 |     4/   10 batches | lr 13.90 | ms/batch 64.13 | loss 11.41 | ppl 89998.83\n",
      "| epoch  48 |     6/   10 batches | lr 13.90 | ms/batch 68.32 | loss  8.00 | ppl  2968.88\n",
      "| epoch  48 |     8/   10 batches | lr 13.90 | ms/batch 67.24 | loss  8.45 | ppl  4664.29\n",
      "| epoch  48 |    10/   10 batches | lr 13.90 | ms/batch 53.60 | loss  7.33 | ppl  1521.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  48 | time:  0.80s | valid loss 10.11 | valid ppl 24583.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  49 |     2/   10 batches | lr 13.20 | ms/batch 103.34 | loss 12.29 | ppl 217929.28\n",
      "| epoch  49 |     4/   10 batches | lr 13.20 | ms/batch 64.88 | loss  8.03 | ppl  3057.48\n",
      "| epoch  49 |     6/   10 batches | lr 13.20 | ms/batch 66.99 | loss  9.78 | ppl 17750.03\n",
      "| epoch  49 |     8/   10 batches | lr 13.20 | ms/batch 69.82 | loss  6.74 | ppl   844.41\n",
      "| epoch  49 |    10/   10 batches | lr 13.20 | ms/batch 50.14 | loss  6.60 | ppl   737.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  49 | time:  0.80s | valid loss  8.75 | valid ppl  6315.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  50 |     2/   10 batches | lr 13.20 | ms/batch 105.25 | loss 16.01 | ppl 8995891.73\n",
      "| epoch  50 |     4/   10 batches | lr 13.20 | ms/batch 66.50 | loss  8.14 | ppl  3437.95\n",
      "| epoch  50 |     6/   10 batches | lr 13.20 | ms/batch 67.00 | loss  7.06 | ppl  1168.30\n",
      "| epoch  50 |     8/   10 batches | lr 13.20 | ms/batch 64.32 | loss  7.32 | ppl  1514.23\n",
      "| epoch  50 |    10/   10 batches | lr 13.20 | ms/batch 51.62 | loss  7.25 | ppl  1412.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  50 | time:  0.80s | valid loss  8.47 | valid ppl  4770.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  51 |     2/   10 batches | lr 13.20 | ms/batch 101.00 | loss 11.16 | ppl 70417.70\n",
      "| epoch  51 |     4/   10 batches | lr 13.20 | ms/batch 74.51 | loss  7.78 | ppl  2381.04\n",
      "| epoch  51 |     6/   10 batches | lr 13.20 | ms/batch 67.26 | loss  7.56 | ppl  1925.82\n",
      "| epoch  51 |     8/   10 batches | lr 13.20 | ms/batch 66.09 | loss  7.61 | ppl  2010.39\n",
      "| epoch  51 |    10/   10 batches | lr 13.20 | ms/batch 46.00 | loss  9.47 | ppl 12970.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  51 | time:  0.81s | valid loss  7.44 | valid ppl  1701.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  52 |     2/   10 batches | lr 12.54 | ms/batch 106.80 | loss 10.25 | ppl 28293.68\n",
      "| epoch  52 |     4/   10 batches | lr 12.54 | ms/batch 68.27 | loss  6.18 | ppl   481.36\n",
      "| epoch  52 |     6/   10 batches | lr 12.54 | ms/batch 68.50 | loss 12.49 | ppl 265581.73\n",
      "| epoch  52 |     8/   10 batches | lr 12.54 | ms/batch 64.50 | loss  8.13 | ppl  3408.47\n",
      "| epoch  52 |    10/   10 batches | lr 12.54 | ms/batch 52.01 | loss  7.06 | ppl  1160.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  52 | time:  0.81s | valid loss 11.38 | valid ppl 87737.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  53 |     2/   10 batches | lr 12.54 | ms/batch 103.00 | loss 12.88 | ppl 392592.23\n",
      "| epoch  53 |     4/   10 batches | lr 12.54 | ms/batch 67.00 | loss  7.27 | ppl  1442.06\n",
      "| epoch  53 |     6/   10 batches | lr 12.54 | ms/batch 67.67 | loss  8.72 | ppl  6146.87\n",
      "| epoch  53 |     8/   10 batches | lr 12.54 | ms/batch 68.97 | loss  7.03 | ppl  1131.46\n",
      "| epoch  53 |    10/   10 batches | lr 12.54 | ms/batch 45.64 | loss  8.02 | ppl  3055.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  53 | time:  0.79s | valid loss  7.30 | valid ppl  1485.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  54 |     2/   10 batches | lr 12.54 | ms/batch 180.11 | loss 10.30 | ppl 29658.07\n",
      "| epoch  54 |     4/   10 batches | lr 12.54 | ms/batch 76.50 | loss  5.99 | ppl   397.67\n",
      "| epoch  54 |     6/   10 batches | lr 12.54 | ms/batch 77.84 | loss  6.49 | ppl   660.62\n",
      "| epoch  54 |     8/   10 batches | lr 12.54 | ms/batch 78.44 | loss  7.44 | ppl  1702.49\n",
      "| epoch  54 |    10/   10 batches | lr 12.54 | ms/batch 68.89 | loss  6.88 | ppl   969.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  54 | time:  1.09s | valid loss  6.46 | valid ppl   640.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  55 |     2/   10 batches | lr 11.92 | ms/batch 158.10 | loss  9.46 | ppl 12842.14\n",
      "| epoch  55 |     4/   10 batches | lr 11.92 | ms/batch 73.50 | loss  8.25 | ppl  3833.20\n",
      "| epoch  55 |     6/   10 batches | lr 11.92 | ms/batch 68.26 | loss  7.58 | ppl  1951.36\n",
      "| epoch  55 |     8/   10 batches | lr 11.92 | ms/batch 67.50 | loss  6.61 | ppl   742.69\n",
      "| epoch  55 |    10/   10 batches | lr 11.92 | ms/batch 54.51 | loss  6.47 | ppl   645.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  55 | time:  0.93s | valid loss  6.21 | valid ppl   499.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  56 |     2/   10 batches | lr 11.92 | ms/batch 102.37 | loss  9.63 | ppl 15183.50\n",
      "| epoch  56 |     4/   10 batches | lr 11.92 | ms/batch 64.25 | loss  6.94 | ppl  1035.72\n",
      "| epoch  56 |     6/   10 batches | lr 11.92 | ms/batch 67.79 | loss  6.40 | ppl   602.30\n",
      "| epoch  56 |     8/   10 batches | lr 11.92 | ms/batch 64.50 | loss  7.53 | ppl  1857.70\n",
      "| epoch  56 |    10/   10 batches | lr 11.92 | ms/batch 49.91 | loss  9.09 | ppl  8871.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  56 | time:  0.79s | valid loss  7.92 | valid ppl  2745.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  57 |     2/   10 batches | lr 11.92 | ms/batch 105.11 | loss  9.98 | ppl 21549.70\n",
      "| epoch  57 |     4/   10 batches | lr 11.92 | ms/batch 62.51 | loss  6.48 | ppl   652.62\n",
      "| epoch  57 |     6/   10 batches | lr 11.92 | ms/batch 66.76 | loss  6.57 | ppl   715.95\n",
      "| epoch  57 |     8/   10 batches | lr 11.92 | ms/batch 69.63 | loss  6.14 | ppl   465.67\n",
      "| epoch  57 |    10/   10 batches | lr 11.92 | ms/batch 51.50 | loss  6.91 | ppl   999.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  57 | time:  0.80s | valid loss  6.13 | valid ppl   460.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  58 |     2/   10 batches | lr 11.32 | ms/batch 94.64 | loss  9.36 | ppl 11606.61\n",
      "| epoch  58 |     4/   10 batches | lr 11.32 | ms/batch 62.69 | loss  6.76 | ppl   862.04\n",
      "| epoch  58 |     6/   10 batches | lr 11.32 | ms/batch 69.56 | loss  6.47 | ppl   643.82\n",
      "| epoch  58 |     8/   10 batches | lr 11.32 | ms/batch 70.81 | loss  6.56 | ppl   704.78\n",
      "| epoch  58 |    10/   10 batches | lr 11.32 | ms/batch 52.07 | loss  6.10 | ppl   447.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  58 | time:  0.79s | valid loss  6.11 | valid ppl   452.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  59 |     2/   10 batches | lr 11.32 | ms/batch 97.64 | loss  9.67 | ppl 15788.65\n",
      "| epoch  59 |     4/   10 batches | lr 11.32 | ms/batch 62.31 | loss  5.91 | ppl   369.45\n",
      "| epoch  59 |     6/   10 batches | lr 11.32 | ms/batch 65.49 | loss  6.22 | ppl   504.78\n",
      "| epoch  59 |     8/   10 batches | lr 11.32 | ms/batch 72.65 | loss  6.53 | ppl   687.68\n",
      "| epoch  59 |    10/   10 batches | lr 11.32 | ms/batch 54.49 | loss  7.40 | ppl  1636.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  59 | time:  0.79s | valid loss  6.92 | valid ppl  1014.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  60 |     2/   10 batches | lr 11.32 | ms/batch 99.92 | loss  9.82 | ppl 18356.41\n",
      "| epoch  60 |     4/   10 batches | lr 11.32 | ms/batch 66.12 | loss  6.27 | ppl   527.59\n",
      "| epoch  60 |     6/   10 batches | lr 11.32 | ms/batch 64.65 | loss  6.41 | ppl   610.73\n",
      "| epoch  60 |     8/   10 batches | lr 11.32 | ms/batch 69.61 | loss  5.93 | ppl   375.21\n",
      "| epoch  60 |    10/   10 batches | lr 11.32 | ms/batch 49.61 | loss  7.46 | ppl  1735.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  60 | time:  0.79s | valid loss  6.61 | valid ppl   744.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  61 |     2/   10 batches | lr 10.75 | ms/batch 95.63 | loss  9.49 | ppl 13229.65\n",
      "| epoch  61 |     4/   10 batches | lr 10.75 | ms/batch 69.50 | loss  7.13 | ppl  1245.63\n",
      "| epoch  61 |     6/   10 batches | lr 10.75 | ms/batch 64.29 | loss  6.04 | ppl   419.87\n",
      "| epoch  61 |     8/   10 batches | lr 10.75 | ms/batch 63.93 | loss  6.36 | ppl   576.22\n",
      "| epoch  61 |    10/   10 batches | lr 10.75 | ms/batch 49.64 | loss  5.82 | ppl   336.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  61 | time:  0.78s | valid loss  6.98 | valid ppl  1077.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  62 |     2/   10 batches | lr 10.75 | ms/batch 104.01 | loss  9.84 | ppl 18718.51\n",
      "| epoch  62 |     4/   10 batches | lr 10.75 | ms/batch 68.99 | loss  6.13 | ppl   458.48\n",
      "| epoch  62 |     6/   10 batches | lr 10.75 | ms/batch 60.28 | loss  5.95 | ppl   384.06\n",
      "| epoch  62 |     8/   10 batches | lr 10.75 | ms/batch 65.00 | loss  6.18 | ppl   481.27\n",
      "| epoch  62 |    10/   10 batches | lr 10.75 | ms/batch 54.15 | loss  6.02 | ppl   411.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  62 | time:  0.80s | valid loss  6.09 | valid ppl   440.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  63 |     2/   10 batches | lr 10.75 | ms/batch 100.92 | loss 10.57 | ppl 38883.45\n",
      "| epoch  63 |     4/   10 batches | lr 10.75 | ms/batch 66.99 | loss  5.98 | ppl   396.35\n",
      "| epoch  63 |     6/   10 batches | lr 10.75 | ms/batch 65.08 | loss  5.87 | ppl   353.70\n",
      "| epoch  63 |     8/   10 batches | lr 10.75 | ms/batch 67.74 | loss  5.86 | ppl   349.51\n",
      "| epoch  63 |    10/   10 batches | lr 10.75 | ms/batch 50.16 | loss  6.13 | ppl   457.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  63 | time:  0.79s | valid loss  5.99 | valid ppl   399.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  64 |     2/   10 batches | lr 10.22 | ms/batch 104.50 | loss  8.91 | ppl  7389.48\n",
      "| epoch  64 |     4/   10 batches | lr 10.22 | ms/batch 59.18 | loss  5.89 | ppl   362.96\n",
      "| epoch  64 |     6/   10 batches | lr 10.22 | ms/batch 67.91 | loss  5.71 | ppl   302.23\n",
      "| epoch  64 |     8/   10 batches | lr 10.22 | ms/batch 68.04 | loss  5.71 | ppl   302.17\n",
      "| epoch  64 |    10/   10 batches | lr 10.22 | ms/batch 51.51 | loss  5.71 | ppl   301.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  64 | time:  0.79s | valid loss  6.09 | valid ppl   442.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  65 |     2/   10 batches | lr 10.22 | ms/batch 95.76 | loss  8.91 | ppl  7417.45\n",
      "| epoch  65 |     4/   10 batches | lr 10.22 | ms/batch 69.58 | loss  6.07 | ppl   430.72\n",
      "| epoch  65 |     6/   10 batches | lr 10.22 | ms/batch 66.35 | loss  5.88 | ppl   356.88\n",
      "| epoch  65 |     8/   10 batches | lr 10.22 | ms/batch 67.90 | loss  5.93 | ppl   374.84\n",
      "| epoch  65 |    10/   10 batches | lr 10.22 | ms/batch 47.73 | loss  5.97 | ppl   389.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  65 | time:  0.79s | valid loss  6.04 | valid ppl   419.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  66 |     2/   10 batches | lr 10.22 | ms/batch 99.93 | loss  8.85 | ppl  6964.22\n",
      "| epoch  66 |     4/   10 batches | lr 10.22 | ms/batch 65.44 | loss  5.85 | ppl   347.91\n",
      "| epoch  66 |     6/   10 batches | lr 10.22 | ms/batch 65.50 | loss  5.80 | ppl   330.11\n",
      "| epoch  66 |     8/   10 batches | lr 10.22 | ms/batch 61.01 | loss  5.65 | ppl   282.91\n",
      "| epoch  66 |    10/   10 batches | lr 10.22 | ms/batch 50.62 | loss  5.71 | ppl   300.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  66 | time:  0.77s | valid loss  5.98 | valid ppl   394.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  67 |     2/   10 batches | lr 9.71 | ms/batch 103.08 | loss  8.73 | ppl  6190.96\n",
      "| epoch  67 |     4/   10 batches | lr 9.71 | ms/batch 60.50 | loss  5.57 | ppl   261.77\n",
      "| epoch  67 |     6/   10 batches | lr 9.71 | ms/batch 69.54 | loss  5.82 | ppl   338.09\n",
      "| epoch  67 |     8/   10 batches | lr 9.71 | ms/batch 61.99 | loss  5.66 | ppl   287.36\n",
      "| epoch  67 |    10/   10 batches | lr 9.71 | ms/batch 48.02 | loss  5.73 | ppl   308.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  67 | time:  0.85s | valid loss  5.87 | valid ppl   354.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  68 |     2/   10 batches | lr 9.71 | ms/batch 119.37 | loss  8.54 | ppl  5096.28\n",
      "| epoch  68 |     4/   10 batches | lr 9.71 | ms/batch 74.01 | loss  5.54 | ppl   254.35\n",
      "| epoch  68 |     6/   10 batches | lr 9.71 | ms/batch 75.60 | loss  5.75 | ppl   313.10\n",
      "| epoch  68 |     8/   10 batches | lr 9.71 | ms/batch 69.50 | loss  5.61 | ppl   272.59\n",
      "| epoch  68 |    10/   10 batches | lr 9.71 | ms/batch 53.72 | loss  5.69 | ppl   296.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  68 | time:  0.88s | valid loss  5.86 | valid ppl   349.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  69 |     2/   10 batches | lr 9.71 | ms/batch 110.86 | loss  8.54 | ppl  5091.86\n",
      "| epoch  69 |     4/   10 batches | lr 9.71 | ms/batch 71.95 | loss  6.57 | ppl   714.44\n",
      "| epoch  69 |     6/   10 batches | lr 9.71 | ms/batch 67.95 | loss  5.69 | ppl   297.05\n",
      "| epoch  69 |     8/   10 batches | lr 9.71 | ms/batch 69.49 | loss  5.75 | ppl   314.29\n",
      "| epoch  69 |    10/   10 batches | lr 9.71 | ms/batch 51.93 | loss  5.61 | ppl   274.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  69 | time:  0.83s | valid loss  5.89 | valid ppl   360.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  70 |     2/   10 batches | lr 9.22 | ms/batch 100.18 | loss  8.78 | ppl  6481.56\n",
      "| epoch  70 |     4/   10 batches | lr 9.22 | ms/batch 63.79 | loss  5.78 | ppl   322.87\n",
      "| epoch  70 |     6/   10 batches | lr 9.22 | ms/batch 67.66 | loss  5.82 | ppl   337.82\n",
      "| epoch  70 |     8/   10 batches | lr 9.22 | ms/batch 70.40 | loss  5.89 | ppl   361.01\n",
      "| epoch  70 |    10/   10 batches | lr 9.22 | ms/batch 44.91 | loss  5.75 | ppl   314.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  70 | time:  0.79s | valid loss  5.82 | valid ppl   336.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  71 |     2/   10 batches | lr 9.22 | ms/batch 102.09 | loss  8.68 | ppl  5886.58\n",
      "| epoch  71 |     4/   10 batches | lr 9.22 | ms/batch 67.00 | loss  5.51 | ppl   246.78\n",
      "| epoch  71 |     6/   10 batches | lr 9.22 | ms/batch 66.26 | loss  5.71 | ppl   302.74\n",
      "| epoch  71 |     8/   10 batches | lr 9.22 | ms/batch 66.20 | loss  5.70 | ppl   299.05\n",
      "| epoch  71 |    10/   10 batches | lr 9.22 | ms/batch 49.65 | loss  5.63 | ppl   278.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  71 | time:  0.78s | valid loss  5.84 | valid ppl   344.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  72 |     2/   10 batches | lr 9.22 | ms/batch 101.88 | loss  8.59 | ppl  5385.46\n",
      "| epoch  72 |     4/   10 batches | lr 9.22 | ms/batch 66.00 | loss  6.04 | ppl   418.79\n",
      "| epoch  72 |     6/   10 batches | lr 9.22 | ms/batch 64.64 | loss  5.70 | ppl   297.55\n",
      "| epoch  72 |     8/   10 batches | lr 9.22 | ms/batch 62.92 | loss  5.59 | ppl   267.55\n",
      "| epoch  72 |    10/   10 batches | lr 9.22 | ms/batch 52.13 | loss  5.69 | ppl   294.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  72 | time:  0.79s | valid loss  6.37 | valid ppl   586.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  73 |     2/   10 batches | lr 8.76 | ms/batch 101.98 | loss  8.76 | ppl  6370.31\n",
      "| epoch  73 |     4/   10 batches | lr 8.76 | ms/batch 61.13 | loss  5.54 | ppl   255.33\n",
      "| epoch  73 |     6/   10 batches | lr 8.76 | ms/batch 72.06 | loss  5.93 | ppl   376.26\n",
      "| epoch  73 |     8/   10 batches | lr 8.76 | ms/batch 64.50 | loss  5.59 | ppl   267.14\n",
      "| epoch  73 |    10/   10 batches | lr 8.76 | ms/batch 48.00 | loss  5.91 | ppl   369.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  73 | time:  0.79s | valid loss  5.85 | valid ppl   348.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  74 |     2/   10 batches | lr 8.76 | ms/batch 96.64 | loss  8.65 | ppl  5725.36\n",
      "| epoch  74 |     4/   10 batches | lr 8.76 | ms/batch 67.15 | loss  5.66 | ppl   287.60\n",
      "| epoch  74 |     6/   10 batches | lr 8.76 | ms/batch 67.49 | loss  5.87 | ppl   354.52\n",
      "| epoch  74 |     8/   10 batches | lr 8.76 | ms/batch 63.01 | loss  5.74 | ppl   309.60\n",
      "| epoch  74 |    10/   10 batches | lr 8.76 | ms/batch 49.22 | loss  5.71 | ppl   300.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  74 | time:  0.77s | valid loss  5.78 | valid ppl   322.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  75 |     2/   10 batches | lr 8.76 | ms/batch 101.25 | loss  8.41 | ppl  4503.56\n",
      "| epoch  75 |     4/   10 batches | lr 8.76 | ms/batch 66.22 | loss  5.67 | ppl   290.50\n",
      "| epoch  75 |     6/   10 batches | lr 8.76 | ms/batch 68.65 | loss  5.64 | ppl   280.47\n",
      "| epoch  75 |     8/   10 batches | lr 8.76 | ms/batch 68.50 | loss  5.67 | ppl   289.16\n",
      "| epoch  75 |    10/   10 batches | lr 8.76 | ms/batch 53.15 | loss  5.66 | ppl   286.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  75 | time:  0.81s | valid loss  5.83 | valid ppl   339.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  76 |     2/   10 batches | lr 8.32 | ms/batch 99.51 | loss  8.45 | ppl  4697.97\n",
      "| epoch  76 |     4/   10 batches | lr 8.32 | ms/batch 67.24 | loss  5.52 | ppl   248.99\n",
      "| epoch  76 |     6/   10 batches | lr 8.32 | ms/batch 63.11 | loss  5.69 | ppl   295.52\n",
      "| epoch  76 |     8/   10 batches | lr 8.32 | ms/batch 67.21 | loss  5.74 | ppl   310.96\n",
      "| epoch  76 |    10/   10 batches | lr 8.32 | ms/batch 49.51 | loss  5.63 | ppl   277.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  76 | time:  0.79s | valid loss  5.84 | valid ppl   343.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  77 |     2/   10 batches | lr 8.32 | ms/batch 90.25 | loss  8.55 | ppl  5141.57\n",
      "| epoch  77 |     4/   10 batches | lr 8.32 | ms/batch 70.64 | loss  5.63 | ppl   278.74\n",
      "| epoch  77 |     6/   10 batches | lr 8.32 | ms/batch 64.64 | loss  5.66 | ppl   286.97\n",
      "| epoch  77 |     8/   10 batches | lr 8.32 | ms/batch 70.78 | loss  5.64 | ppl   280.38\n",
      "| epoch  77 |    10/   10 batches | lr 8.32 | ms/batch 51.24 | loss  5.67 | ppl   290.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  77 | time:  0.79s | valid loss  5.79 | valid ppl   328.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  78 |     2/   10 batches | lr 8.32 | ms/batch 100.51 | loss  8.41 | ppl  4502.35\n",
      "| epoch  78 |     4/   10 batches | lr 8.32 | ms/batch 67.06 | loss  5.54 | ppl   255.14\n",
      "| epoch  78 |     6/   10 batches | lr 8.32 | ms/batch 70.21 | loss  5.65 | ppl   285.35\n",
      "| epoch  78 |     8/   10 batches | lr 8.32 | ms/batch 64.00 | loss  5.62 | ppl   275.84\n",
      "| epoch  78 |    10/   10 batches | lr 8.32 | ms/batch 45.13 | loss  5.66 | ppl   288.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  78 | time:  0.83s | valid loss  5.82 | valid ppl   338.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  79 |     2/   10 batches | lr 7.91 | ms/batch 101.33 | loss  8.43 | ppl  4596.50\n",
      "| epoch  79 |     4/   10 batches | lr 7.91 | ms/batch 63.30 | loss  5.54 | ppl   253.99\n",
      "| epoch  79 |     6/   10 batches | lr 7.91 | ms/batch 70.14 | loss  5.71 | ppl   301.84\n",
      "| epoch  79 |     8/   10 batches | lr 7.91 | ms/batch 64.77 | loss  5.62 | ppl   276.59\n",
      "| epoch  79 |    10/   10 batches | lr 7.91 | ms/batch 47.63 | loss  5.64 | ppl   281.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  79 | time:  0.78s | valid loss  5.81 | valid ppl   334.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  80 |     2/   10 batches | lr 7.91 | ms/batch 105.34 | loss  8.43 | ppl  4585.48\n",
      "| epoch  80 |     4/   10 batches | lr 7.91 | ms/batch 67.62 | loss  5.53 | ppl   251.85\n",
      "| epoch  80 |     6/   10 batches | lr 7.91 | ms/batch 64.68 | loss  5.67 | ppl   289.38\n",
      "| epoch  80 |     8/   10 batches | lr 7.91 | ms/batch 66.78 | loss  5.64 | ppl   280.28\n",
      "| epoch  80 |    10/   10 batches | lr 7.91 | ms/batch 45.48 | loss  5.63 | ppl   279.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  80 | time:  0.79s | valid loss  5.81 | valid ppl   332.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  81 |     2/   10 batches | lr 7.91 | ms/batch 103.18 | loss  8.43 | ppl  4598.44\n",
      "| epoch  81 |     4/   10 batches | lr 7.91 | ms/batch 60.99 | loss  5.53 | ppl   253.34\n",
      "| epoch  81 |     6/   10 batches | lr 7.91 | ms/batch 68.80 | loss  5.66 | ppl   288.35\n",
      "| epoch  81 |     8/   10 batches | lr 7.91 | ms/batch 64.83 | loss  5.63 | ppl   277.67\n",
      "| epoch  81 |    10/   10 batches | lr 7.91 | ms/batch 48.25 | loss  5.63 | ppl   279.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  81 | time:  0.79s | valid loss  5.83 | valid ppl   338.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  82 |     2/   10 batches | lr 7.51 | ms/batch 101.47 | loss  8.44 | ppl  4612.94\n",
      "| epoch  82 |     4/   10 batches | lr 7.51 | ms/batch 63.78 | loss  5.52 | ppl   250.16\n",
      "| epoch  82 |     6/   10 batches | lr 7.51 | ms/batch 70.10 | loss  5.66 | ppl   287.36\n",
      "| epoch  82 |     8/   10 batches | lr 7.51 | ms/batch 62.51 | loss  5.62 | ppl   276.50\n",
      "| epoch  82 |    10/   10 batches | lr 7.51 | ms/batch 49.61 | loss  5.63 | ppl   279.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  82 | time:  0.79s | valid loss  5.79 | valid ppl   328.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  83 |     2/   10 batches | lr 7.51 | ms/batch 100.87 | loss  8.44 | ppl  4609.18\n",
      "| epoch  83 |     4/   10 batches | lr 7.51 | ms/batch 63.38 | loss  5.51 | ppl   246.12\n",
      "| epoch  83 |     6/   10 batches | lr 7.51 | ms/batch 64.26 | loss  5.66 | ppl   286.99\n",
      "| epoch  83 |     8/   10 batches | lr 7.51 | ms/batch 67.01 | loss  5.62 | ppl   276.79\n",
      "| epoch  83 |    10/   10 batches | lr 7.51 | ms/batch 52.44 | loss  5.64 | ppl   280.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  83 | time:  0.78s | valid loss  5.81 | valid ppl   332.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  84 |     2/   10 batches | lr 7.51 | ms/batch 104.16 | loss  8.44 | ppl  4630.34\n",
      "| epoch  84 |     4/   10 batches | lr 7.51 | ms/batch 66.50 | loss  5.51 | ppl   247.01\n",
      "| epoch  84 |     6/   10 batches | lr 7.51 | ms/batch 67.13 | loss  5.66 | ppl   288.53\n",
      "| epoch  84 |     8/   10 batches | lr 7.51 | ms/batch 66.05 | loss  5.63 | ppl   278.31\n",
      "| epoch  84 |    10/   10 batches | lr 7.51 | ms/batch 50.73 | loss  5.64 | ppl   280.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  84 | time:  0.80s | valid loss  5.81 | valid ppl   334.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  85 |     2/   10 batches | lr 7.13 | ms/batch 94.63 | loss  8.44 | ppl  4636.24\n",
      "| epoch  85 |     4/   10 batches | lr 7.13 | ms/batch 64.63 | loss  5.51 | ppl   246.77\n",
      "| epoch  85 |     6/   10 batches | lr 7.13 | ms/batch 65.27 | loss  5.67 | ppl   289.13\n",
      "| epoch  85 |     8/   10 batches | lr 7.13 | ms/batch 65.38 | loss  5.62 | ppl   276.80\n",
      "| epoch  85 |    10/   10 batches | lr 7.13 | ms/batch 51.50 | loss  5.66 | ppl   286.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  85 | time:  0.77s | valid loss  5.79 | valid ppl   328.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  86 |     2/   10 batches | lr 7.13 | ms/batch 96.93 | loss  8.43 | ppl  4572.98\n",
      "| epoch  86 |     4/   10 batches | lr 7.13 | ms/batch 67.00 | loss  5.50 | ppl   244.93\n",
      "| epoch  86 |     6/   10 batches | lr 7.13 | ms/batch 66.50 | loss  5.67 | ppl   289.93\n",
      "| epoch  86 |     8/   10 batches | lr 7.13 | ms/batch 60.50 | loss  5.63 | ppl   277.82\n",
      "| epoch  86 |    10/   10 batches | lr 7.13 | ms/batch 48.50 | loss  5.63 | ppl   277.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  86 | time:  0.77s | valid loss  5.81 | valid ppl   333.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  87 |     2/   10 batches | lr 7.13 | ms/batch 93.71 | loss  8.43 | ppl  4604.81\n",
      "| epoch  87 |     4/   10 batches | lr 7.13 | ms/batch 63.00 | loss  5.50 | ppl   245.22\n",
      "| epoch  87 |     6/   10 batches | lr 7.13 | ms/batch 70.00 | loss  5.66 | ppl   288.49\n",
      "| epoch  87 |     8/   10 batches | lr 7.13 | ms/batch 62.51 | loss  5.63 | ppl   277.52\n",
      "| epoch  87 |    10/   10 batches | lr 7.13 | ms/batch 47.49 | loss  5.63 | ppl   277.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  87 | time:  0.76s | valid loss  5.81 | valid ppl   332.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  88 |     2/   10 batches | lr 6.78 | ms/batch 101.62 | loss  8.44 | ppl  4641.00\n",
      "| epoch  88 |     4/   10 batches | lr 6.78 | ms/batch 67.14 | loss  5.50 | ppl   245.34\n",
      "| epoch  88 |     6/   10 batches | lr 6.78 | ms/batch 89.45 | loss  5.66 | ppl   288.17\n",
      "| epoch  88 |     8/   10 batches | lr 6.78 | ms/batch 65.15 | loss  5.63 | ppl   278.91\n",
      "| epoch  88 |    10/   10 batches | lr 6.78 | ms/batch 51.01 | loss  5.63 | ppl   279.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  88 | time:  0.83s | valid loss  5.81 | valid ppl   332.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  89 |     2/   10 batches | lr 6.78 | ms/batch 100.64 | loss  8.43 | ppl  4582.22\n",
      "| epoch  89 |     4/   10 batches | lr 6.78 | ms/batch 67.16 | loss  5.50 | ppl   245.16\n",
      "| epoch  89 |     6/   10 batches | lr 6.78 | ms/batch 65.10 | loss  5.67 | ppl   289.07\n",
      "| epoch  89 |     8/   10 batches | lr 6.78 | ms/batch 63.00 | loss  5.63 | ppl   278.01\n",
      "| epoch  89 |    10/   10 batches | lr 6.78 | ms/batch 48.64 | loss  5.63 | ppl   278.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  89 | time:  0.78s | valid loss  5.78 | valid ppl   325.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  90 |     2/   10 batches | lr 6.78 | ms/batch 103.37 | loss  8.42 | ppl  4555.94\n",
      "| epoch  90 |     4/   10 batches | lr 6.78 | ms/batch 61.64 | loss  5.49 | ppl   242.78\n",
      "| epoch  90 |     6/   10 batches | lr 6.78 | ms/batch 65.67 | loss  5.66 | ppl   286.64\n",
      "| epoch  90 |     8/   10 batches | lr 6.78 | ms/batch 65.10 | loss  5.62 | ppl   276.69\n",
      "| epoch  90 |    10/   10 batches | lr 6.78 | ms/batch 49.83 | loss  5.64 | ppl   281.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  90 | time:  0.78s | valid loss  5.79 | valid ppl   327.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  91 |     2/   10 batches | lr 6.44 | ms/batch 97.66 | loss  8.43 | ppl  4588.78\n",
      "| epoch  91 |     4/   10 batches | lr 6.44 | ms/batch 67.00 | loss  5.50 | ppl   245.68\n",
      "| epoch  91 |     6/   10 batches | lr 6.44 | ms/batch 64.15 | loss  5.67 | ppl   288.97\n",
      "| epoch  91 |     8/   10 batches | lr 6.44 | ms/batch 60.50 | loss  5.62 | ppl   275.66\n",
      "| epoch  91 |    10/   10 batches | lr 6.44 | ms/batch 48.50 | loss  5.63 | ppl   279.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  91 | time:  0.76s | valid loss  5.81 | valid ppl   332.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  92 |     2/   10 batches | lr 6.44 | ms/batch 97.49 | loss  8.43 | ppl  4567.86\n",
      "| epoch  92 |     4/   10 batches | lr 6.44 | ms/batch 62.40 | loss  5.50 | ppl   245.04\n",
      "| epoch  92 |     6/   10 batches | lr 6.44 | ms/batch 68.75 | loss  5.67 | ppl   290.78\n",
      "| epoch  92 |     8/   10 batches | lr 6.44 | ms/batch 62.62 | loss  5.62 | ppl   276.02\n",
      "| epoch  92 |    10/   10 batches | lr 6.44 | ms/batch 47.59 | loss  5.63 | ppl   279.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  92 | time:  0.77s | valid loss  5.80 | valid ppl   328.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  93 |     2/   10 batches | lr 6.44 | ms/batch 101.00 | loss  8.43 | ppl  4575.44\n",
      "| epoch  93 |     4/   10 batches | lr 6.44 | ms/batch 65.00 | loss  5.51 | ppl   246.42\n",
      "| epoch  93 |     6/   10 batches | lr 6.44 | ms/batch 66.12 | loss  5.66 | ppl   288.07\n",
      "| epoch  93 |     8/   10 batches | lr 6.44 | ms/batch 65.26 | loss  5.62 | ppl   275.98\n",
      "| epoch  93 |    10/   10 batches | lr 6.44 | ms/batch 47.62 | loss  5.63 | ppl   279.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  93 | time:  0.78s | valid loss  5.79 | valid ppl   327.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  94 |     2/   10 batches | lr 6.12 | ms/batch 99.73 | loss  8.43 | ppl  4581.95\n",
      "| epoch  94 |     4/   10 batches | lr 6.12 | ms/batch 61.50 | loss  5.51 | ppl   246.27\n",
      "| epoch  94 |     6/   10 batches | lr 6.12 | ms/batch 68.77 | loss  5.66 | ppl   287.44\n",
      "| epoch  94 |     8/   10 batches | lr 6.12 | ms/batch 59.99 | loss  5.62 | ppl   276.07\n",
      "| epoch  94 |    10/   10 batches | lr 6.12 | ms/batch 47.00 | loss  5.64 | ppl   280.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  94 | time:  0.76s | valid loss  5.79 | valid ppl   327.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  95 |     2/   10 batches | lr 6.12 | ms/batch 102.45 | loss  8.43 | ppl  4562.50\n",
      "| epoch  95 |     4/   10 batches | lr 6.12 | ms/batch 66.19 | loss  5.50 | ppl   245.27\n",
      "| epoch  95 |     6/   10 batches | lr 6.12 | ms/batch 63.90 | loss  5.66 | ppl   288.16\n",
      "| epoch  95 |     8/   10 batches | lr 6.12 | ms/batch 60.81 | loss  5.62 | ppl   275.00\n",
      "| epoch  95 |    10/   10 batches | lr 6.12 | ms/batch 47.68 | loss  5.65 | ppl   284.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  95 | time:  0.78s | valid loss  5.78 | valid ppl   323.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  96 |     2/   10 batches | lr 6.12 | ms/batch 99.72 | loss  8.43 | ppl  4560.32\n",
      "| epoch  96 |     4/   10 batches | lr 6.12 | ms/batch 66.75 | loss  5.50 | ppl   243.85\n",
      "| epoch  96 |     6/   10 batches | lr 6.12 | ms/batch 65.50 | loss  5.66 | ppl   286.96\n",
      "| epoch  96 |     8/   10 batches | lr 6.12 | ms/batch 60.62 | loss  5.62 | ppl   276.63\n",
      "| epoch  96 |    10/   10 batches | lr 6.12 | ms/batch 47.13 | loss  5.63 | ppl   277.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  96 | time:  0.77s | valid loss  5.80 | valid ppl   330.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  97 |     2/   10 batches | lr 5.81 | ms/batch 125.25 | loss  8.42 | ppl  4547.87\n",
      "| epoch  97 |     4/   10 batches | lr 5.81 | ms/batch 64.51 | loss  5.50 | ppl   245.34\n",
      "| epoch  97 |     6/   10 batches | lr 5.81 | ms/batch 65.11 | loss  5.67 | ppl   289.29\n",
      "| epoch  97 |     8/   10 batches | lr 5.81 | ms/batch 69.40 | loss  5.62 | ppl   276.19\n",
      "| epoch  97 |    10/   10 batches | lr 5.81 | ms/batch 51.00 | loss  5.63 | ppl   279.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  97 | time:  0.83s | valid loss  5.80 | valid ppl   330.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  98 |     2/   10 batches | lr 5.81 | ms/batch 98.15 | loss  8.42 | ppl  4545.67\n",
      "| epoch  98 |     4/   10 batches | lr 5.81 | ms/batch 66.15 | loss  5.50 | ppl   244.53\n",
      "| epoch  98 |     6/   10 batches | lr 5.81 | ms/batch 66.12 | loss  5.66 | ppl   288.51\n",
      "| epoch  98 |     8/   10 batches | lr 5.81 | ms/batch 65.19 | loss  5.62 | ppl   275.54\n",
      "| epoch  98 |    10/   10 batches | lr 5.81 | ms/batch 46.64 | loss  5.63 | ppl   279.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  98 | time:  0.77s | valid loss  5.81 | valid ppl   332.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  99 |     2/   10 batches | lr 5.81 | ms/batch 99.63 | loss  8.43 | ppl  4563.66\n",
      "| epoch  99 |     4/   10 batches | lr 5.81 | ms/batch 65.17 | loss  5.50 | ppl   244.57\n",
      "| epoch  99 |     6/   10 batches | lr 5.81 | ms/batch 61.50 | loss  5.66 | ppl   288.43\n",
      "| epoch  99 |     8/   10 batches | lr 5.81 | ms/batch 64.27 | loss  5.62 | ppl   275.36\n",
      "| epoch  99 |    10/   10 batches | lr 5.81 | ms/batch 50.51 | loss  5.64 | ppl   280.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  99 | time:  0.78s | valid loss  5.80 | valid ppl   331.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 100 |     2/   10 batches | lr 5.52 | ms/batch 96.00 | loss  8.42 | ppl  4545.98\n",
      "| epoch 100 |     4/   10 batches | lr 5.52 | ms/batch 65.01 | loss  5.50 | ppl   245.83\n",
      "| epoch 100 |     6/   10 batches | lr 5.52 | ms/batch 66.16 | loss  5.66 | ppl   287.60\n",
      "| epoch 100 |     8/   10 batches | lr 5.52 | ms/batch 65.09 | loss  5.61 | ppl   274.34\n",
      "| epoch 100 |    10/   10 batches | lr 5.52 | ms/batch 47.28 | loss  5.64 | ppl   280.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 100 | time:  0.77s | valid loss  5.80 | valid ppl   330.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 101 |     2/   10 batches | lr 5.52 | ms/batch 103.98 | loss  8.42 | ppl  4555.61\n",
      "| epoch 101 |     4/   10 batches | lr 5.52 | ms/batch 64.75 | loss  5.50 | ppl   244.53\n",
      "| epoch 101 |     6/   10 batches | lr 5.52 | ms/batch 67.63 | loss  5.66 | ppl   286.92\n",
      "| epoch 101 |     8/   10 batches | lr 5.52 | ms/batch 64.11 | loss  5.62 | ppl   274.83\n",
      "| epoch 101 |    10/   10 batches | lr 5.52 | ms/batch 42.13 | loss  5.64 | ppl   280.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 101 | time:  0.78s | valid loss  5.80 | valid ppl   328.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 102 |     2/   10 batches | lr 5.52 | ms/batch 102.07 | loss  8.42 | ppl  4526.14\n",
      "| epoch 102 |     4/   10 batches | lr 5.52 | ms/batch 64.32 | loss  5.50 | ppl   244.94\n",
      "| epoch 102 |     6/   10 batches | lr 5.52 | ms/batch 68.00 | loss  5.66 | ppl   288.18\n",
      "| epoch 102 |     8/   10 batches | lr 5.52 | ms/batch 66.11 | loss  5.62 | ppl   274.62\n",
      "| epoch 102 |    10/   10 batches | lr 5.52 | ms/batch 51.39 | loss  5.63 | ppl   279.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 102 | time:  0.78s | valid loss  5.81 | valid ppl   335.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 103 |     2/   10 batches | lr 5.24 | ms/batch 96.18 | loss  8.41 | ppl  4511.25\n",
      "| epoch 103 |     4/   10 batches | lr 5.24 | ms/batch 66.51 | loss  5.50 | ppl   245.00\n",
      "| epoch 103 |     6/   10 batches | lr 5.24 | ms/batch 68.16 | loss  5.67 | ppl   288.76\n",
      "| epoch 103 |     8/   10 batches | lr 5.24 | ms/batch 65.64 | loss  5.61 | ppl   273.83\n",
      "| epoch 103 |    10/   10 batches | lr 5.24 | ms/batch 43.66 | loss  5.63 | ppl   279.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 103 | time:  0.78s | valid loss  5.81 | valid ppl   333.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 104 |     2/   10 batches | lr 5.24 | ms/batch 98.26 | loss  8.42 | ppl  4526.47\n",
      "| epoch 104 |     4/   10 batches | lr 5.24 | ms/batch 66.67 | loss  5.50 | ppl   244.57\n",
      "| epoch 104 |     6/   10 batches | lr 5.24 | ms/batch 65.76 | loss  5.67 | ppl   290.24\n",
      "| epoch 104 |     8/   10 batches | lr 5.24 | ms/batch 67.65 | loss  5.61 | ppl   273.08\n",
      "| epoch 104 |    10/   10 batches | lr 5.24 | ms/batch 48.99 | loss  5.63 | ppl   278.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 104 | time:  0.79s | valid loss  5.81 | valid ppl   333.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 105 |     2/   10 batches | lr 5.24 | ms/batch 96.38 | loss  8.42 | ppl  4558.94\n",
      "| epoch 105 |     4/   10 batches | lr 5.24 | ms/batch 148.88 | loss  5.50 | ppl   244.96\n",
      "| epoch 105 |     6/   10 batches | lr 5.24 | ms/batch 78.07 | loss  5.67 | ppl   289.88\n",
      "| epoch 105 |     8/   10 batches | lr 5.24 | ms/batch 73.67 | loss  5.61 | ppl   273.14\n",
      "| epoch 105 |    10/   10 batches | lr 5.24 | ms/batch 56.71 | loss  5.63 | ppl   278.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 105 | time:  1.01s | valid loss  5.83 | valid ppl   338.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 106 |     2/   10 batches | lr 4.98 | ms/batch 130.40 | loss  8.42 | ppl  4542.87\n",
      "| epoch 106 |     4/   10 batches | lr 4.98 | ms/batch 122.43 | loss  5.50 | ppl   245.19\n",
      "| epoch 106 |     6/   10 batches | lr 4.98 | ms/batch 73.29 | loss  5.67 | ppl   289.25\n",
      "| epoch 106 |     8/   10 batches | lr 4.98 | ms/batch 68.33 | loss  5.61 | ppl   272.93\n",
      "| epoch 106 |    10/   10 batches | lr 4.98 | ms/batch 52.63 | loss  5.64 | ppl   280.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 106 | time:  0.99s | valid loss  5.82 | valid ppl   335.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 107 |     2/   10 batches | lr 4.98 | ms/batch 97.63 | loss  8.41 | ppl  4503.55\n",
      "| epoch 107 |     4/   10 batches | lr 4.98 | ms/batch 62.41 | loss  5.50 | ppl   244.11\n",
      "| epoch 107 |     6/   10 batches | lr 4.98 | ms/batch 64.27 | loss  5.67 | ppl   290.09\n",
      "| epoch 107 |     8/   10 batches | lr 4.98 | ms/batch 67.91 | loss  5.61 | ppl   272.91\n",
      "| epoch 107 |    10/   10 batches | lr 4.98 | ms/batch 50.50 | loss  5.63 | ppl   278.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 107 | time:  0.78s | valid loss  5.81 | valid ppl   334.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 108 |     2/   10 batches | lr 4.98 | ms/batch 93.65 | loss  8.41 | ppl  4490.68\n",
      "| epoch 108 |     4/   10 batches | lr 4.98 | ms/batch 64.49 | loss  5.50 | ppl   245.02\n",
      "| epoch 108 |     6/   10 batches | lr 4.98 | ms/batch 66.61 | loss  5.66 | ppl   288.25\n",
      "| epoch 108 |     8/   10 batches | lr 4.98 | ms/batch 67.10 | loss  5.61 | ppl   273.83\n",
      "| epoch 108 |    10/   10 batches | lr 4.98 | ms/batch 49.81 | loss  5.63 | ppl   279.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 108 | time:  0.77s | valid loss  5.82 | valid ppl   337.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 109 |     2/   10 batches | lr 4.73 | ms/batch 94.00 | loss  8.42 | ppl  4516.26\n",
      "| epoch 109 |     4/   10 batches | lr 4.73 | ms/batch 59.67 | loss  5.50 | ppl   245.23\n",
      "| epoch 109 |     6/   10 batches | lr 4.73 | ms/batch 68.60 | loss  5.67 | ppl   289.04\n",
      "| epoch 109 |     8/   10 batches | lr 4.73 | ms/batch 65.65 | loss  5.61 | ppl   272.85\n",
      "| epoch 109 |    10/   10 batches | lr 4.73 | ms/batch 49.00 | loss  5.63 | ppl   277.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 109 | time:  0.77s | valid loss  5.82 | valid ppl   338.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 110 |     2/   10 batches | lr 4.73 | ms/batch 98.49 | loss  8.41 | ppl  4508.96\n",
      "| epoch 110 |     4/   10 batches | lr 4.73 | ms/batch 58.66 | loss  5.50 | ppl   244.05\n",
      "| epoch 110 |     6/   10 batches | lr 4.73 | ms/batch 64.92 | loss  5.67 | ppl   289.00\n",
      "| epoch 110 |     8/   10 batches | lr 4.73 | ms/batch 63.51 | loss  5.61 | ppl   273.37\n",
      "| epoch 110 |    10/   10 batches | lr 4.73 | ms/batch 47.99 | loss  5.64 | ppl   280.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 110 | time:  0.76s | valid loss  5.81 | valid ppl   334.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 111 |     2/   10 batches | lr 4.73 | ms/batch 97.71 | loss  8.41 | ppl  4505.43\n",
      "| epoch 111 |     4/   10 batches | lr 4.73 | ms/batch 65.65 | loss  5.50 | ppl   243.68\n",
      "| epoch 111 |     6/   10 batches | lr 4.73 | ms/batch 68.51 | loss  5.66 | ppl   287.76\n",
      "| epoch 111 |     8/   10 batches | lr 4.73 | ms/batch 61.60 | loss  5.61 | ppl   272.41\n",
      "| epoch 111 |    10/   10 batches | lr 4.73 | ms/batch 48.14 | loss  5.64 | ppl   280.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 111 | time:  0.78s | valid loss  5.81 | valid ppl   333.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 112 |     2/   10 batches | lr 4.50 | ms/batch 99.78 | loss  8.41 | ppl  4508.88\n",
      "| epoch 112 |     4/   10 batches | lr 4.50 | ms/batch 65.10 | loss  5.50 | ppl   244.33\n",
      "| epoch 112 |     6/   10 batches | lr 4.50 | ms/batch 64.81 | loss  5.67 | ppl   288.61\n",
      "| epoch 112 |     8/   10 batches | lr 4.50 | ms/batch 61.50 | loss  5.61 | ppl   273.28\n",
      "| epoch 112 |    10/   10 batches | lr 4.50 | ms/batch 50.00 | loss  5.63 | ppl   279.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 112 | time:  0.77s | valid loss  5.82 | valid ppl   335.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 113 |     2/   10 batches | lr 4.50 | ms/batch 103.39 | loss  8.41 | ppl  4498.72\n",
      "| epoch 113 |     4/   10 batches | lr 4.50 | ms/batch 66.48 | loss  5.50 | ppl   245.02\n",
      "| epoch 113 |     6/   10 batches | lr 4.50 | ms/batch 93.73 | loss  5.66 | ppl   287.47\n",
      "| epoch 113 |     8/   10 batches | lr 4.50 | ms/batch 69.00 | loss  5.61 | ppl   272.92\n",
      "| epoch 113 |    10/   10 batches | lr 4.50 | ms/batch 50.00 | loss  5.63 | ppl   279.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 113 | time:  0.85s | valid loss  5.81 | valid ppl   335.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 114 |     2/   10 batches | lr 4.50 | ms/batch 99.33 | loss  8.41 | ppl  4507.12\n",
      "| epoch 114 |     4/   10 batches | lr 4.50 | ms/batch 66.13 | loss  5.50 | ppl   244.90\n",
      "| epoch 114 |     6/   10 batches | lr 4.50 | ms/batch 66.31 | loss  5.66 | ppl   287.83\n",
      "| epoch 114 |     8/   10 batches | lr 4.50 | ms/batch 65.00 | loss  5.61 | ppl   271.91\n",
      "| epoch 114 |    10/   10 batches | lr 4.50 | ms/batch 45.50 | loss  5.63 | ppl   279.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 114 | time:  0.78s | valid loss  5.83 | valid ppl   339.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 115 |     2/   10 batches | lr 4.27 | ms/batch 103.12 | loss  8.41 | ppl  4491.82\n",
      "| epoch 115 |     4/   10 batches | lr 4.27 | ms/batch 66.49 | loss  5.50 | ppl   244.71\n",
      "| epoch 115 |     6/   10 batches | lr 4.27 | ms/batch 64.51 | loss  5.66 | ppl   288.07\n",
      "| epoch 115 |     8/   10 batches | lr 4.27 | ms/batch 66.42 | loss  5.61 | ppl   272.06\n",
      "| epoch 115 |    10/   10 batches | lr 4.27 | ms/batch 48.65 | loss  5.63 | ppl   278.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 115 | time:  0.79s | valid loss  5.82 | valid ppl   336.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 116 |     2/   10 batches | lr 4.27 | ms/batch 97.28 | loss  8.41 | ppl  4513.86\n",
      "| epoch 116 |     4/   10 batches | lr 4.27 | ms/batch 63.51 | loss  5.50 | ppl   244.12\n",
      "| epoch 116 |     6/   10 batches | lr 4.27 | ms/batch 68.67 | loss  5.66 | ppl   288.24\n",
      "| epoch 116 |     8/   10 batches | lr 4.27 | ms/batch 62.00 | loss  5.61 | ppl   272.01\n",
      "| epoch 116 |    10/   10 batches | lr 4.27 | ms/batch 51.15 | loss  5.63 | ppl   278.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 116 | time:  0.78s | valid loss  5.82 | valid ppl   337.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 117 |     2/   10 batches | lr 4.27 | ms/batch 95.38 | loss  8.41 | ppl  4512.99\n",
      "| epoch 117 |     4/   10 batches | lr 4.27 | ms/batch 65.76 | loss  5.50 | ppl   245.76\n",
      "| epoch 117 |     6/   10 batches | lr 4.27 | ms/batch 66.99 | loss  5.66 | ppl   287.17\n",
      "| epoch 117 |     8/   10 batches | lr 4.27 | ms/batch 64.25 | loss  5.61 | ppl   272.77\n",
      "| epoch 117 |    10/   10 batches | lr 4.27 | ms/batch 52.08 | loss  5.63 | ppl   277.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 117 | time:  0.78s | valid loss  5.82 | valid ppl   337.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 118 |     2/   10 batches | lr 4.06 | ms/batch 98.45 | loss  8.41 | ppl  4513.32\n",
      "| epoch 118 |     4/   10 batches | lr 4.06 | ms/batch 66.62 | loss  5.50 | ppl   244.13\n",
      "| epoch 118 |     6/   10 batches | lr 4.06 | ms/batch 63.58 | loss  5.66 | ppl   288.34\n",
      "| epoch 118 |     8/   10 batches | lr 4.06 | ms/batch 66.51 | loss  5.61 | ppl   273.38\n",
      "| epoch 118 |    10/   10 batches | lr 4.06 | ms/batch 51.29 | loss  5.63 | ppl   277.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 118 | time:  0.79s | valid loss  5.83 | valid ppl   338.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 119 |     2/   10 batches | lr 4.06 | ms/batch 100.53 | loss  8.41 | ppl  4498.74\n",
      "| epoch 119 |     4/   10 batches | lr 4.06 | ms/batch 64.53 | loss  5.50 | ppl   243.70\n",
      "| epoch 119 |     6/   10 batches | lr 4.06 | ms/batch 66.31 | loss  5.67 | ppl   289.70\n",
      "| epoch 119 |     8/   10 batches | lr 4.06 | ms/batch 65.62 | loss  5.61 | ppl   271.91\n",
      "| epoch 119 |    10/   10 batches | lr 4.06 | ms/batch 46.28 | loss  5.63 | ppl   277.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 119 | time:  0.79s | valid loss  5.83 | valid ppl   341.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 120 |     2/   10 batches | lr 4.06 | ms/batch 122.13 | loss  8.41 | ppl  4504.79\n",
      "| epoch 120 |     4/   10 batches | lr 4.06 | ms/batch 73.75 | loss  5.50 | ppl   244.16\n",
      "| epoch 120 |     6/   10 batches | lr 4.06 | ms/batch 96.75 | loss  5.67 | ppl   289.49\n",
      "| epoch 120 |     8/   10 batches | lr 4.06 | ms/batch 69.28 | loss  5.61 | ppl   272.25\n",
      "| epoch 120 |    10/   10 batches | lr 4.06 | ms/batch 57.66 | loss  5.62 | ppl   277.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 120 | time:  0.95s | valid loss  5.83 | valid ppl   338.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 121 |     2/   10 batches | lr 3.86 | ms/batch 135.66 | loss  8.41 | ppl  4494.25\n",
      "| epoch 121 |     4/   10 batches | lr 3.86 | ms/batch 76.25 | loss  5.50 | ppl   243.87\n",
      "| epoch 121 |     6/   10 batches | lr 3.86 | ms/batch 72.89 | loss  5.66 | ppl   288.17\n",
      "| epoch 121 |     8/   10 batches | lr 3.86 | ms/batch 69.42 | loss  5.61 | ppl   272.06\n",
      "| epoch 121 |    10/   10 batches | lr 3.86 | ms/batch 51.50 | loss  5.62 | ppl   277.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 121 | time:  0.90s | valid loss  5.82 | valid ppl   337.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 122 |     2/   10 batches | lr 3.86 | ms/batch 101.02 | loss  8.41 | ppl  4491.24\n",
      "| epoch 122 |     4/   10 batches | lr 3.86 | ms/batch 65.67 | loss  5.49 | ppl   243.29\n",
      "| epoch 122 |     6/   10 batches | lr 3.86 | ms/batch 62.01 | loss  5.67 | ppl   290.31\n",
      "| epoch 122 |     8/   10 batches | lr 3.86 | ms/batch 65.70 | loss  5.61 | ppl   272.76\n",
      "| epoch 122 |    10/   10 batches | lr 3.86 | ms/batch 47.98 | loss  5.63 | ppl   277.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 122 | time:  0.77s | valid loss  5.83 | valid ppl   340.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 123 |     2/   10 batches | lr 3.86 | ms/batch 95.95 | loss  8.41 | ppl  4493.35\n",
      "| epoch 123 |     4/   10 batches | lr 3.86 | ms/batch 63.76 | loss  5.49 | ppl   243.10\n",
      "| epoch 123 |     6/   10 batches | lr 3.86 | ms/batch 67.28 | loss  5.67 | ppl   288.75\n",
      "| epoch 123 |     8/   10 batches | lr 3.86 | ms/batch 64.10 | loss  5.61 | ppl   272.75\n",
      "| epoch 123 |    10/   10 batches | lr 3.86 | ms/batch 45.85 | loss  5.62 | ppl   276.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 123 | time:  0.76s | valid loss  5.83 | valid ppl   339.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 124 |     2/   10 batches | lr 3.66 | ms/batch 99.02 | loss  8.41 | ppl  4500.27\n",
      "| epoch 124 |     4/   10 batches | lr 3.66 | ms/batch 65.11 | loss  5.50 | ppl   244.20\n",
      "| epoch 124 |     6/   10 batches | lr 3.66 | ms/batch 66.60 | loss  5.66 | ppl   288.32\n",
      "| epoch 124 |     8/   10 batches | lr 3.66 | ms/batch 62.31 | loss  5.61 | ppl   272.99\n",
      "| epoch 124 |    10/   10 batches | lr 3.66 | ms/batch 48.78 | loss  5.62 | ppl   276.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 124 | time:  0.77s | valid loss  5.83 | valid ppl   338.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 125 |     2/   10 batches | lr 3.66 | ms/batch 95.63 | loss  8.41 | ppl  4477.53\n",
      "| epoch 125 |     4/   10 batches | lr 3.66 | ms/batch 65.67 | loss  5.50 | ppl   244.05\n",
      "| epoch 125 |     6/   10 batches | lr 3.66 | ms/batch 66.15 | loss  5.66 | ppl   288.44\n",
      "| epoch 125 |     8/   10 batches | lr 3.66 | ms/batch 62.29 | loss  5.61 | ppl   272.31\n",
      "| epoch 125 |    10/   10 batches | lr 3.66 | ms/batch 52.00 | loss  5.63 | ppl   277.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 125 | time:  0.77s | valid loss  5.83 | valid ppl   339.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 126 |     2/   10 batches | lr 3.66 | ms/batch 103.67 | loss  8.41 | ppl  4481.56\n",
      "| epoch 126 |     4/   10 batches | lr 3.66 | ms/batch 60.65 | loss  5.49 | ppl   243.45\n",
      "| epoch 126 |     6/   10 batches | lr 3.66 | ms/batch 65.80 | loss  5.66 | ppl   288.18\n",
      "| epoch 126 |     8/   10 batches | lr 3.66 | ms/batch 64.99 | loss  5.61 | ppl   274.12\n",
      "| epoch 126 |    10/   10 batches | lr 3.66 | ms/batch 46.50 | loss  5.62 | ppl   276.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 126 | time:  0.78s | valid loss  5.82 | valid ppl   336.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 127 |     2/   10 batches | lr 3.48 | ms/batch 126.91 | loss  8.41 | ppl  4488.37\n",
      "| epoch 127 |     4/   10 batches | lr 3.48 | ms/batch 63.49 | loss  5.49 | ppl   243.32\n",
      "| epoch 127 |     6/   10 batches | lr 3.48 | ms/batch 69.66 | loss  5.66 | ppl   288.42\n",
      "| epoch 127 |     8/   10 batches | lr 3.48 | ms/batch 66.99 | loss  5.61 | ppl   272.92\n",
      "| epoch 127 |    10/   10 batches | lr 3.48 | ms/batch 43.64 | loss  5.62 | ppl   276.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 127 | time:  0.84s | valid loss  5.82 | valid ppl   337.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 128 |     2/   10 batches | lr 3.48 | ms/batch 99.82 | loss  8.41 | ppl  4489.94\n",
      "| epoch 128 |     4/   10 batches | lr 3.48 | ms/batch 62.19 | loss  5.50 | ppl   244.21\n",
      "| epoch 128 |     6/   10 batches | lr 3.48 | ms/batch 66.63 | loss  5.66 | ppl   287.68\n",
      "| epoch 128 |     8/   10 batches | lr 3.48 | ms/batch 57.21 | loss  5.61 | ppl   273.11\n",
      "| epoch 128 |    10/   10 batches | lr 3.48 | ms/batch 53.64 | loss  5.62 | ppl   276.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 128 | time:  0.77s | valid loss  5.83 | valid ppl   340.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 129 |     2/   10 batches | lr 3.48 | ms/batch 100.98 | loss  8.41 | ppl  4490.92\n",
      "| epoch 129 |     4/   10 batches | lr 3.48 | ms/batch 60.49 | loss  5.49 | ppl   243.34\n",
      "| epoch 129 |     6/   10 batches | lr 3.48 | ms/batch 65.28 | loss  5.66 | ppl   287.80\n",
      "| epoch 129 |     8/   10 batches | lr 3.48 | ms/batch 66.18 | loss  5.61 | ppl   274.11\n",
      "| epoch 129 |    10/   10 batches | lr 3.48 | ms/batch 50.57 | loss  5.62 | ppl   276.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 129 | time:  0.78s | valid loss  5.81 | valid ppl   334.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 130 |     2/   10 batches | lr 3.31 | ms/batch 102.03 | loss  8.40 | ppl  4453.94\n",
      "| epoch 130 |     4/   10 batches | lr 3.31 | ms/batch 63.14 | loss  5.49 | ppl   241.16\n",
      "| epoch 130 |     6/   10 batches | lr 3.31 | ms/batch 65.90 | loss  5.67 | ppl   288.65\n",
      "| epoch 130 |     8/   10 batches | lr 3.31 | ms/batch 67.62 | loss  5.61 | ppl   272.61\n",
      "| epoch 130 |    10/   10 batches | lr 3.31 | ms/batch 47.64 | loss  5.63 | ppl   278.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 130 | time:  0.77s | valid loss  5.82 | valid ppl   337.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 131 |     2/   10 batches | lr 3.31 | ms/batch 97.19 | loss  8.41 | ppl  4479.88\n",
      "| epoch 131 |     4/   10 batches | lr 3.31 | ms/batch 67.36 | loss  5.49 | ppl   243.07\n",
      "| epoch 131 |     6/   10 batches | lr 3.31 | ms/batch 63.30 | loss  5.67 | ppl   289.01\n",
      "| epoch 131 |     8/   10 batches | lr 3.31 | ms/batch 65.51 | loss  5.61 | ppl   272.50\n",
      "| epoch 131 |    10/   10 batches | lr 3.31 | ms/batch 46.49 | loss  5.63 | ppl   278.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 131 | time:  0.77s | valid loss  5.82 | valid ppl   337.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 132 |     2/   10 batches | lr 3.31 | ms/batch 101.71 | loss  8.41 | ppl  4469.64\n",
      "| epoch 132 |     4/   10 batches | lr 3.31 | ms/batch 58.00 | loss  5.49 | ppl   242.63\n",
      "| epoch 132 |     6/   10 batches | lr 3.31 | ms/batch 64.24 | loss  5.66 | ppl   288.20\n",
      "| epoch 132 |     8/   10 batches | lr 3.31 | ms/batch 68.60 | loss  5.61 | ppl   273.23\n",
      "| epoch 132 |    10/   10 batches | lr 3.31 | ms/batch 48.16 | loss  5.63 | ppl   277.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 132 | time:  0.77s | valid loss  5.81 | valid ppl   333.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 133 |     2/   10 batches | lr 3.14 | ms/batch 97.50 | loss  8.40 | ppl  4448.90\n",
      "| epoch 133 |     4/   10 batches | lr 3.14 | ms/batch 58.33 | loss  5.49 | ppl   242.23\n",
      "| epoch 133 |     6/   10 batches | lr 3.14 | ms/batch 93.77 | loss  5.66 | ppl   287.53\n",
      "| epoch 133 |     8/   10 batches | lr 3.14 | ms/batch 69.02 | loss  5.61 | ppl   273.32\n",
      "| epoch 133 |    10/   10 batches | lr 3.14 | ms/batch 47.22 | loss  5.63 | ppl   278.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 133 | time:  0.83s | valid loss  5.83 | valid ppl   338.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 134 |     2/   10 batches | lr 3.14 | ms/batch 97.16 | loss  8.40 | ppl  4468.25\n",
      "| epoch 134 |     4/   10 batches | lr 3.14 | ms/batch 64.16 | loss  5.49 | ppl   242.70\n",
      "| epoch 134 |     6/   10 batches | lr 3.14 | ms/batch 65.17 | loss  5.66 | ppl   288.38\n",
      "| epoch 134 |     8/   10 batches | lr 3.14 | ms/batch 64.51 | loss  5.61 | ppl   272.25\n",
      "| epoch 134 |    10/   10 batches | lr 3.14 | ms/batch 52.99 | loss  5.63 | ppl   278.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 134 | time:  0.78s | valid loss  5.82 | valid ppl   337.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 135 |     2/   10 batches | lr 3.14 | ms/batch 99.66 | loss  8.40 | ppl  4467.94\n",
      "| epoch 135 |     4/   10 batches | lr 3.14 | ms/batch 57.76 | loss  5.49 | ppl   242.19\n",
      "| epoch 135 |     6/   10 batches | lr 3.14 | ms/batch 60.76 | loss  5.66 | ppl   287.85\n",
      "| epoch 135 |     8/   10 batches | lr 3.14 | ms/batch 70.14 | loss  5.61 | ppl   272.87\n",
      "| epoch 135 |    10/   10 batches | lr 3.14 | ms/batch 49.49 | loss  5.63 | ppl   278.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 135 | time:  0.77s | valid loss  5.82 | valid ppl   337.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 136 |     2/   10 batches | lr 2.98 | ms/batch 98.43 | loss  8.40 | ppl  4444.04\n",
      "| epoch 136 |     4/   10 batches | lr 2.98 | ms/batch 63.00 | loss  5.50 | ppl   243.53\n",
      "| epoch 136 |     6/   10 batches | lr 2.98 | ms/batch 60.77 | loss  5.66 | ppl   287.72\n",
      "| epoch 136 |     8/   10 batches | lr 2.98 | ms/batch 67.79 | loss  5.61 | ppl   272.97\n",
      "| epoch 136 |    10/   10 batches | lr 2.98 | ms/batch 49.14 | loss  5.63 | ppl   277.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 136 | time:  0.76s | valid loss  5.82 | valid ppl   337.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 137 |     2/   10 batches | lr 2.98 | ms/batch 97.44 | loss  8.41 | ppl  4482.29\n",
      "| epoch 137 |     4/   10 batches | lr 2.98 | ms/batch 67.99 | loss  5.49 | ppl   242.15\n",
      "| epoch 137 |     6/   10 batches | lr 2.98 | ms/batch 72.45 | loss  5.66 | ppl   287.28\n",
      "| epoch 137 |     8/   10 batches | lr 2.98 | ms/batch 64.71 | loss  5.61 | ppl   272.92\n",
      "| epoch 137 |    10/   10 batches | lr 2.98 | ms/batch 51.14 | loss  5.63 | ppl   277.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 137 | time:  0.79s | valid loss  5.82 | valid ppl   336.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 138 |     2/   10 batches | lr 2.98 | ms/batch 99.50 | loss  8.41 | ppl  4481.11\n",
      "| epoch 138 |     4/   10 batches | lr 2.98 | ms/batch 62.00 | loss  5.49 | ppl   242.74\n",
      "| epoch 138 |     6/   10 batches | lr 2.98 | ms/batch 63.90 | loss  5.66 | ppl   287.88\n",
      "| epoch 138 |     8/   10 batches | lr 2.98 | ms/batch 67.92 | loss  5.61 | ppl   272.30\n",
      "| epoch 138 |    10/   10 batches | lr 2.98 | ms/batch 48.60 | loss  5.62 | ppl   276.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 138 | time:  0.77s | valid loss  5.82 | valid ppl   336.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 139 |     2/   10 batches | lr 2.83 | ms/batch 99.74 | loss  8.41 | ppl  4472.97\n",
      "| epoch 139 |     4/   10 batches | lr 2.83 | ms/batch 64.11 | loss  5.49 | ppl   242.76\n",
      "| epoch 139 |     6/   10 batches | lr 2.83 | ms/batch 94.04 | loss  5.66 | ppl   287.60\n",
      "| epoch 139 |     8/   10 batches | lr 2.83 | ms/batch 64.71 | loss  5.61 | ppl   272.98\n",
      "| epoch 139 |    10/   10 batches | lr 2.83 | ms/batch 48.71 | loss  5.62 | ppl   274.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 139 | time:  0.84s | valid loss  5.83 | valid ppl   338.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 140 |     2/   10 batches | lr 2.83 | ms/batch 101.16 | loss  8.41 | ppl  4474.53\n",
      "| epoch 140 |     4/   10 batches | lr 2.83 | ms/batch 65.29 | loss  5.49 | ppl   242.66\n",
      "| epoch 140 |     6/   10 batches | lr 2.83 | ms/batch 65.73 | loss  5.66 | ppl   288.31\n",
      "| epoch 140 |     8/   10 batches | lr 2.83 | ms/batch 62.93 | loss  5.61 | ppl   272.61\n",
      "| epoch 140 |    10/   10 batches | lr 2.83 | ms/batch 47.99 | loss  5.63 | ppl   277.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 140 | time:  0.78s | valid loss  5.83 | valid ppl   340.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 141 |     2/   10 batches | lr 2.83 | ms/batch 99.78 | loss  8.40 | ppl  4469.16\n",
      "| epoch 141 |     4/   10 batches | lr 2.83 | ms/batch 65.54 | loss  5.49 | ppl   242.66\n",
      "| epoch 141 |     6/   10 batches | lr 2.83 | ms/batch 66.51 | loss  5.66 | ppl   288.14\n",
      "| epoch 141 |     8/   10 batches | lr 2.83 | ms/batch 66.68 | loss  5.61 | ppl   272.49\n",
      "| epoch 141 |    10/   10 batches | lr 2.83 | ms/batch 51.16 | loss  5.62 | ppl   276.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 141 | time:  0.79s | valid loss  5.82 | valid ppl   338.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 142 |     2/   10 batches | lr 2.69 | ms/batch 98.80 | loss  8.41 | ppl  4475.14\n",
      "| epoch 142 |     4/   10 batches | lr 2.69 | ms/batch 63.12 | loss  5.49 | ppl   243.01\n",
      "| epoch 142 |     6/   10 batches | lr 2.69 | ms/batch 67.50 | loss  5.66 | ppl   287.85\n",
      "| epoch 142 |     8/   10 batches | lr 2.69 | ms/batch 65.00 | loss  5.61 | ppl   271.87\n",
      "| epoch 142 |    10/   10 batches | lr 2.69 | ms/batch 53.26 | loss  5.62 | ppl   275.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 142 | time:  0.78s | valid loss  5.83 | valid ppl   339.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 143 |     2/   10 batches | lr 2.69 | ms/batch 98.99 | loss  8.40 | ppl  4459.98\n",
      "| epoch 143 |     4/   10 batches | lr 2.69 | ms/batch 65.83 | loss  5.50 | ppl   243.48\n",
      "| epoch 143 |     6/   10 batches | lr 2.69 | ms/batch 63.76 | loss  5.66 | ppl   288.14\n",
      "| epoch 143 |     8/   10 batches | lr 2.69 | ms/batch 65.66 | loss  5.61 | ppl   272.99\n",
      "| epoch 143 |    10/   10 batches | lr 2.69 | ms/batch 50.10 | loss  5.62 | ppl   276.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 143 | time:  0.78s | valid loss  5.82 | valid ppl   338.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 144 |     2/   10 batches | lr 2.69 | ms/batch 101.27 | loss  8.40 | ppl  4466.35\n",
      "| epoch 144 |     4/   10 batches | lr 2.69 | ms/batch 63.32 | loss  5.49 | ppl   242.50\n",
      "| epoch 144 |     6/   10 batches | lr 2.69 | ms/batch 68.78 | loss  5.67 | ppl   288.67\n",
      "| epoch 144 |     8/   10 batches | lr 2.69 | ms/batch 62.96 | loss  5.61 | ppl   272.89\n",
      "| epoch 144 |    10/   10 batches | lr 2.69 | ms/batch 51.12 | loss  5.62 | ppl   276.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 144 | time:  0.78s | valid loss  5.82 | valid ppl   338.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 145 |     2/   10 batches | lr 2.56 | ms/batch 124.40 | loss  8.40 | ppl  4456.34\n",
      "| epoch 145 |     4/   10 batches | lr 2.56 | ms/batch 68.76 | loss  5.49 | ppl   243.03\n",
      "| epoch 145 |     6/   10 batches | lr 2.56 | ms/batch 68.89 | loss  5.66 | ppl   287.92\n",
      "| epoch 145 |     8/   10 batches | lr 2.56 | ms/batch 57.01 | loss  5.61 | ppl   272.90\n",
      "| epoch 145 |    10/   10 batches | lr 2.56 | ms/batch 47.99 | loss  5.62 | ppl   275.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 145 | time:  0.83s | valid loss  5.82 | valid ppl   337.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 146 |     2/   10 batches | lr 2.56 | ms/batch 98.15 | loss  8.40 | ppl  4468.22\n",
      "| epoch 146 |     4/   10 batches | lr 2.56 | ms/batch 65.50 | loss  5.49 | ppl   242.75\n",
      "| epoch 146 |     6/   10 batches | lr 2.56 | ms/batch 67.02 | loss  5.66 | ppl   287.57\n",
      "| epoch 146 |     8/   10 batches | lr 2.56 | ms/batch 62.79 | loss  5.61 | ppl   273.18\n",
      "| epoch 146 |    10/   10 batches | lr 2.56 | ms/batch 46.73 | loss  5.62 | ppl   275.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 146 | time:  0.77s | valid loss  5.82 | valid ppl   338.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 147 |     2/   10 batches | lr 2.56 | ms/batch 97.50 | loss  8.40 | ppl  4456.35\n",
      "| epoch 147 |     4/   10 batches | lr 2.56 | ms/batch 66.00 | loss  5.49 | ppl   242.89\n",
      "| epoch 147 |     6/   10 batches | lr 2.56 | ms/batch 66.14 | loss  5.66 | ppl   287.61\n",
      "| epoch 147 |     8/   10 batches | lr 2.56 | ms/batch 62.35 | loss  5.61 | ppl   272.66\n",
      "| epoch 147 |    10/   10 batches | lr 2.56 | ms/batch 48.15 | loss  5.62 | ppl   276.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 147 | time:  0.77s | valid loss  5.82 | valid ppl   337.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 148 |     2/   10 batches | lr 2.43 | ms/batch 94.31 | loss  8.40 | ppl  4465.67\n",
      "| epoch 148 |     4/   10 batches | lr 2.43 | ms/batch 60.00 | loss  5.49 | ppl   242.78\n",
      "| epoch 148 |     6/   10 batches | lr 2.43 | ms/batch 68.61 | loss  5.66 | ppl   287.68\n",
      "| epoch 148 |     8/   10 batches | lr 2.43 | ms/batch 65.17 | loss  5.61 | ppl   272.57\n",
      "| epoch 148 |    10/   10 batches | lr 2.43 | ms/batch 49.63 | loss  5.62 | ppl   275.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 148 | time:  0.76s | valid loss  5.83 | valid ppl   339.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 149 |     2/   10 batches | lr 2.43 | ms/batch 91.80 | loss  8.40 | ppl  4455.77\n",
      "| epoch 149 |     4/   10 batches | lr 2.43 | ms/batch 67.16 | loss  5.49 | ppl   243.18\n",
      "| epoch 149 |     6/   10 batches | lr 2.43 | ms/batch 65.37 | loss  5.66 | ppl   288.07\n",
      "| epoch 149 |     8/   10 batches | lr 2.43 | ms/batch 65.65 | loss  5.61 | ppl   273.00\n",
      "| epoch 149 |    10/   10 batches | lr 2.43 | ms/batch 51.65 | loss  5.62 | ppl   275.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 149 | time:  0.77s | valid loss  5.82 | valid ppl   338.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 150 |     2/   10 batches | lr 2.43 | ms/batch 98.19 | loss  8.40 | ppl  4447.90\n",
      "| epoch 150 |     4/   10 batches | lr 2.43 | ms/batch 65.20 | loss  5.49 | ppl   243.31\n",
      "| epoch 150 |     6/   10 batches | lr 2.43 | ms/batch 65.69 | loss  5.66 | ppl   287.58\n",
      "| epoch 150 |     8/   10 batches | lr 2.43 | ms/batch 64.98 | loss  5.61 | ppl   272.59\n",
      "| epoch 150 |    10/   10 batches | lr 2.43 | ms/batch 50.26 | loss  5.62 | ppl   274.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 150 | time:  0.78s | valid loss  5.83 | valid ppl   340.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 151 |     2/   10 batches | lr 2.31 | ms/batch 121.42 | loss  8.40 | ppl  4462.38\n",
      "| epoch 151 |     4/   10 batches | lr 2.31 | ms/batch 68.64 | loss  5.49 | ppl   242.85\n",
      "| epoch 151 |     6/   10 batches | lr 2.31 | ms/batch 63.72 | loss  5.66 | ppl   287.67\n",
      "| epoch 151 |     8/   10 batches | lr 2.31 | ms/batch 66.38 | loss  5.61 | ppl   273.00\n",
      "| epoch 151 |    10/   10 batches | lr 2.31 | ms/batch 51.61 | loss  5.62 | ppl   275.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 151 | time:  0.83s | valid loss  5.83 | valid ppl   338.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 152 |     2/   10 batches | lr 2.31 | ms/batch 98.62 | loss  8.40 | ppl  4447.13\n",
      "| epoch 152 |     4/   10 batches | lr 2.31 | ms/batch 66.51 | loss  5.49 | ppl   243.13\n",
      "| epoch 152 |     6/   10 batches | lr 2.31 | ms/batch 65.28 | loss  5.66 | ppl   287.33\n",
      "| epoch 152 |     8/   10 batches | lr 2.31 | ms/batch 60.83 | loss  5.61 | ppl   272.33\n",
      "| epoch 152 |    10/   10 batches | lr 2.31 | ms/batch 48.39 | loss  5.62 | ppl   275.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 152 | time:  0.77s | valid loss  5.82 | valid ppl   337.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 153 |     2/   10 batches | lr 2.31 | ms/batch 100.06 | loss  8.40 | ppl  4444.76\n",
      "| epoch 153 |     4/   10 batches | lr 2.31 | ms/batch 67.27 | loss  5.49 | ppl   243.40\n",
      "| epoch 153 |     6/   10 batches | lr 2.31 | ms/batch 71.38 | loss  5.66 | ppl   288.26\n",
      "| epoch 153 |     8/   10 batches | lr 2.31 | ms/batch 66.64 | loss  5.61 | ppl   273.03\n",
      "| epoch 153 |    10/   10 batches | lr 2.31 | ms/batch 116.68 | loss  5.62 | ppl   275.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 153 | time:  1.20s | valid loss  5.83 | valid ppl   338.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 154 |     2/   10 batches | lr 2.19 | ms/batch 104.25 | loss  8.40 | ppl  4460.26\n",
      "| epoch 154 |     4/   10 batches | lr 2.19 | ms/batch 81.74 | loss  5.49 | ppl   242.63\n",
      "| epoch 154 |     6/   10 batches | lr 2.19 | ms/batch 71.89 | loss  5.66 | ppl   287.27\n",
      "| epoch 154 |     8/   10 batches | lr 2.19 | ms/batch 65.77 | loss  5.61 | ppl   272.60\n",
      "| epoch 154 |    10/   10 batches | lr 2.19 | ms/batch 51.88 | loss  5.62 | ppl   275.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 154 | time:  0.85s | valid loss  5.83 | valid ppl   338.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 155 |     2/   10 batches | lr 2.19 | ms/batch 102.14 | loss  8.40 | ppl  4455.68\n",
      "| epoch 155 |     4/   10 batches | lr 2.19 | ms/batch 63.53 | loss  5.49 | ppl   242.82\n",
      "| epoch 155 |     6/   10 batches | lr 2.19 | ms/batch 65.24 | loss  5.66 | ppl   287.98\n",
      "| epoch 155 |     8/   10 batches | lr 2.19 | ms/batch 63.88 | loss  5.61 | ppl   273.04\n",
      "| epoch 155 |    10/   10 batches | lr 2.19 | ms/batch 49.20 | loss  5.62 | ppl   275.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 155 | time:  0.78s | valid loss  5.83 | valid ppl   339.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 156 |     2/   10 batches | lr 2.19 | ms/batch 128.10 | loss  8.40 | ppl  4452.67\n",
      "| epoch 156 |     4/   10 batches | lr 2.19 | ms/batch 67.54 | loss  5.49 | ppl   243.21\n",
      "| epoch 156 |     6/   10 batches | lr 2.19 | ms/batch 64.01 | loss  5.66 | ppl   287.81\n",
      "| epoch 156 |     8/   10 batches | lr 2.19 | ms/batch 62.49 | loss  5.61 | ppl   273.22\n",
      "| epoch 156 |    10/   10 batches | lr 2.19 | ms/batch 49.49 | loss  5.62 | ppl   275.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 156 | time:  0.83s | valid loss  5.83 | valid ppl   340.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 157 |     2/   10 batches | lr 2.08 | ms/batch 100.66 | loss  8.40 | ppl  4460.27\n",
      "| epoch 157 |     4/   10 batches | lr 2.08 | ms/batch 64.65 | loss  5.49 | ppl   242.82\n",
      "| epoch 157 |     6/   10 batches | lr 2.08 | ms/batch 64.14 | loss  5.66 | ppl   287.39\n",
      "| epoch 157 |     8/   10 batches | lr 2.08 | ms/batch 65.60 | loss  5.61 | ppl   272.65\n",
      "| epoch 157 |    10/   10 batches | lr 2.08 | ms/batch 50.14 | loss  5.62 | ppl   275.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 157 | time:  0.78s | valid loss  5.83 | valid ppl   339.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 158 |     2/   10 batches | lr 2.08 | ms/batch 99.62 | loss  8.40 | ppl  4435.38\n",
      "| epoch 158 |     4/   10 batches | lr 2.08 | ms/batch 64.50 | loss  5.49 | ppl   243.03\n",
      "| epoch 158 |     6/   10 batches | lr 2.08 | ms/batch 59.13 | loss  5.66 | ppl   287.34\n",
      "| epoch 158 |     8/   10 batches | lr 2.08 | ms/batch 71.61 | loss  5.61 | ppl   272.64\n",
      "| epoch 158 |    10/   10 batches | lr 2.08 | ms/batch 50.18 | loss  5.62 | ppl   275.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 158 | time:  0.78s | valid loss  5.83 | valid ppl   339.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 159 |     2/   10 batches | lr 2.08 | ms/batch 100.25 | loss  8.40 | ppl  4443.64\n",
      "| epoch 159 |     4/   10 batches | lr 2.08 | ms/batch 66.10 | loss  5.49 | ppl   242.53\n",
      "| epoch 159 |     6/   10 batches | lr 2.08 | ms/batch 65.27 | loss  5.66 | ppl   287.42\n",
      "| epoch 159 |     8/   10 batches | lr 2.08 | ms/batch 64.67 | loss  5.61 | ppl   272.97\n",
      "| epoch 159 |    10/   10 batches | lr 2.08 | ms/batch 50.02 | loss  5.62 | ppl   275.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 159 | time:  0.78s | valid loss  5.83 | valid ppl   338.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 160 |     2/   10 batches | lr 1.98 | ms/batch 98.65 | loss  8.40 | ppl  4449.07\n",
      "| epoch 160 |     4/   10 batches | lr 1.98 | ms/batch 65.78 | loss  5.49 | ppl   242.65\n",
      "| epoch 160 |     6/   10 batches | lr 1.98 | ms/batch 63.41 | loss  5.66 | ppl   287.38\n",
      "| epoch 160 |     8/   10 batches | lr 1.98 | ms/batch 67.65 | loss  5.61 | ppl   272.39\n",
      "| epoch 160 |    10/   10 batches | lr 1.98 | ms/batch 49.39 | loss  5.62 | ppl   275.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 160 | time:  0.78s | valid loss  5.83 | valid ppl   340.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 161 |     2/   10 batches | lr 1.98 | ms/batch 97.47 | loss  8.40 | ppl  4450.90\n",
      "| epoch 161 |     4/   10 batches | lr 1.98 | ms/batch 64.22 | loss  5.49 | ppl   242.43\n",
      "| epoch 161 |     6/   10 batches | lr 1.98 | ms/batch 90.00 | loss  5.66 | ppl   287.40\n",
      "| epoch 161 |     8/   10 batches | lr 1.98 | ms/batch 66.00 | loss  5.61 | ppl   272.75\n",
      "| epoch 161 |    10/   10 batches | lr 1.98 | ms/batch 47.73 | loss  5.62 | ppl   275.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 161 | time:  0.81s | valid loss  5.83 | valid ppl   341.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 162 |     2/   10 batches | lr 1.98 | ms/batch 93.51 | loss  8.40 | ppl  4444.24\n",
      "| epoch 162 |     4/   10 batches | lr 1.98 | ms/batch 63.49 | loss  5.49 | ppl   242.36\n",
      "| epoch 162 |     6/   10 batches | lr 1.98 | ms/batch 67.50 | loss  5.66 | ppl   287.54\n",
      "| epoch 162 |     8/   10 batches | lr 1.98 | ms/batch 65.50 | loss  5.61 | ppl   272.82\n",
      "| epoch 162 |    10/   10 batches | lr 1.98 | ms/batch 48.36 | loss  5.62 | ppl   275.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 162 | time:  0.76s | valid loss  5.84 | valid ppl   342.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 163 |     2/   10 batches | lr 1.88 | ms/batch 95.63 | loss  8.40 | ppl  4446.40\n",
      "| epoch 163 |     4/   10 batches | lr 1.88 | ms/batch 62.50 | loss  5.49 | ppl   242.86\n",
      "| epoch 163 |     6/   10 batches | lr 1.88 | ms/batch 64.81 | loss  5.66 | ppl   287.68\n",
      "| epoch 163 |     8/   10 batches | lr 1.88 | ms/batch 67.00 | loss  5.61 | ppl   272.59\n",
      "| epoch 163 |    10/   10 batches | lr 1.88 | ms/batch 47.68 | loss  5.61 | ppl   274.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 163 | time:  0.76s | valid loss  5.83 | valid ppl   339.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 164 |     2/   10 batches | lr 1.88 | ms/batch 97.62 | loss  8.40 | ppl  4435.79\n",
      "| epoch 164 |     4/   10 batches | lr 1.88 | ms/batch 62.76 | loss  5.49 | ppl   242.99\n",
      "| epoch 164 |     6/   10 batches | lr 1.88 | ms/batch 68.00 | loss  5.66 | ppl   287.18\n",
      "| epoch 164 |     8/   10 batches | lr 1.88 | ms/batch 65.51 | loss  5.61 | ppl   273.01\n",
      "| epoch 164 |    10/   10 batches | lr 1.88 | ms/batch 52.49 | loss  5.62 | ppl   275.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 164 | time:  0.78s | valid loss  5.82 | valid ppl   336.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 165 |     2/   10 batches | lr 1.88 | ms/batch 99.22 | loss  8.40 | ppl  4435.15\n",
      "| epoch 165 |     4/   10 batches | lr 1.88 | ms/batch 63.63 | loss  5.49 | ppl   242.26\n",
      "| epoch 165 |     6/   10 batches | lr 1.88 | ms/batch 66.81 | loss  5.66 | ppl   286.79\n",
      "| epoch 165 |     8/   10 batches | lr 1.88 | ms/batch 68.13 | loss  5.61 | ppl   272.59\n",
      "| epoch 165 |    10/   10 batches | lr 1.88 | ms/batch 47.94 | loss  5.62 | ppl   277.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 165 | time:  0.78s | valid loss  5.82 | valid ppl   338.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 166 |     2/   10 batches | lr 1.79 | ms/batch 97.16 | loss  8.40 | ppl  4439.54\n",
      "| epoch 166 |     4/   10 batches | lr 1.79 | ms/batch 62.10 | loss  5.49 | ppl   242.39\n",
      "| epoch 166 |     6/   10 batches | lr 1.79 | ms/batch 95.78 | loss  5.66 | ppl   286.68\n",
      "| epoch 166 |     8/   10 batches | lr 1.79 | ms/batch 65.65 | loss  5.61 | ppl   273.11\n",
      "| epoch 166 |    10/   10 batches | lr 1.79 | ms/batch 51.12 | loss  5.62 | ppl   276.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 166 | time:  0.84s | valid loss  5.83 | valid ppl   339.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 167 |     2/   10 batches | lr 1.79 | ms/batch 102.61 | loss  8.40 | ppl  4429.74\n",
      "| epoch 167 |     4/   10 batches | lr 1.79 | ms/batch 61.82 | loss  5.49 | ppl   242.45\n",
      "| epoch 167 |     6/   10 batches | lr 1.79 | ms/batch 63.65 | loss  5.66 | ppl   286.90\n",
      "| epoch 167 |     8/   10 batches | lr 1.79 | ms/batch 66.00 | loss  5.61 | ppl   272.36\n",
      "| epoch 167 |    10/   10 batches | lr 1.79 | ms/batch 50.64 | loss  5.62 | ppl   276.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 167 | time:  0.78s | valid loss  5.83 | valid ppl   340.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 168 |     2/   10 batches | lr 1.79 | ms/batch 92.01 | loss  8.40 | ppl  4446.85\n",
      "| epoch 168 |     4/   10 batches | lr 1.79 | ms/batch 68.11 | loss  5.49 | ppl   242.65\n",
      "| epoch 168 |     6/   10 batches | lr 1.79 | ms/batch 67.03 | loss  5.66 | ppl   286.26\n",
      "| epoch 168 |     8/   10 batches | lr 1.79 | ms/batch 63.00 | loss  5.61 | ppl   272.33\n",
      "| epoch 168 |    10/   10 batches | lr 1.79 | ms/batch 48.52 | loss  5.62 | ppl   276.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 168 | time:  0.77s | valid loss  5.83 | valid ppl   341.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 169 |     2/   10 batches | lr 1.70 | ms/batch 97.00 | loss  8.40 | ppl  4451.22\n",
      "| epoch 169 |     4/   10 batches | lr 1.70 | ms/batch 62.00 | loss  5.49 | ppl   242.55\n",
      "| epoch 169 |     6/   10 batches | lr 1.70 | ms/batch 67.13 | loss  5.66 | ppl   286.42\n",
      "| epoch 169 |     8/   10 batches | lr 1.70 | ms/batch 65.00 | loss  5.61 | ppl   271.84\n",
      "| epoch 169 |    10/   10 batches | lr 1.70 | ms/batch 49.08 | loss  5.62 | ppl   275.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 169 | time:  0.77s | valid loss  5.83 | valid ppl   340.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 170 |     2/   10 batches | lr 1.70 | ms/batch 99.04 | loss  8.40 | ppl  4439.06\n",
      "| epoch 170 |     4/   10 batches | lr 1.70 | ms/batch 63.99 | loss  5.49 | ppl   242.33\n",
      "| epoch 170 |     6/   10 batches | lr 1.70 | ms/batch 69.01 | loss  5.66 | ppl   286.74\n",
      "| epoch 170 |     8/   10 batches | lr 1.70 | ms/batch 64.78 | loss  5.61 | ppl   272.41\n",
      "| epoch 170 |    10/   10 batches | lr 1.70 | ms/batch 52.09 | loss  5.62 | ppl   275.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 170 | time:  0.79s | valid loss  5.83 | valid ppl   340.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 171 |     2/   10 batches | lr 1.70 | ms/batch 91.81 | loss  8.40 | ppl  4445.67\n",
      "| epoch 171 |     4/   10 batches | lr 1.70 | ms/batch 65.15 | loss  5.49 | ppl   242.46\n",
      "| epoch 171 |     6/   10 batches | lr 1.70 | ms/batch 97.65 | loss  5.66 | ppl   286.58\n",
      "| epoch 171 |     8/   10 batches | lr 1.70 | ms/batch 67.49 | loss  5.61 | ppl   272.57\n",
      "| epoch 171 |    10/   10 batches | lr 1.70 | ms/batch 47.54 | loss  5.62 | ppl   274.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 171 | time:  0.83s | valid loss  5.83 | valid ppl   339.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 172 |     2/   10 batches | lr 1.61 | ms/batch 100.41 | loss  8.40 | ppl  4434.45\n",
      "| epoch 172 |     4/   10 batches | lr 1.61 | ms/batch 62.00 | loss  5.49 | ppl   242.30\n",
      "| epoch 172 |     6/   10 batches | lr 1.61 | ms/batch 67.22 | loss  5.66 | ppl   286.65\n",
      "| epoch 172 |     8/   10 batches | lr 1.61 | ms/batch 65.15 | loss  5.61 | ppl   272.39\n",
      "| epoch 172 |    10/   10 batches | lr 1.61 | ms/batch 43.60 | loss  5.62 | ppl   275.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 172 | time:  0.77s | valid loss  5.83 | valid ppl   341.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 173 |     2/   10 batches | lr 1.61 | ms/batch 100.76 | loss  8.40 | ppl  4440.61\n",
      "| epoch 173 |     4/   10 batches | lr 1.61 | ms/batch 67.60 | loss  5.49 | ppl   242.58\n",
      "| epoch 173 |     6/   10 batches | lr 1.61 | ms/batch 66.01 | loss  5.66 | ppl   287.05\n",
      "| epoch 173 |     8/   10 batches | lr 1.61 | ms/batch 64.37 | loss  5.61 | ppl   272.37\n",
      "| epoch 173 |    10/   10 batches | lr 1.61 | ms/batch 43.49 | loss  5.62 | ppl   274.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 173 | time:  0.78s | valid loss  5.83 | valid ppl   341.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 174 |     2/   10 batches | lr 1.61 | ms/batch 103.10 | loss  8.40 | ppl  4441.30\n",
      "| epoch 174 |     4/   10 batches | lr 1.61 | ms/batch 64.76 | loss  5.49 | ppl   242.95\n",
      "| epoch 174 |     6/   10 batches | lr 1.61 | ms/batch 67.53 | loss  5.66 | ppl   286.66\n",
      "| epoch 174 |     8/   10 batches | lr 1.61 | ms/batch 63.61 | loss  5.61 | ppl   271.92\n",
      "| epoch 174 |    10/   10 batches | lr 1.61 | ms/batch 48.50 | loss  5.62 | ppl   275.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 174 | time:  0.79s | valid loss  5.83 | valid ppl   341.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 175 |     2/   10 batches | lr 1.53 | ms/batch 106.89 | loss  8.40 | ppl  4452.78\n",
      "| epoch 175 |     4/   10 batches | lr 1.53 | ms/batch 65.73 | loss  5.49 | ppl   242.64\n",
      "| epoch 175 |     6/   10 batches | lr 1.53 | ms/batch 69.50 | loss  5.66 | ppl   287.27\n",
      "| epoch 175 |     8/   10 batches | lr 1.53 | ms/batch 68.00 | loss  5.61 | ppl   273.19\n",
      "| epoch 175 |    10/   10 batches | lr 1.53 | ms/batch 45.00 | loss  5.62 | ppl   275.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 175 | time:  0.80s | valid loss  5.83 | valid ppl   340.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 176 |     2/   10 batches | lr 1.53 | ms/batch 130.56 | loss  8.40 | ppl  4434.41\n",
      "| epoch 176 |     4/   10 batches | lr 1.53 | ms/batch 68.64 | loss  5.49 | ppl   242.98\n",
      "| epoch 176 |     6/   10 batches | lr 1.53 | ms/batch 63.34 | loss  5.66 | ppl   286.01\n",
      "| epoch 176 |     8/   10 batches | lr 1.53 | ms/batch 67.31 | loss  5.61 | ppl   272.32\n",
      "| epoch 176 |    10/   10 batches | lr 1.53 | ms/batch 46.57 | loss  5.62 | ppl   274.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 176 | time:  0.84s | valid loss  5.84 | valid ppl   342.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 177 |     2/   10 batches | lr 1.53 | ms/batch 99.29 | loss  8.40 | ppl  4447.87\n",
      "| epoch 177 |     4/   10 batches | lr 1.53 | ms/batch 61.12 | loss  5.49 | ppl   242.61\n",
      "| epoch 177 |     6/   10 batches | lr 1.53 | ms/batch 66.79 | loss  5.66 | ppl   287.00\n",
      "| epoch 177 |     8/   10 batches | lr 1.53 | ms/batch 67.67 | loss  5.61 | ppl   273.06\n",
      "| epoch 177 |    10/   10 batches | lr 1.53 | ms/batch 45.89 | loss  5.62 | ppl   274.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 177 | time:  0.77s | valid loss  5.83 | valid ppl   339.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 178 |     2/   10 batches | lr 1.45 | ms/batch 96.27 | loss  8.40 | ppl  4441.51\n",
      "| epoch 178 |     4/   10 batches | lr 1.45 | ms/batch 66.17 | loss  5.49 | ppl   242.02\n",
      "| epoch 178 |     6/   10 batches | lr 1.45 | ms/batch 67.61 | loss  5.66 | ppl   287.17\n",
      "| epoch 178 |     8/   10 batches | lr 1.45 | ms/batch 63.29 | loss  5.61 | ppl   272.63\n",
      "| epoch 178 |    10/   10 batches | lr 1.45 | ms/batch 46.13 | loss  5.62 | ppl   274.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 178 | time:  0.77s | valid loss  5.83 | valid ppl   340.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 179 |     2/   10 batches | lr 1.45 | ms/batch 100.91 | loss  8.40 | ppl  4439.69\n",
      "| epoch 179 |     4/   10 batches | lr 1.45 | ms/batch 65.32 | loss  5.49 | ppl   242.16\n",
      "| epoch 179 |     6/   10 batches | lr 1.45 | ms/batch 65.79 | loss  5.66 | ppl   286.41\n",
      "| epoch 179 |     8/   10 batches | lr 1.45 | ms/batch 66.93 | loss  5.61 | ppl   272.68\n",
      "| epoch 179 |    10/   10 batches | lr 1.45 | ms/batch 51.00 | loss  5.62 | ppl   275.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 179 | time:  0.79s | valid loss  5.83 | valid ppl   341.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 180 |     2/   10 batches | lr 1.45 | ms/batch 98.84 | loss  8.40 | ppl  4439.23\n",
      "| epoch 180 |     4/   10 batches | lr 1.45 | ms/batch 62.38 | loss  5.49 | ppl   242.94\n",
      "| epoch 180 |     6/   10 batches | lr 1.45 | ms/batch 64.49 | loss  5.66 | ppl   286.23\n",
      "| epoch 180 |     8/   10 batches | lr 1.45 | ms/batch 64.78 | loss  5.61 | ppl   272.90\n",
      "| epoch 180 |    10/   10 batches | lr 1.45 | ms/batch 48.11 | loss  5.62 | ppl   275.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 180 | time:  0.83s | valid loss  5.84 | valid ppl   342.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 181 |     2/   10 batches | lr 1.38 | ms/batch 101.48 | loss  8.40 | ppl  4430.92\n",
      "| epoch 181 |     4/   10 batches | lr 1.38 | ms/batch 62.28 | loss  5.49 | ppl   242.92\n",
      "| epoch 181 |     6/   10 batches | lr 1.38 | ms/batch 67.66 | loss  5.66 | ppl   287.33\n",
      "| epoch 181 |     8/   10 batches | lr 1.38 | ms/batch 66.74 | loss  5.61 | ppl   272.72\n",
      "| epoch 181 |    10/   10 batches | lr 1.38 | ms/batch 47.46 | loss  5.62 | ppl   275.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 181 | time:  0.78s | valid loss  5.83 | valid ppl   341.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 182 |     2/   10 batches | lr 1.38 | ms/batch 97.70 | loss  8.40 | ppl  4446.80\n",
      "| epoch 182 |     4/   10 batches | lr 1.38 | ms/batch 65.14 | loss  5.49 | ppl   242.39\n",
      "| epoch 182 |     6/   10 batches | lr 1.38 | ms/batch 68.11 | loss  5.66 | ppl   286.73\n",
      "| epoch 182 |     8/   10 batches | lr 1.38 | ms/batch 66.49 | loss  5.61 | ppl   272.69\n",
      "| epoch 182 |    10/   10 batches | lr 1.38 | ms/batch 43.00 | loss  5.62 | ppl   275.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 182 | time:  0.78s | valid loss  5.83 | valid ppl   341.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 183 |     2/   10 batches | lr 1.38 | ms/batch 98.63 | loss  8.40 | ppl  4442.51\n",
      "| epoch 183 |     4/   10 batches | lr 1.38 | ms/batch 62.64 | loss  5.49 | ppl   242.29\n",
      "| epoch 183 |     6/   10 batches | lr 1.38 | ms/batch 67.37 | loss  5.66 | ppl   287.23\n",
      "| epoch 183 |     8/   10 batches | lr 1.38 | ms/batch 62.50 | loss  5.61 | ppl   272.66\n",
      "| epoch 183 |    10/   10 batches | lr 1.38 | ms/batch 50.50 | loss  5.62 | ppl   274.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 183 | time:  0.77s | valid loss  5.83 | valid ppl   339.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 184 |     2/   10 batches | lr 1.31 | ms/batch 99.28 | loss  8.40 | ppl  4439.03\n",
      "| epoch 184 |     4/   10 batches | lr 1.31 | ms/batch 65.99 | loss  5.49 | ppl   241.98\n",
      "| epoch 184 |     6/   10 batches | lr 1.31 | ms/batch 70.00 | loss  5.66 | ppl   286.83\n",
      "| epoch 184 |     8/   10 batches | lr 1.31 | ms/batch 63.51 | loss  5.61 | ppl   272.59\n",
      "| epoch 184 |    10/   10 batches | lr 1.31 | ms/batch 45.02 | loss  5.61 | ppl   274.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 184 | time:  0.77s | valid loss  5.83 | valid ppl   340.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 185 |     2/   10 batches | lr 1.31 | ms/batch 99.11 | loss  8.40 | ppl  4436.06\n",
      "| epoch 185 |     4/   10 batches | lr 1.31 | ms/batch 62.50 | loss  5.49 | ppl   242.41\n",
      "| epoch 185 |     6/   10 batches | lr 1.31 | ms/batch 99.25 | loss  5.66 | ppl   286.37\n",
      "| epoch 185 |     8/   10 batches | lr 1.31 | ms/batch 69.51 | loss  5.61 | ppl   272.88\n",
      "| epoch 185 |    10/   10 batches | lr 1.31 | ms/batch 52.12 | loss  5.61 | ppl   273.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 185 | time:  0.85s | valid loss  5.83 | valid ppl   340.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 186 |     2/   10 batches | lr 1.31 | ms/batch 97.50 | loss  8.40 | ppl  4433.09\n",
      "| epoch 186 |     4/   10 batches | lr 1.31 | ms/batch 62.00 | loss  5.49 | ppl   243.05\n",
      "| epoch 186 |     6/   10 batches | lr 1.31 | ms/batch 65.64 | loss  5.66 | ppl   286.58\n",
      "| epoch 186 |     8/   10 batches | lr 1.31 | ms/batch 69.17 | loss  5.61 | ppl   272.27\n",
      "| epoch 186 |    10/   10 batches | lr 1.31 | ms/batch 46.64 | loss  5.61 | ppl   274.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 186 | time:  0.77s | valid loss  5.83 | valid ppl   340.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 187 |     2/   10 batches | lr 1.25 | ms/batch 99.34 | loss  8.40 | ppl  4429.25\n",
      "| epoch 187 |     4/   10 batches | lr 1.25 | ms/batch 62.40 | loss  5.49 | ppl   242.24\n",
      "| epoch 187 |     6/   10 batches | lr 1.25 | ms/batch 66.95 | loss  5.66 | ppl   286.73\n",
      "| epoch 187 |     8/   10 batches | lr 1.25 | ms/batch 61.17 | loss  5.61 | ppl   272.59\n",
      "| epoch 187 |    10/   10 batches | lr 1.25 | ms/batch 47.51 | loss  5.61 | ppl   274.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 187 | time:  0.77s | valid loss  5.83 | valid ppl   341.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 188 |     2/   10 batches | lr 1.25 | ms/batch 96.63 | loss  8.40 | ppl  4439.65\n",
      "| epoch 188 |     4/   10 batches | lr 1.25 | ms/batch 67.69 | loss  5.49 | ppl   242.65\n",
      "| epoch 188 |     6/   10 batches | lr 1.25 | ms/batch 64.48 | loss  5.66 | ppl   286.66\n",
      "| epoch 188 |     8/   10 batches | lr 1.25 | ms/batch 64.00 | loss  5.61 | ppl   272.90\n",
      "| epoch 188 |    10/   10 batches | lr 1.25 | ms/batch 49.62 | loss  5.62 | ppl   274.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 188 | time:  0.77s | valid loss  5.83 | valid ppl   341.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 189 |     2/   10 batches | lr 1.25 | ms/batch 99.64 | loss  8.40 | ppl  4430.63\n",
      "| epoch 189 |     4/   10 batches | lr 1.25 | ms/batch 65.57 | loss  5.49 | ppl   242.09\n",
      "| epoch 189 |     6/   10 batches | lr 1.25 | ms/batch 62.50 | loss  5.66 | ppl   286.81\n",
      "| epoch 189 |     8/   10 batches | lr 1.25 | ms/batch 69.24 | loss  5.61 | ppl   272.89\n",
      "| epoch 189 |    10/   10 batches | lr 1.25 | ms/batch 51.50 | loss  5.61 | ppl   274.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 189 | time:  0.79s | valid loss  5.83 | valid ppl   341.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 190 |     2/   10 batches | lr 1.18 | ms/batch 127.85 | loss  8.40 | ppl  4433.02\n",
      "| epoch 190 |     4/   10 batches | lr 1.18 | ms/batch 68.30 | loss  5.49 | ppl   242.21\n",
      "| epoch 190 |     6/   10 batches | lr 1.18 | ms/batch 73.97 | loss  5.66 | ppl   286.52\n",
      "| epoch 190 |     8/   10 batches | lr 1.18 | ms/batch 68.41 | loss  5.61 | ppl   272.34\n",
      "| epoch 190 |    10/   10 batches | lr 1.18 | ms/batch 58.28 | loss  5.62 | ppl   275.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 190 | time:  0.88s | valid loss  5.83 | valid ppl   340.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 191 |     2/   10 batches | lr 1.18 | ms/batch 110.60 | loss  8.40 | ppl  4428.47\n",
      "| epoch 191 |     4/   10 batches | lr 1.18 | ms/batch 75.18 | loss  5.49 | ppl   242.57\n",
      "| epoch 191 |     6/   10 batches | lr 1.18 | ms/batch 83.32 | loss  5.66 | ppl   287.20\n",
      "| epoch 191 |     8/   10 batches | lr 1.18 | ms/batch 82.55 | loss  5.61 | ppl   272.72\n",
      "| epoch 191 |    10/   10 batches | lr 1.18 | ms/batch 63.77 | loss  5.62 | ppl   275.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 191 | time:  0.95s | valid loss  5.83 | valid ppl   341.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 192 |     2/   10 batches | lr 1.18 | ms/batch 125.48 | loss  8.40 | ppl  4436.23\n",
      "| epoch 192 |     4/   10 batches | lr 1.18 | ms/batch 67.67 | loss  5.49 | ppl   242.28\n",
      "| epoch 192 |     6/   10 batches | lr 1.18 | ms/batch 73.37 | loss  5.66 | ppl   286.63\n",
      "| epoch 192 |     8/   10 batches | lr 1.18 | ms/batch 71.40 | loss  5.61 | ppl   272.34\n",
      "| epoch 192 |    10/   10 batches | lr 1.18 | ms/batch 54.50 | loss  5.62 | ppl   275.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 192 | time:  0.88s | valid loss  5.83 | valid ppl   341.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 193 |     2/   10 batches | lr 1.13 | ms/batch 106.05 | loss  8.40 | ppl  4429.02\n",
      "| epoch 193 |     4/   10 batches | lr 1.13 | ms/batch 73.34 | loss  5.49 | ppl   242.15\n",
      "| epoch 193 |     6/   10 batches | lr 1.13 | ms/batch 81.72 | loss  5.66 | ppl   286.71\n",
      "| epoch 193 |     8/   10 batches | lr 1.13 | ms/batch 80.25 | loss  5.61 | ppl   272.76\n",
      "| epoch 193 |    10/   10 batches | lr 1.13 | ms/batch 63.64 | loss  5.61 | ppl   274.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 193 | time:  0.91s | valid loss  5.84 | valid ppl   343.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 194 |     2/   10 batches | lr 1.13 | ms/batch 108.19 | loss  8.40 | ppl  4433.42\n",
      "| epoch 194 |     4/   10 batches | lr 1.13 | ms/batch 79.86 | loss  5.49 | ppl   242.57\n",
      "| epoch 194 |     6/   10 batches | lr 1.13 | ms/batch 113.14 | loss  5.66 | ppl   286.74\n",
      "| epoch 194 |     8/   10 batches | lr 1.13 | ms/batch 79.42 | loss  5.61 | ppl   272.58\n",
      "| epoch 194 |    10/   10 batches | lr 1.13 | ms/batch 63.78 | loss  5.62 | ppl   274.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 194 | time:  1.00s | valid loss  5.84 | valid ppl   342.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 195 |     2/   10 batches | lr 1.13 | ms/batch 125.99 | loss  8.40 | ppl  4430.73\n",
      "| epoch 195 |     4/   10 batches | lr 1.13 | ms/batch 74.55 | loss  5.49 | ppl   242.72\n",
      "| epoch 195 |     6/   10 batches | lr 1.13 | ms/batch 352.28 | loss  5.66 | ppl   286.59\n",
      "| epoch 195 |     8/   10 batches | lr 1.13 | ms/batch 93.20 | loss  5.61 | ppl   272.65\n",
      "| epoch 195 |    10/   10 batches | lr 1.13 | ms/batch 60.44 | loss  5.62 | ppl   275.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 195 | time:  1.55s | valid loss  5.83 | valid ppl   341.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 196 |     2/   10 batches | lr 1.07 | ms/batch 119.55 | loss  8.40 | ppl  4430.26\n",
      "| epoch 196 |     4/   10 batches | lr 1.07 | ms/batch 69.17 | loss  5.49 | ppl   242.06\n",
      "| epoch 196 |     6/   10 batches | lr 1.07 | ms/batch 68.46 | loss  5.66 | ppl   286.56\n",
      "| epoch 196 |     8/   10 batches | lr 1.07 | ms/batch 68.25 | loss  5.61 | ppl   272.63\n",
      "| epoch 196 |    10/   10 batches | lr 1.07 | ms/batch 55.47 | loss  5.62 | ppl   275.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 196 | time:  0.86s | valid loss  5.83 | valid ppl   342.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 197 |     2/   10 batches | lr 1.07 | ms/batch 102.00 | loss  8.39 | ppl  4422.54\n",
      "| epoch 197 |     4/   10 batches | lr 1.07 | ms/batch 67.02 | loss  5.49 | ppl   242.36\n",
      "| epoch 197 |     6/   10 batches | lr 1.07 | ms/batch 71.77 | loss  5.66 | ppl   286.71\n",
      "| epoch 197 |     8/   10 batches | lr 1.07 | ms/batch 68.81 | loss  5.61 | ppl   271.99\n",
      "| epoch 197 |    10/   10 batches | lr 1.07 | ms/batch 49.67 | loss  5.62 | ppl   274.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 197 | time:  0.81s | valid loss  5.84 | valid ppl   342.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 198 |     2/   10 batches | lr 1.07 | ms/batch 105.40 | loss  8.40 | ppl  4425.41\n",
      "| epoch 198 |     4/   10 batches | lr 1.07 | ms/batch 67.59 | loss  5.49 | ppl   242.23\n",
      "| epoch 198 |     6/   10 batches | lr 1.07 | ms/batch 66.79 | loss  5.66 | ppl   286.45\n",
      "| epoch 198 |     8/   10 batches | lr 1.07 | ms/batch 67.12 | loss  5.61 | ppl   272.32\n",
      "| epoch 198 |    10/   10 batches | lr 1.07 | ms/batch 76.62 | loss  5.62 | ppl   275.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 198 | time:  0.88s | valid loss  5.84 | valid ppl   343.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 199 |     2/   10 batches | lr 1.02 | ms/batch 108.12 | loss  8.39 | ppl  4422.16\n",
      "| epoch 199 |     4/   10 batches | lr 1.02 | ms/batch 69.13 | loss  5.49 | ppl   241.91\n",
      "| epoch 199 |     6/   10 batches | lr 1.02 | ms/batch 66.49 | loss  5.66 | ppl   286.61\n",
      "| epoch 199 |     8/   10 batches | lr 1.02 | ms/batch 69.14 | loss  5.61 | ppl   272.47\n",
      "| epoch 199 |    10/   10 batches | lr 1.02 | ms/batch 52.65 | loss  5.62 | ppl   275.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 199 | time:  0.82s | valid loss  5.84 | valid ppl   343.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 200 |     2/   10 batches | lr 1.02 | ms/batch 108.11 | loss  8.39 | ppl  4419.26\n",
      "| epoch 200 |     4/   10 batches | lr 1.02 | ms/batch 68.65 | loss  5.49 | ppl   242.11\n",
      "| epoch 200 |     6/   10 batches | lr 1.02 | ms/batch 69.80 | loss  5.66 | ppl   286.19\n",
      "| epoch 200 |     8/   10 batches | lr 1.02 | ms/batch 68.48 | loss  5.61 | ppl   272.47\n",
      "| epoch 200 |    10/   10 batches | lr 1.02 | ms/batch 54.41 | loss  5.62 | ppl   275.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 200 | time:  0.83s | valid loss  5.84 | valid ppl   344.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 201 |     2/   10 batches | lr 1.02 | ms/batch 99.85 | loss  8.40 | ppl  4427.70\n",
      "| epoch 201 |     4/   10 batches | lr 1.02 | ms/batch 65.48 | loss  5.49 | ppl   242.18\n",
      "| epoch 201 |     6/   10 batches | lr 1.02 | ms/batch 64.50 | loss  5.66 | ppl   286.62\n",
      "| epoch 201 |     8/   10 batches | lr 1.02 | ms/batch 68.56 | loss  5.61 | ppl   272.51\n",
      "| epoch 201 |    10/   10 batches | lr 1.02 | ms/batch 48.00 | loss  5.61 | ppl   274.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 201 | time:  0.78s | valid loss  5.84 | valid ppl   344.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 202 |     2/   10 batches | lr 0.97 | ms/batch 98.67 | loss  8.40 | ppl  4431.18\n",
      "| epoch 202 |     4/   10 batches | lr 0.97 | ms/batch 65.28 | loss  5.49 | ppl   242.02\n",
      "| epoch 202 |     6/   10 batches | lr 0.97 | ms/batch 63.88 | loss  5.66 | ppl   286.67\n",
      "| epoch 202 |     8/   10 batches | lr 0.97 | ms/batch 64.51 | loss  5.61 | ppl   272.88\n",
      "| epoch 202 |    10/   10 batches | lr 0.97 | ms/batch 44.65 | loss  5.62 | ppl   274.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 202 | time:  0.77s | valid loss  5.84 | valid ppl   343.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 203 |     2/   10 batches | lr 0.97 | ms/batch 124.31 | loss  8.39 | ppl  4420.42\n",
      "| epoch 203 |     4/   10 batches | lr 0.97 | ms/batch 66.38 | loss  5.49 | ppl   242.05\n",
      "| epoch 203 |     6/   10 batches | lr 0.97 | ms/batch 65.81 | loss  5.66 | ppl   286.75\n",
      "| epoch 203 |     8/   10 batches | lr 0.97 | ms/batch 67.49 | loss  5.61 | ppl   272.77\n",
      "| epoch 203 |    10/   10 batches | lr 0.97 | ms/batch 50.00 | loss  5.62 | ppl   274.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 203 | time:  0.84s | valid loss  5.84 | valid ppl   342.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 204 |     2/   10 batches | lr 0.97 | ms/batch 99.88 | loss  8.39 | ppl  4416.97\n",
      "| epoch 204 |     4/   10 batches | lr 0.97 | ms/batch 66.16 | loss  5.49 | ppl   242.11\n",
      "| epoch 204 |     6/   10 batches | lr 0.97 | ms/batch 66.69 | loss  5.66 | ppl   286.57\n",
      "| epoch 204 |     8/   10 batches | lr 0.97 | ms/batch 65.25 | loss  5.61 | ppl   272.39\n",
      "| epoch 204 |    10/   10 batches | lr 0.97 | ms/batch 45.53 | loss  5.61 | ppl   274.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 204 | time:  0.78s | valid loss  5.84 | valid ppl   342.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 205 |     2/   10 batches | lr 0.92 | ms/batch 99.94 | loss  8.40 | ppl  4425.21\n",
      "| epoch 205 |     4/   10 batches | lr 0.92 | ms/batch 63.64 | loss  5.49 | ppl   242.50\n",
      "| epoch 205 |     6/   10 batches | lr 0.92 | ms/batch 62.85 | loss  5.66 | ppl   287.00\n",
      "| epoch 205 |     8/   10 batches | lr 0.92 | ms/batch 65.84 | loss  5.61 | ppl   272.05\n",
      "| epoch 205 |    10/   10 batches | lr 0.92 | ms/batch 49.60 | loss  5.62 | ppl   275.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 205 | time:  0.78s | valid loss  5.83 | valid ppl   341.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 206 |     2/   10 batches | lr 0.92 | ms/batch 100.58 | loss  8.40 | ppl  4430.31\n",
      "| epoch 206 |     4/   10 batches | lr 0.92 | ms/batch 59.00 | loss  5.49 | ppl   241.87\n",
      "| epoch 206 |     6/   10 batches | lr 0.92 | ms/batch 67.66 | loss  5.66 | ppl   286.59\n",
      "| epoch 206 |     8/   10 batches | lr 0.92 | ms/batch 66.33 | loss  5.61 | ppl   272.44\n",
      "| epoch 206 |    10/   10 batches | lr 0.92 | ms/batch 44.66 | loss  5.62 | ppl   274.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 206 | time:  0.76s | valid loss  5.83 | valid ppl   341.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 207 |     2/   10 batches | lr 0.92 | ms/batch 121.63 | loss  8.39 | ppl  4418.83\n",
      "| epoch 207 |     4/   10 batches | lr 0.92 | ms/batch 61.53 | loss  5.49 | ppl   242.54\n",
      "| epoch 207 |     6/   10 batches | lr 0.92 | ms/batch 68.01 | loss  5.66 | ppl   286.50\n",
      "| epoch 207 |     8/   10 batches | lr 0.92 | ms/batch 66.20 | loss  5.61 | ppl   272.88\n",
      "| epoch 207 |    10/   10 batches | lr 0.92 | ms/batch 45.75 | loss  5.61 | ppl   274.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 207 | time:  0.82s | valid loss  5.83 | valid ppl   341.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 208 |     2/   10 batches | lr 0.87 | ms/batch 100.62 | loss  8.40 | ppl  4431.42\n",
      "| epoch 208 |     4/   10 batches | lr 0.87 | ms/batch 65.68 | loss  5.49 | ppl   242.11\n",
      "| epoch 208 |     6/   10 batches | lr 0.87 | ms/batch 65.35 | loss  5.66 | ppl   286.45\n",
      "| epoch 208 |     8/   10 batches | lr 0.87 | ms/batch 65.75 | loss  5.61 | ppl   272.22\n",
      "| epoch 208 |    10/   10 batches | lr 0.87 | ms/batch 51.11 | loss  5.62 | ppl   274.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 208 | time:  0.79s | valid loss  5.83 | valid ppl   341.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 209 |     2/   10 batches | lr 0.87 | ms/batch 100.44 | loss  8.39 | ppl  4421.84\n",
      "| epoch 209 |     4/   10 batches | lr 0.87 | ms/batch 62.47 | loss  5.49 | ppl   241.94\n",
      "| epoch 209 |     6/   10 batches | lr 0.87 | ms/batch 65.36 | loss  5.66 | ppl   286.44\n",
      "| epoch 209 |     8/   10 batches | lr 0.87 | ms/batch 66.72 | loss  5.61 | ppl   272.23\n",
      "| epoch 209 |    10/   10 batches | lr 0.87 | ms/batch 50.64 | loss  5.62 | ppl   275.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 209 | time:  0.78s | valid loss  5.83 | valid ppl   340.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 210 |     2/   10 batches | lr 0.87 | ms/batch 100.22 | loss  8.39 | ppl  4423.81\n",
      "| epoch 210 |     4/   10 batches | lr 0.87 | ms/batch 64.52 | loss  5.49 | ppl   242.11\n",
      "| epoch 210 |     6/   10 batches | lr 0.87 | ms/batch 67.98 | loss  5.66 | ppl   286.64\n",
      "| epoch 210 |     8/   10 batches | lr 0.87 | ms/batch 67.94 | loss  5.61 | ppl   272.80\n",
      "| epoch 210 |    10/   10 batches | lr 0.87 | ms/batch 49.01 | loss  5.62 | ppl   274.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 210 | time:  0.78s | valid loss  5.83 | valid ppl   340.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 211 |     2/   10 batches | lr 0.83 | ms/batch 123.66 | loss  8.39 | ppl  4412.14\n",
      "| epoch 211 |     4/   10 batches | lr 0.83 | ms/batch 64.67 | loss  5.49 | ppl   242.13\n",
      "| epoch 211 |     6/   10 batches | lr 0.83 | ms/batch 66.94 | loss  5.66 | ppl   286.42\n",
      "| epoch 211 |     8/   10 batches | lr 0.83 | ms/batch 65.89 | loss  5.61 | ppl   272.34\n",
      "| epoch 211 |    10/   10 batches | lr 0.83 | ms/batch 49.50 | loss  5.61 | ppl   274.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 211 | time:  0.83s | valid loss  5.83 | valid ppl   340.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 212 |     2/   10 batches | lr 0.83 | ms/batch 101.48 | loss  8.39 | ppl  4422.42\n",
      "| epoch 212 |     4/   10 batches | lr 0.83 | ms/batch 70.50 | loss  5.49 | ppl   241.71\n",
      "| epoch 212 |     6/   10 batches | lr 0.83 | ms/batch 66.90 | loss  5.66 | ppl   286.84\n",
      "| epoch 212 |     8/   10 batches | lr 0.83 | ms/batch 63.63 | loss  5.60 | ppl   271.78\n",
      "| epoch 212 |    10/   10 batches | lr 0.83 | ms/batch 46.49 | loss  5.61 | ppl   273.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 212 | time:  0.79s | valid loss  5.83 | valid ppl   340.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 213 |     2/   10 batches | lr 0.83 | ms/batch 97.26 | loss  8.40 | ppl  4430.93\n",
      "| epoch 213 |     4/   10 batches | lr 0.83 | ms/batch 65.27 | loss  5.49 | ppl   242.07\n",
      "| epoch 213 |     6/   10 batches | lr 0.83 | ms/batch 62.63 | loss  5.66 | ppl   287.19\n",
      "| epoch 213 |     8/   10 batches | lr 0.83 | ms/batch 64.35 | loss  5.61 | ppl   272.34\n",
      "| epoch 213 |    10/   10 batches | lr 0.83 | ms/batch 49.00 | loss  5.61 | ppl   273.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 213 | time:  0.77s | valid loss  5.83 | valid ppl   340.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 214 |     2/   10 batches | lr 0.79 | ms/batch 102.07 | loss  8.39 | ppl  4420.06\n",
      "| epoch 214 |     4/   10 batches | lr 0.79 | ms/batch 62.68 | loss  5.49 | ppl   242.28\n",
      "| epoch 214 |     6/   10 batches | lr 0.79 | ms/batch 66.27 | loss  5.66 | ppl   286.89\n",
      "| epoch 214 |     8/   10 batches | lr 0.79 | ms/batch 67.64 | loss  5.61 | ppl   271.79\n",
      "| epoch 214 |    10/   10 batches | lr 0.79 | ms/batch 46.99 | loss  5.61 | ppl   274.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 214 | time:  0.78s | valid loss  5.83 | valid ppl   340.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 215 |     2/   10 batches | lr 0.79 | ms/batch 129.40 | loss  8.39 | ppl  4420.36\n",
      "| epoch 215 |     4/   10 batches | lr 0.79 | ms/batch 63.51 | loss  5.49 | ppl   242.29\n",
      "| epoch 215 |     6/   10 batches | lr 0.79 | ms/batch 68.50 | loss  5.66 | ppl   286.61\n",
      "| epoch 215 |     8/   10 batches | lr 0.79 | ms/batch 65.99 | loss  5.61 | ppl   272.24\n",
      "| epoch 215 |    10/   10 batches | lr 0.79 | ms/batch 47.65 | loss  5.62 | ppl   274.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 215 | time:  0.84s | valid loss  5.83 | valid ppl   340.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 216 |     2/   10 batches | lr 0.79 | ms/batch 93.77 | loss  8.39 | ppl  4415.17\n",
      "| epoch 216 |     4/   10 batches | lr 0.79 | ms/batch 65.50 | loss  5.49 | ppl   242.09\n",
      "| epoch 216 |     6/   10 batches | lr 0.79 | ms/batch 63.01 | loss  5.66 | ppl   286.62\n",
      "| epoch 216 |     8/   10 batches | lr 0.79 | ms/batch 62.78 | loss  5.61 | ppl   272.32\n",
      "| epoch 216 |    10/   10 batches | lr 0.79 | ms/batch 48.12 | loss  5.61 | ppl   274.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 216 | time:  0.76s | valid loss  5.83 | valid ppl   341.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 217 |     2/   10 batches | lr 0.75 | ms/batch 100.58 | loss  8.39 | ppl  4420.38\n",
      "| epoch 217 |     4/   10 batches | lr 0.75 | ms/batch 66.00 | loss  5.49 | ppl   242.04\n",
      "| epoch 217 |     6/   10 batches | lr 0.75 | ms/batch 67.69 | loss  5.66 | ppl   286.86\n",
      "| epoch 217 |     8/   10 batches | lr 0.75 | ms/batch 63.61 | loss  5.61 | ppl   272.21\n",
      "| epoch 217 |    10/   10 batches | lr 0.75 | ms/batch 49.95 | loss  5.61 | ppl   273.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 217 | time:  0.78s | valid loss  5.83 | valid ppl   341.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 218 |     2/   10 batches | lr 0.75 | ms/batch 99.61 | loss  8.39 | ppl  4421.12\n",
      "| epoch 218 |     4/   10 batches | lr 0.75 | ms/batch 69.10 | loss  5.49 | ppl   242.34\n",
      "| epoch 218 |     6/   10 batches | lr 0.75 | ms/batch 64.80 | loss  5.66 | ppl   286.60\n",
      "| epoch 218 |     8/   10 batches | lr 0.75 | ms/batch 64.65 | loss  5.61 | ppl   272.30\n",
      "| epoch 218 |    10/   10 batches | lr 0.75 | ms/batch 42.41 | loss  5.62 | ppl   274.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 218 | time:  0.77s | valid loss  5.84 | valid ppl   342.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 219 |     2/   10 batches | lr 0.75 | ms/batch 115.68 | loss  8.39 | ppl  4420.06\n",
      "| epoch 219 |     4/   10 batches | lr 0.75 | ms/batch 65.75 | loss  5.49 | ppl   242.44\n",
      "| epoch 219 |     6/   10 batches | lr 0.75 | ms/batch 66.23 | loss  5.66 | ppl   286.74\n",
      "| epoch 219 |     8/   10 batches | lr 0.75 | ms/batch 62.11 | loss  5.61 | ppl   272.47\n",
      "| epoch 219 |    10/   10 batches | lr 0.75 | ms/batch 52.90 | loss  5.61 | ppl   274.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 219 | time:  0.82s | valid loss  5.83 | valid ppl   342.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 220 |     2/   10 batches | lr 0.71 | ms/batch 98.23 | loss  8.39 | ppl  4412.27\n",
      "| epoch 220 |     4/   10 batches | lr 0.71 | ms/batch 67.75 | loss  5.49 | ppl   242.20\n",
      "| epoch 220 |     6/   10 batches | lr 0.71 | ms/batch 63.66 | loss  5.66 | ppl   287.01\n",
      "| epoch 220 |     8/   10 batches | lr 0.71 | ms/batch 63.12 | loss  5.61 | ppl   272.39\n",
      "| epoch 220 |    10/   10 batches | lr 0.71 | ms/batch 47.64 | loss  5.61 | ppl   273.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 220 | time:  0.78s | valid loss  5.83 | valid ppl   341.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 221 |     2/   10 batches | lr 0.71 | ms/batch 99.15 | loss  8.39 | ppl  4418.41\n",
      "| epoch 221 |     4/   10 batches | lr 0.71 | ms/batch 61.29 | loss  5.49 | ppl   241.91\n",
      "| epoch 221 |     6/   10 batches | lr 0.71 | ms/batch 66.39 | loss  5.66 | ppl   286.82\n",
      "| epoch 221 |     8/   10 batches | lr 0.71 | ms/batch 68.03 | loss  5.61 | ppl   272.52\n",
      "| epoch 221 |    10/   10 batches | lr 0.71 | ms/batch 51.26 | loss  5.62 | ppl   274.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 221 | time:  0.78s | valid loss  5.83 | valid ppl   341.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 222 |     2/   10 batches | lr 0.71 | ms/batch 94.80 | loss  8.39 | ppl  4416.67\n",
      "| epoch 222 |     4/   10 batches | lr 0.71 | ms/batch 64.27 | loss  5.49 | ppl   242.01\n",
      "| epoch 222 |     6/   10 batches | lr 0.71 | ms/batch 67.12 | loss  5.66 | ppl   286.59\n",
      "| epoch 222 |     8/   10 batches | lr 0.71 | ms/batch 63.55 | loss  5.61 | ppl   272.70\n",
      "| epoch 222 |    10/   10 batches | lr 0.71 | ms/batch 50.74 | loss  5.62 | ppl   274.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 222 | time:  0.82s | valid loss  5.83 | valid ppl   340.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 223 |     2/   10 batches | lr 0.67 | ms/batch 97.00 | loss  8.39 | ppl  4414.23\n",
      "| epoch 223 |     4/   10 batches | lr 0.67 | ms/batch 69.32 | loss  5.49 | ppl   242.31\n",
      "| epoch 223 |     6/   10 batches | lr 0.67 | ms/batch 63.08 | loss  5.66 | ppl   286.69\n",
      "| epoch 223 |     8/   10 batches | lr 0.67 | ms/batch 56.44 | loss  5.61 | ppl   272.35\n",
      "| epoch 223 |    10/   10 batches | lr 0.67 | ms/batch 45.00 | loss  5.62 | ppl   274.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 223 | time:  0.76s | valid loss  5.83 | valid ppl   340.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 224 |     2/   10 batches | lr 0.67 | ms/batch 98.78 | loss  8.39 | ppl  4417.11\n",
      "| epoch 224 |     4/   10 batches | lr 0.67 | ms/batch 69.94 | loss  5.49 | ppl   241.85\n",
      "| epoch 224 |     6/   10 batches | lr 0.67 | ms/batch 66.30 | loss  5.66 | ppl   287.00\n",
      "| epoch 224 |     8/   10 batches | lr 0.67 | ms/batch 63.31 | loss  5.61 | ppl   272.52\n",
      "| epoch 224 |    10/   10 batches | lr 0.67 | ms/batch 48.00 | loss  5.61 | ppl   274.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 224 | time:  0.78s | valid loss  5.83 | valid ppl   341.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 225 |     2/   10 batches | lr 0.67 | ms/batch 97.64 | loss  8.39 | ppl  4422.63\n",
      "| epoch 225 |     4/   10 batches | lr 0.67 | ms/batch 64.21 | loss  5.49 | ppl   242.13\n",
      "| epoch 225 |     6/   10 batches | lr 0.67 | ms/batch 61.57 | loss  5.66 | ppl   286.37\n",
      "| epoch 225 |     8/   10 batches | lr 0.67 | ms/batch 64.13 | loss  5.61 | ppl   272.32\n",
      "| epoch 225 |    10/   10 batches | lr 0.67 | ms/batch 51.67 | loss  5.62 | ppl   274.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 225 | time:  0.77s | valid loss  5.83 | valid ppl   341.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 226 |     2/   10 batches | lr 0.64 | ms/batch 99.01 | loss  8.39 | ppl  4415.26\n",
      "| epoch 226 |     4/   10 batches | lr 0.64 | ms/batch 66.01 | loss  5.49 | ppl   241.81\n",
      "| epoch 226 |     6/   10 batches | lr 0.64 | ms/batch 63.69 | loss  5.66 | ppl   286.53\n",
      "| epoch 226 |     8/   10 batches | lr 0.64 | ms/batch 62.00 | loss  5.61 | ppl   272.40\n",
      "| epoch 226 |    10/   10 batches | lr 0.64 | ms/batch 46.00 | loss  5.61 | ppl   274.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 226 | time:  0.83s | valid loss  5.83 | valid ppl   341.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 227 |     2/   10 batches | lr 0.64 | ms/batch 106.35 | loss  8.39 | ppl  4415.52\n",
      "| epoch 227 |     4/   10 batches | lr 0.64 | ms/batch 64.00 | loss  5.49 | ppl   241.88\n",
      "| epoch 227 |     6/   10 batches | lr 0.64 | ms/batch 62.14 | loss  5.66 | ppl   286.82\n",
      "| epoch 227 |     8/   10 batches | lr 0.64 | ms/batch 67.29 | loss  5.61 | ppl   272.36\n",
      "| epoch 227 |    10/   10 batches | lr 0.64 | ms/batch 52.50 | loss  5.61 | ppl   274.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 227 | time:  0.79s | valid loss  5.83 | valid ppl   341.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 228 |     2/   10 batches | lr 0.64 | ms/batch 98.82 | loss  8.39 | ppl  4415.90\n",
      "| epoch 228 |     4/   10 batches | lr 0.64 | ms/batch 63.51 | loss  5.49 | ppl   241.92\n",
      "| epoch 228 |     6/   10 batches | lr 0.64 | ms/batch 64.12 | loss  5.66 | ppl   286.69\n",
      "| epoch 228 |     8/   10 batches | lr 0.64 | ms/batch 62.14 | loss  5.61 | ppl   272.26\n",
      "| epoch 228 |    10/   10 batches | lr 0.64 | ms/batch 49.50 | loss  5.61 | ppl   273.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 228 | time:  0.77s | valid loss  5.84 | valid ppl   342.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 229 |     2/   10 batches | lr 0.61 | ms/batch 97.90 | loss  8.39 | ppl  4413.87\n",
      "| epoch 229 |     4/   10 batches | lr 0.61 | ms/batch 63.50 | loss  5.49 | ppl   242.02\n",
      "| epoch 229 |     6/   10 batches | lr 0.61 | ms/batch 66.93 | loss  5.66 | ppl   286.58\n",
      "| epoch 229 |     8/   10 batches | lr 0.61 | ms/batch 65.89 | loss  5.61 | ppl   272.36\n",
      "| epoch 229 |    10/   10 batches | lr 0.61 | ms/batch 49.51 | loss  5.61 | ppl   273.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 229 | time:  0.78s | valid loss  5.84 | valid ppl   342.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 230 |     2/   10 batches | lr 0.61 | ms/batch 100.66 | loss  8.39 | ppl  4417.07\n",
      "| epoch 230 |     4/   10 batches | lr 0.61 | ms/batch 63.50 | loss  5.49 | ppl   242.08\n",
      "| epoch 230 |     6/   10 batches | lr 0.61 | ms/batch 96.47 | loss  5.66 | ppl   286.38\n",
      "| epoch 230 |     8/   10 batches | lr 0.61 | ms/batch 68.66 | loss  5.61 | ppl   272.20\n",
      "| epoch 230 |    10/   10 batches | lr 0.61 | ms/batch 49.63 | loss  5.61 | ppl   274.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 230 | time:  0.85s | valid loss  5.84 | valid ppl   342.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 231 |     2/   10 batches | lr 0.61 | ms/batch 91.51 | loss  8.39 | ppl  4416.30\n",
      "| epoch 231 |     4/   10 batches | lr 0.61 | ms/batch 66.72 | loss  5.49 | ppl   242.06\n",
      "| epoch 231 |     6/   10 batches | lr 0.61 | ms/batch 62.99 | loss  5.66 | ppl   286.59\n",
      "| epoch 231 |     8/   10 batches | lr 0.61 | ms/batch 63.50 | loss  5.61 | ppl   272.24\n",
      "| epoch 231 |    10/   10 batches | lr 0.61 | ms/batch 48.00 | loss  5.61 | ppl   274.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 231 | time:  0.76s | valid loss  5.83 | valid ppl   342.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 232 |     2/   10 batches | lr 0.58 | ms/batch 100.06 | loss  8.39 | ppl  4408.50\n",
      "| epoch 232 |     4/   10 batches | lr 0.58 | ms/batch 65.15 | loss  5.49 | ppl   241.90\n",
      "| epoch 232 |     6/   10 batches | lr 0.58 | ms/batch 67.38 | loss  5.66 | ppl   286.98\n",
      "| epoch 232 |     8/   10 batches | lr 0.58 | ms/batch 66.09 | loss  5.61 | ppl   272.44\n",
      "| epoch 232 |    10/   10 batches | lr 0.58 | ms/batch 47.29 | loss  5.61 | ppl   274.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 232 | time:  0.77s | valid loss  5.83 | valid ppl   341.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 233 |     2/   10 batches | lr 0.58 | ms/batch 97.66 | loss  8.39 | ppl  4417.65\n",
      "| epoch 233 |     4/   10 batches | lr 0.58 | ms/batch 66.32 | loss  5.49 | ppl   242.06\n",
      "| epoch 233 |     6/   10 batches | lr 0.58 | ms/batch 68.13 | loss  5.66 | ppl   286.30\n",
      "| epoch 233 |     8/   10 batches | lr 0.58 | ms/batch 62.34 | loss  5.61 | ppl   272.38\n",
      "| epoch 233 |    10/   10 batches | lr 0.58 | ms/batch 49.99 | loss  5.61 | ppl   274.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 233 | time:  0.78s | valid loss  5.83 | valid ppl   341.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 234 |     2/   10 batches | lr 0.58 | ms/batch 126.61 | loss  8.39 | ppl  4410.14\n",
      "| epoch 234 |     4/   10 batches | lr 0.58 | ms/batch 65.73 | loss  5.49 | ppl   241.91\n",
      "| epoch 234 |     6/   10 batches | lr 0.58 | ms/batch 69.62 | loss  5.66 | ppl   286.68\n",
      "| epoch 234 |     8/   10 batches | lr 0.58 | ms/batch 67.37 | loss  5.61 | ppl   272.61\n",
      "| epoch 234 |    10/   10 batches | lr 0.58 | ms/batch 42.80 | loss  5.62 | ppl   274.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 234 | time:  0.84s | valid loss  5.83 | valid ppl   341.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 235 |     2/   10 batches | lr 0.55 | ms/batch 100.10 | loss  8.39 | ppl  4410.93\n",
      "| epoch 235 |     4/   10 batches | lr 0.55 | ms/batch 66.67 | loss  5.49 | ppl   242.24\n",
      "| epoch 235 |     6/   10 batches | lr 0.55 | ms/batch 63.76 | loss  5.66 | ppl   286.55\n",
      "| epoch 235 |     8/   10 batches | lr 0.55 | ms/batch 64.27 | loss  5.61 | ppl   272.34\n",
      "| epoch 235 |    10/   10 batches | lr 0.55 | ms/batch 47.83 | loss  5.61 | ppl   273.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 235 | time:  0.77s | valid loss  5.83 | valid ppl   341.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 236 |     2/   10 batches | lr 0.55 | ms/batch 98.48 | loss  8.39 | ppl  4415.78\n",
      "| epoch 236 |     4/   10 batches | lr 0.55 | ms/batch 65.43 | loss  5.49 | ppl   242.00\n",
      "| epoch 236 |     6/   10 batches | lr 0.55 | ms/batch 64.99 | loss  5.66 | ppl   286.72\n",
      "| epoch 236 |     8/   10 batches | lr 0.55 | ms/batch 67.65 | loss  5.61 | ppl   272.48\n",
      "| epoch 236 |    10/   10 batches | lr 0.55 | ms/batch 51.15 | loss  5.61 | ppl   274.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 236 | time:  0.78s | valid loss  5.83 | valid ppl   340.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 237 |     2/   10 batches | lr 0.55 | ms/batch 100.50 | loss  8.39 | ppl  4411.89\n",
      "| epoch 237 |     4/   10 batches | lr 0.55 | ms/batch 64.79 | loss  5.49 | ppl   241.78\n",
      "| epoch 237 |     6/   10 batches | lr 0.55 | ms/batch 63.41 | loss  5.66 | ppl   286.05\n",
      "| epoch 237 |     8/   10 batches | lr 0.55 | ms/batch 69.23 | loss  5.61 | ppl   272.46\n",
      "| epoch 237 |    10/   10 batches | lr 0.55 | ms/batch 48.35 | loss  5.61 | ppl   274.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 237 | time:  0.84s | valid loss  5.83 | valid ppl   340.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 238 |     2/   10 batches | lr 0.52 | ms/batch 102.06 | loss  8.39 | ppl  4414.33\n",
      "| epoch 238 |     4/   10 batches | lr 0.52 | ms/batch 69.31 | loss  5.49 | ppl   242.25\n",
      "| epoch 238 |     6/   10 batches | lr 0.52 | ms/batch 64.72 | loss  5.66 | ppl   286.26\n",
      "| epoch 238 |     8/   10 batches | lr 0.52 | ms/batch 61.01 | loss  5.61 | ppl   272.44\n",
      "| epoch 238 |    10/   10 batches | lr 0.52 | ms/batch 47.25 | loss  5.61 | ppl   274.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 238 | time:  0.77s | valid loss  5.83 | valid ppl   341.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 239 |     2/   10 batches | lr 0.52 | ms/batch 104.27 | loss  8.39 | ppl  4417.13\n",
      "| epoch 239 |     4/   10 batches | lr 0.52 | ms/batch 58.28 | loss  5.49 | ppl   241.99\n",
      "| epoch 239 |     6/   10 batches | lr 0.52 | ms/batch 63.00 | loss  5.66 | ppl   286.32\n",
      "| epoch 239 |     8/   10 batches | lr 0.52 | ms/batch 67.91 | loss  5.61 | ppl   272.55\n",
      "| epoch 239 |    10/   10 batches | lr 0.52 | ms/batch 47.99 | loss  5.61 | ppl   273.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 239 | time:  0.78s | valid loss  5.83 | valid ppl   341.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 240 |     2/   10 batches | lr 0.52 | ms/batch 99.72 | loss  8.39 | ppl  4417.34\n",
      "| epoch 240 |     4/   10 batches | lr 0.52 | ms/batch 65.76 | loss  5.49 | ppl   241.98\n",
      "| epoch 240 |     6/   10 batches | lr 0.52 | ms/batch 67.15 | loss  5.66 | ppl   286.37\n",
      "| epoch 240 |     8/   10 batches | lr 0.52 | ms/batch 66.00 | loss  5.61 | ppl   272.36\n",
      "| epoch 240 |    10/   10 batches | lr 0.52 | ms/batch 45.28 | loss  5.61 | ppl   273.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 240 | time:  0.78s | valid loss  5.83 | valid ppl   341.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 241 |     2/   10 batches | lr 0.50 | ms/batch 95.50 | loss  8.39 | ppl  4413.12\n",
      "| epoch 241 |     4/   10 batches | lr 0.50 | ms/batch 148.97 | loss  5.49 | ppl   242.07\n",
      "| epoch 241 |     6/   10 batches | lr 0.50 | ms/batch 73.48 | loss  5.66 | ppl   286.43\n",
      "| epoch 241 |     8/   10 batches | lr 0.50 | ms/batch 73.83 | loss  5.61 | ppl   272.21\n",
      "| epoch 241 |    10/   10 batches | lr 0.50 | ms/batch 59.33 | loss  5.61 | ppl   274.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 241 | time:  1.05s | valid loss  5.83 | valid ppl   341.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 242 |     2/   10 batches | lr 0.50 | ms/batch 153.45 | loss  8.39 | ppl  4410.78\n",
      "| epoch 242 |     4/   10 batches | lr 0.50 | ms/batch 73.45 | loss  5.49 | ppl   241.74\n",
      "| epoch 242 |     6/   10 batches | lr 0.50 | ms/batch 70.63 | loss  5.66 | ppl   286.58\n",
      "| epoch 242 |     8/   10 batches | lr 0.50 | ms/batch 66.51 | loss  5.61 | ppl   272.33\n",
      "| epoch 242 |    10/   10 batches | lr 0.50 | ms/batch 53.00 | loss  5.61 | ppl   274.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 242 | time:  0.93s | valid loss  5.83 | valid ppl   341.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 243 |     2/   10 batches | lr 0.50 | ms/batch 105.57 | loss  8.39 | ppl  4411.91\n",
      "| epoch 243 |     4/   10 batches | lr 0.50 | ms/batch 66.28 | loss  5.49 | ppl   241.81\n",
      "| epoch 243 |     6/   10 batches | lr 0.50 | ms/batch 68.01 | loss  5.66 | ppl   286.26\n",
      "| epoch 243 |     8/   10 batches | lr 0.50 | ms/batch 64.62 | loss  5.61 | ppl   272.09\n",
      "| epoch 243 |    10/   10 batches | lr 0.50 | ms/batch 50.00 | loss  5.61 | ppl   273.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 243 | time:  0.79s | valid loss  5.83 | valid ppl   341.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 244 |     2/   10 batches | lr 0.47 | ms/batch 98.17 | loss  8.39 | ppl  4411.29\n",
      "| epoch 244 |     4/   10 batches | lr 0.47 | ms/batch 67.40 | loss  5.49 | ppl   241.66\n",
      "| epoch 244 |     6/   10 batches | lr 0.47 | ms/batch 65.62 | loss  5.66 | ppl   286.41\n",
      "| epoch 244 |     8/   10 batches | lr 0.47 | ms/batch 68.50 | loss  5.61 | ppl   272.20\n",
      "| epoch 244 |    10/   10 batches | lr 0.47 | ms/batch 54.56 | loss  5.61 | ppl   273.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 244 | time:  0.81s | valid loss  5.83 | valid ppl   341.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 245 |     2/   10 batches | lr 0.47 | ms/batch 97.26 | loss  8.39 | ppl  4412.25\n",
      "| epoch 245 |     4/   10 batches | lr 0.47 | ms/batch 64.63 | loss  5.49 | ppl   241.87\n",
      "| epoch 245 |     6/   10 batches | lr 0.47 | ms/batch 85.50 | loss  5.66 | ppl   286.53\n",
      "| epoch 245 |     8/   10 batches | lr 0.47 | ms/batch 70.00 | loss  5.61 | ppl   272.41\n",
      "| epoch 245 |    10/   10 batches | lr 0.47 | ms/batch 51.29 | loss  5.61 | ppl   273.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 245 | time:  0.83s | valid loss  5.83 | valid ppl   342.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 246 |     2/   10 batches | lr 0.47 | ms/batch 104.77 | loss  8.39 | ppl  4416.73\n",
      "| epoch 246 |     4/   10 batches | lr 0.47 | ms/batch 61.99 | loss  5.49 | ppl   241.98\n",
      "| epoch 246 |     6/   10 batches | lr 0.47 | ms/batch 70.67 | loss  5.66 | ppl   286.58\n",
      "| epoch 246 |     8/   10 batches | lr 0.47 | ms/batch 65.50 | loss  5.61 | ppl   272.22\n",
      "| epoch 246 |    10/   10 batches | lr 0.47 | ms/batch 50.21 | loss  5.61 | ppl   273.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 246 | time:  0.80s | valid loss  5.83 | valid ppl   341.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 247 |     2/   10 batches | lr 0.45 | ms/batch 90.81 | loss  8.39 | ppl  4409.35\n",
      "| epoch 247 |     4/   10 batches | lr 0.45 | ms/batch 62.00 | loss  5.49 | ppl   241.68\n",
      "| epoch 247 |     6/   10 batches | lr 0.45 | ms/batch 68.45 | loss  5.66 | ppl   286.49\n",
      "| epoch 247 |     8/   10 batches | lr 0.45 | ms/batch 59.68 | loss  5.61 | ppl   272.59\n",
      "| epoch 247 |    10/   10 batches | lr 0.45 | ms/batch 47.52 | loss  5.61 | ppl   274.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 247 | time:  0.75s | valid loss  5.83 | valid ppl   341.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 248 |     2/   10 batches | lr 0.45 | ms/batch 97.45 | loss  8.39 | ppl  4418.61\n",
      "| epoch 248 |     4/   10 batches | lr 0.45 | ms/batch 66.99 | loss  5.49 | ppl   241.79\n",
      "| epoch 248 |     6/   10 batches | lr 0.45 | ms/batch 63.76 | loss  5.66 | ppl   286.75\n",
      "| epoch 248 |     8/   10 batches | lr 0.45 | ms/batch 61.98 | loss  5.61 | ppl   272.13\n",
      "| epoch 248 |    10/   10 batches | lr 0.45 | ms/batch 77.13 | loss  5.61 | ppl   273.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 248 | time:  0.84s | valid loss  5.84 | valid ppl   342.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 249 |     2/   10 batches | lr 0.45 | ms/batch 96.93 | loss  8.39 | ppl  4412.08\n",
      "| epoch 249 |     4/   10 batches | lr 0.45 | ms/batch 65.69 | loss  5.49 | ppl   241.89\n",
      "| epoch 249 |     6/   10 batches | lr 0.45 | ms/batch 67.64 | loss  5.66 | ppl   286.97\n",
      "| epoch 249 |     8/   10 batches | lr 0.45 | ms/batch 65.32 | loss  5.61 | ppl   272.65\n",
      "| epoch 249 |    10/   10 batches | lr 0.45 | ms/batch 51.73 | loss  5.61 | ppl   274.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 249 | time:  0.79s | valid loss  5.83 | valid ppl   341.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 250 |     2/   10 batches | lr 0.42 | ms/batch 103.56 | loss  8.39 | ppl  4410.22\n",
      "| epoch 250 |     4/   10 batches | lr 0.42 | ms/batch 68.50 | loss  5.49 | ppl   241.93\n",
      "| epoch 250 |     6/   10 batches | lr 0.42 | ms/batch 74.64 | loss  5.66 | ppl   286.34\n",
      "| epoch 250 |     8/   10 batches | lr 0.42 | ms/batch 59.50 | loss  5.61 | ppl   272.26\n",
      "| epoch 250 |    10/   10 batches | lr 0.42 | ms/batch 50.93 | loss  5.61 | ppl   273.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 250 | time:  0.81s | valid loss  5.83 | valid ppl   341.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 251 |     2/   10 batches | lr 0.42 | ms/batch 96.15 | loss  8.39 | ppl  4416.57\n",
      "| epoch 251 |     4/   10 batches | lr 0.42 | ms/batch 63.50 | loss  5.49 | ppl   241.68\n",
      "| epoch 251 |     6/   10 batches | lr 0.42 | ms/batch 67.27 | loss  5.66 | ppl   286.47\n",
      "| epoch 251 |     8/   10 batches | lr 0.42 | ms/batch 69.73 | loss  5.61 | ppl   272.20\n",
      "| epoch 251 |    10/   10 batches | lr 0.42 | ms/batch 52.50 | loss  5.61 | ppl   273.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 251 | time:  0.79s | valid loss  5.83 | valid ppl   341.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 252 |     2/   10 batches | lr 0.42 | ms/batch 122.59 | loss  8.39 | ppl  4415.89\n",
      "| epoch 252 |     4/   10 batches | lr 0.42 | ms/batch 65.28 | loss  5.49 | ppl   241.66\n",
      "| epoch 252 |     6/   10 batches | lr 0.42 | ms/batch 67.49 | loss  5.66 | ppl   286.41\n",
      "| epoch 252 |     8/   10 batches | lr 0.42 | ms/batch 62.51 | loss  5.61 | ppl   272.40\n",
      "| epoch 252 |    10/   10 batches | lr 0.42 | ms/batch 47.27 | loss  5.61 | ppl   274.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 252 | time:  0.82s | valid loss  5.83 | valid ppl   341.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 253 |     2/   10 batches | lr 0.40 | ms/batch 96.93 | loss  8.39 | ppl  4415.34\n",
      "| epoch 253 |     4/   10 batches | lr 0.40 | ms/batch 66.99 | loss  5.49 | ppl   241.77\n",
      "| epoch 253 |     6/   10 batches | lr 0.40 | ms/batch 64.64 | loss  5.66 | ppl   286.60\n",
      "| epoch 253 |     8/   10 batches | lr 0.40 | ms/batch 64.49 | loss  5.61 | ppl   272.11\n",
      "| epoch 253 |    10/   10 batches | lr 0.40 | ms/batch 46.01 | loss  5.61 | ppl   273.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 253 | time:  0.77s | valid loss  5.83 | valid ppl   341.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 254 |     2/   10 batches | lr 0.40 | ms/batch 105.57 | loss  8.39 | ppl  4413.79\n",
      "| epoch 254 |     4/   10 batches | lr 0.40 | ms/batch 59.78 | loss  5.49 | ppl   241.69\n",
      "| epoch 254 |     6/   10 batches | lr 0.40 | ms/batch 66.50 | loss  5.66 | ppl   286.88\n",
      "| epoch 254 |     8/   10 batches | lr 0.40 | ms/batch 64.07 | loss  5.61 | ppl   272.10\n",
      "| epoch 254 |    10/   10 batches | lr 0.40 | ms/batch 51.15 | loss  5.61 | ppl   273.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 254 | time:  0.78s | valid loss  5.83 | valid ppl   341.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 255 |     2/   10 batches | lr 0.40 | ms/batch 95.25 | loss  8.39 | ppl  4412.51\n",
      "| epoch 255 |     4/   10 batches | lr 0.40 | ms/batch 59.00 | loss  5.49 | ppl   241.71\n",
      "| epoch 255 |     6/   10 batches | lr 0.40 | ms/batch 91.71 | loss  5.66 | ppl   286.68\n",
      "| epoch 255 |     8/   10 batches | lr 0.40 | ms/batch 70.14 | loss  5.61 | ppl   272.36\n",
      "| epoch 255 |    10/   10 batches | lr 0.40 | ms/batch 50.49 | loss  5.61 | ppl   274.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 255 | time:  0.82s | valid loss  5.83 | valid ppl   341.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 256 |     2/   10 batches | lr 0.38 | ms/batch 100.97 | loss  8.39 | ppl  4408.17\n",
      "| epoch 256 |     4/   10 batches | lr 0.38 | ms/batch 63.64 | loss  5.49 | ppl   241.86\n",
      "| epoch 256 |     6/   10 batches | lr 0.38 | ms/batch 64.69 | loss  5.66 | ppl   286.86\n",
      "| epoch 256 |     8/   10 batches | lr 0.38 | ms/batch 65.13 | loss  5.61 | ppl   272.43\n",
      "| epoch 256 |    10/   10 batches | lr 0.38 | ms/batch 52.74 | loss  5.61 | ppl   274.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 256 | time:  0.79s | valid loss  5.83 | valid ppl   341.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 257 |     2/   10 batches | lr 0.38 | ms/batch 99.69 | loss  8.39 | ppl  4407.02\n",
      "| epoch 257 |     4/   10 batches | lr 0.38 | ms/batch 64.67 | loss  5.49 | ppl   242.05\n",
      "| epoch 257 |     6/   10 batches | lr 0.38 | ms/batch 67.44 | loss  5.66 | ppl   286.72\n",
      "| epoch 257 |     8/   10 batches | lr 0.38 | ms/batch 65.99 | loss  5.61 | ppl   272.46\n",
      "| epoch 257 |    10/   10 batches | lr 0.38 | ms/batch 50.51 | loss  5.61 | ppl   273.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 257 | time:  0.78s | valid loss  5.83 | valid ppl   341.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 258 |     2/   10 batches | lr 0.38 | ms/batch 99.67 | loss  8.39 | ppl  4414.70\n",
      "| epoch 258 |     4/   10 batches | lr 0.38 | ms/batch 64.12 | loss  5.49 | ppl   241.91\n",
      "| epoch 258 |     6/   10 batches | lr 0.38 | ms/batch 68.85 | loss  5.66 | ppl   286.67\n",
      "| epoch 258 |     8/   10 batches | lr 0.38 | ms/batch 65.00 | loss  5.61 | ppl   272.41\n",
      "| epoch 258 |    10/   10 batches | lr 0.38 | ms/batch 75.04 | loss  5.61 | ppl   273.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 258 | time:  0.85s | valid loss  5.83 | valid ppl   341.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 259 |     2/   10 batches | lr 0.36 | ms/batch 97.25 | loss  8.39 | ppl  4405.17\n",
      "| epoch 259 |     4/   10 batches | lr 0.36 | ms/batch 64.14 | loss  5.49 | ppl   241.64\n",
      "| epoch 259 |     6/   10 batches | lr 0.36 | ms/batch 67.00 | loss  5.66 | ppl   286.41\n",
      "| epoch 259 |     8/   10 batches | lr 0.36 | ms/batch 63.00 | loss  5.61 | ppl   272.41\n",
      "| epoch 259 |    10/   10 batches | lr 0.36 | ms/batch 48.31 | loss  5.61 | ppl   273.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 259 | time:  0.77s | valid loss  5.83 | valid ppl   342.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 260 |     2/   10 batches | lr 0.36 | ms/batch 101.50 | loss  8.39 | ppl  4412.52\n",
      "| epoch 260 |     4/   10 batches | lr 0.36 | ms/batch 63.66 | loss  5.49 | ppl   241.85\n",
      "| epoch 260 |     6/   10 batches | lr 0.36 | ms/batch 64.14 | loss  5.66 | ppl   286.32\n",
      "| epoch 260 |     8/   10 batches | lr 0.36 | ms/batch 62.40 | loss  5.61 | ppl   272.13\n",
      "| epoch 260 |    10/   10 batches | lr 0.36 | ms/batch 50.50 | loss  5.61 | ppl   273.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 260 | time:  0.78s | valid loss  5.83 | valid ppl   341.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 261 |     2/   10 batches | lr 0.36 | ms/batch 95.81 | loss  8.39 | ppl  4415.26\n",
      "| epoch 261 |     4/   10 batches | lr 0.36 | ms/batch 63.99 | loss  5.49 | ppl   241.82\n",
      "| epoch 261 |     6/   10 batches | lr 0.36 | ms/batch 67.02 | loss  5.66 | ppl   286.30\n",
      "| epoch 261 |     8/   10 batches | lr 0.36 | ms/batch 64.63 | loss  5.61 | ppl   272.65\n",
      "| epoch 261 |    10/   10 batches | lr 0.36 | ms/batch 49.75 | loss  5.61 | ppl   274.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 261 | time:  0.78s | valid loss  5.84 | valid ppl   342.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 262 |     2/   10 batches | lr 0.35 | ms/batch 119.66 | loss  8.39 | ppl  4406.31\n",
      "| epoch 262 |     4/   10 batches | lr 0.35 | ms/batch 65.01 | loss  5.49 | ppl   241.63\n",
      "| epoch 262 |     6/   10 batches | lr 0.35 | ms/batch 64.28 | loss  5.66 | ppl   286.39\n",
      "| epoch 262 |     8/   10 batches | lr 0.35 | ms/batch 68.00 | loss  5.61 | ppl   272.51\n",
      "| epoch 262 |    10/   10 batches | lr 0.35 | ms/batch 46.25 | loss  5.61 | ppl   273.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 262 | time:  0.82s | valid loss  5.83 | valid ppl   341.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 263 |     2/   10 batches | lr 0.35 | ms/batch 95.27 | loss  8.39 | ppl  4408.44\n",
      "| epoch 263 |     4/   10 batches | lr 0.35 | ms/batch 62.18 | loss  5.49 | ppl   241.87\n",
      "| epoch 263 |     6/   10 batches | lr 0.35 | ms/batch 66.00 | loss  5.66 | ppl   286.39\n",
      "| epoch 263 |     8/   10 batches | lr 0.35 | ms/batch 64.64 | loss  5.61 | ppl   272.20\n",
      "| epoch 263 |    10/   10 batches | lr 0.35 | ms/batch 46.50 | loss  5.61 | ppl   273.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 263 | time:  0.76s | valid loss  5.83 | valid ppl   341.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 264 |     2/   10 batches | lr 0.35 | ms/batch 98.27 | loss  8.39 | ppl  4408.46\n",
      "| epoch 264 |     4/   10 batches | lr 0.35 | ms/batch 65.09 | loss  5.49 | ppl   241.75\n",
      "| epoch 264 |     6/   10 batches | lr 0.35 | ms/batch 67.89 | loss  5.66 | ppl   286.23\n",
      "| epoch 264 |     8/   10 batches | lr 0.35 | ms/batch 62.28 | loss  5.61 | ppl   272.40\n",
      "| epoch 264 |    10/   10 batches | lr 0.35 | ms/batch 51.01 | loss  5.61 | ppl   273.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 264 | time:  0.78s | valid loss  5.83 | valid ppl   341.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 265 |     2/   10 batches | lr 0.33 | ms/batch 98.60 | loss  8.39 | ppl  4408.65\n",
      "| epoch 265 |     4/   10 batches | lr 0.33 | ms/batch 59.50 | loss  5.49 | ppl   241.75\n",
      "| epoch 265 |     6/   10 batches | lr 0.33 | ms/batch 89.04 | loss  5.66 | ppl   286.53\n",
      "| epoch 265 |     8/   10 batches | lr 0.33 | ms/batch 62.12 | loss  5.61 | ppl   272.24\n",
      "| epoch 265 |    10/   10 batches | lr 0.33 | ms/batch 51.32 | loss  5.61 | ppl   274.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 265 | time:  0.82s | valid loss  5.83 | valid ppl   341.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 266 |     2/   10 batches | lr 0.33 | ms/batch 99.65 | loss  8.39 | ppl  4404.60\n",
      "| epoch 266 |     4/   10 batches | lr 0.33 | ms/batch 57.80 | loss  5.49 | ppl   241.45\n",
      "| epoch 266 |     6/   10 batches | lr 0.33 | ms/batch 66.75 | loss  5.66 | ppl   286.59\n",
      "| epoch 266 |     8/   10 batches | lr 0.33 | ms/batch 65.10 | loss  5.61 | ppl   272.00\n",
      "| epoch 266 |    10/   10 batches | lr 0.33 | ms/batch 48.00 | loss  5.61 | ppl   273.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 266 | time:  0.76s | valid loss  5.83 | valid ppl   341.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 267 |     2/   10 batches | lr 0.33 | ms/batch 99.73 | loss  8.39 | ppl  4407.70\n",
      "| epoch 267 |     4/   10 batches | lr 0.33 | ms/batch 61.75 | loss  5.49 | ppl   241.25\n",
      "| epoch 267 |     6/   10 batches | lr 0.33 | ms/batch 69.00 | loss  5.66 | ppl   286.26\n",
      "| epoch 267 |     8/   10 batches | lr 0.33 | ms/batch 67.50 | loss  5.61 | ppl   272.52\n",
      "| epoch 267 |    10/   10 batches | lr 0.33 | ms/batch 46.00 | loss  5.61 | ppl   274.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 267 | time:  0.77s | valid loss  5.83 | valid ppl   341.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 268 |     2/   10 batches | lr 0.31 | ms/batch 95.55 | loss  8.39 | ppl  4404.13\n",
      "| epoch 268 |     4/   10 batches | lr 0.31 | ms/batch 67.00 | loss  5.49 | ppl   241.86\n",
      "| epoch 268 |     6/   10 batches | lr 0.31 | ms/batch 66.78 | loss  5.66 | ppl   286.56\n",
      "| epoch 268 |     8/   10 batches | lr 0.31 | ms/batch 60.99 | loss  5.61 | ppl   272.30\n",
      "| epoch 268 |    10/   10 batches | lr 0.31 | ms/batch 46.51 | loss  5.61 | ppl   274.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 268 | time:  0.82s | valid loss  5.83 | valid ppl   341.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 269 |     2/   10 batches | lr 0.31 | ms/batch 102.44 | loss  8.39 | ppl  4408.21\n",
      "| epoch 269 |     4/   10 batches | lr 0.31 | ms/batch 66.40 | loss  5.49 | ppl   241.61\n",
      "| epoch 269 |     6/   10 batches | lr 0.31 | ms/batch 63.50 | loss  5.66 | ppl   286.58\n",
      "| epoch 269 |     8/   10 batches | lr 0.31 | ms/batch 65.13 | loss  5.61 | ppl   272.01\n",
      "| epoch 269 |    10/   10 batches | lr 0.31 | ms/batch 48.73 | loss  5.61 | ppl   274.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 269 | time:  0.79s | valid loss  5.83 | valid ppl   341.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 270 |     2/   10 batches | lr 0.31 | ms/batch 93.61 | loss  8.39 | ppl  4409.42\n",
      "| epoch 270 |     4/   10 batches | lr 0.31 | ms/batch 69.18 | loss  5.49 | ppl   241.58\n",
      "| epoch 270 |     6/   10 batches | lr 0.31 | ms/batch 66.37 | loss  5.66 | ppl   286.37\n",
      "| epoch 270 |     8/   10 batches | lr 0.31 | ms/batch 67.62 | loss  5.61 | ppl   272.21\n",
      "| epoch 270 |    10/   10 batches | lr 0.31 | ms/batch 48.15 | loss  5.61 | ppl   274.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 270 | time:  0.78s | valid loss  5.83 | valid ppl   341.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 271 |     2/   10 batches | lr 0.30 | ms/batch 99.16 | loss  8.39 | ppl  4411.27\n",
      "| epoch 271 |     4/   10 batches | lr 0.30 | ms/batch 62.00 | loss  5.49 | ppl   241.72\n",
      "| epoch 271 |     6/   10 batches | lr 0.30 | ms/batch 62.00 | loss  5.66 | ppl   286.61\n",
      "| epoch 271 |     8/   10 batches | lr 0.30 | ms/batch 67.66 | loss  5.61 | ppl   272.52\n",
      "| epoch 271 |    10/   10 batches | lr 0.30 | ms/batch 52.16 | loss  5.61 | ppl   273.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 271 | time:  0.78s | valid loss  5.83 | valid ppl   341.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 272 |     2/   10 batches | lr 0.30 | ms/batch 125.33 | loss  8.39 | ppl  4403.19\n",
      "| epoch 272 |     4/   10 batches | lr 0.30 | ms/batch 69.13 | loss  5.49 | ppl   241.82\n",
      "| epoch 272 |     6/   10 batches | lr 0.30 | ms/batch 62.70 | loss  5.66 | ppl   286.52\n",
      "| epoch 272 |     8/   10 batches | lr 0.30 | ms/batch 63.90 | loss  5.61 | ppl   272.26\n",
      "| epoch 272 |    10/   10 batches | lr 0.30 | ms/batch 49.14 | loss  5.61 | ppl   273.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 272 | time:  0.83s | valid loss  5.84 | valid ppl   342.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 273 |     2/   10 batches | lr 0.30 | ms/batch 104.33 | loss  8.39 | ppl  4408.58\n",
      "| epoch 273 |     4/   10 batches | lr 0.30 | ms/batch 63.58 | loss  5.49 | ppl   241.67\n",
      "| epoch 273 |     6/   10 batches | lr 0.30 | ms/batch 61.00 | loss  5.66 | ppl   286.33\n",
      "| epoch 273 |     8/   10 batches | lr 0.30 | ms/batch 67.14 | loss  5.61 | ppl   272.51\n",
      "| epoch 273 |    10/   10 batches | lr 0.30 | ms/batch 50.13 | loss  5.61 | ppl   274.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 273 | time:  0.79s | valid loss  5.83 | valid ppl   341.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 274 |     2/   10 batches | lr 0.28 | ms/batch 98.18 | loss  8.39 | ppl  4403.92\n",
      "| epoch 274 |     4/   10 batches | lr 0.28 | ms/batch 68.31 | loss  5.49 | ppl   241.58\n",
      "| epoch 274 |     6/   10 batches | lr 0.28 | ms/batch 64.65 | loss  5.66 | ppl   286.49\n",
      "| epoch 274 |     8/   10 batches | lr 0.28 | ms/batch 65.63 | loss  5.61 | ppl   272.39\n",
      "| epoch 274 |    10/   10 batches | lr 0.28 | ms/batch 44.78 | loss  5.61 | ppl   274.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 274 | time:  0.78s | valid loss  5.83 | valid ppl   342.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 275 |     2/   10 batches | lr 0.28 | ms/batch 123.58 | loss  8.39 | ppl  4408.61\n",
      "| epoch 275 |     4/   10 batches | lr 0.28 | ms/batch 62.77 | loss  5.49 | ppl   241.54\n",
      "| epoch 275 |     6/   10 batches | lr 0.28 | ms/batch 65.54 | loss  5.66 | ppl   286.24\n",
      "| epoch 275 |     8/   10 batches | lr 0.28 | ms/batch 66.00 | loss  5.61 | ppl   272.51\n",
      "| epoch 275 |    10/   10 batches | lr 0.28 | ms/batch 51.01 | loss  5.61 | ppl   274.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 275 | time:  0.82s | valid loss  5.83 | valid ppl   341.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 276 |     2/   10 batches | lr 0.28 | ms/batch 98.79 | loss  8.39 | ppl  4408.87\n",
      "| epoch 276 |     4/   10 batches | lr 0.28 | ms/batch 60.50 | loss  5.49 | ppl   241.96\n",
      "| epoch 276 |     6/   10 batches | lr 0.28 | ms/batch 65.50 | loss  5.66 | ppl   286.67\n",
      "| epoch 276 |     8/   10 batches | lr 0.28 | ms/batch 62.34 | loss  5.61 | ppl   272.20\n",
      "| epoch 276 |    10/   10 batches | lr 0.28 | ms/batch 48.99 | loss  5.61 | ppl   274.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 276 | time:  0.77s | valid loss  5.83 | valid ppl   341.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 277 |     2/   10 batches | lr 0.27 | ms/batch 100.05 | loss  8.39 | ppl  4412.52\n",
      "| epoch 277 |     4/   10 batches | lr 0.27 | ms/batch 66.01 | loss  5.49 | ppl   241.43\n",
      "| epoch 277 |     6/   10 batches | lr 0.27 | ms/batch 68.67 | loss  5.66 | ppl   286.30\n",
      "| epoch 277 |     8/   10 batches | lr 0.27 | ms/batch 63.12 | loss  5.61 | ppl   272.29\n",
      "| epoch 277 |    10/   10 batches | lr 0.27 | ms/batch 45.49 | loss  5.61 | ppl   273.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 277 | time:  0.78s | valid loss  5.83 | valid ppl   341.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 278 |     2/   10 batches | lr 0.27 | ms/batch 127.35 | loss  8.39 | ppl  4405.06\n",
      "| epoch 278 |     4/   10 batches | lr 0.27 | ms/batch 65.69 | loss  5.49 | ppl   241.82\n",
      "| epoch 278 |     6/   10 batches | lr 0.27 | ms/batch 65.22 | loss  5.66 | ppl   286.29\n",
      "| epoch 278 |     8/   10 batches | lr 0.27 | ms/batch 70.64 | loss  5.61 | ppl   272.42\n",
      "| epoch 278 |    10/   10 batches | lr 0.27 | ms/batch 55.13 | loss  5.61 | ppl   273.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 278 | time:  0.88s | valid loss  5.83 | valid ppl   341.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 279 |     2/   10 batches | lr 0.27 | ms/batch 132.51 | loss  8.39 | ppl  4403.18\n",
      "| epoch 279 |     4/   10 batches | lr 0.27 | ms/batch 81.52 | loss  5.49 | ppl   241.77\n",
      "| epoch 279 |     6/   10 batches | lr 0.27 | ms/batch 83.28 | loss  5.66 | ppl   286.44\n",
      "| epoch 279 |     8/   10 batches | lr 0.27 | ms/batch 93.47 | loss  5.61 | ppl   272.43\n",
      "| epoch 279 |    10/   10 batches | lr 0.27 | ms/batch 67.88 | loss  5.61 | ppl   273.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 279 | time:  1.07s | valid loss  5.83 | valid ppl   341.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 280 |     2/   10 batches | lr 0.25 | ms/batch 191.12 | loss  8.39 | ppl  4402.39\n",
      "| epoch 280 |     4/   10 batches | lr 0.25 | ms/batch 82.67 | loss  5.49 | ppl   241.96\n",
      "| epoch 280 |     6/   10 batches | lr 0.25 | ms/batch 81.12 | loss  5.66 | ppl   286.60\n",
      "| epoch 280 |     8/   10 batches | lr 0.25 | ms/batch 84.26 | loss  5.61 | ppl   272.11\n",
      "| epoch 280 |    10/   10 batches | lr 0.25 | ms/batch 76.87 | loss  5.61 | ppl   273.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 280 | time:  1.14s | valid loss  5.83 | valid ppl   341.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 281 |     2/   10 batches | lr 0.25 | ms/batch 168.79 | loss  8.39 | ppl  4407.16\n",
      "| epoch 281 |     4/   10 batches | lr 0.25 | ms/batch 68.50 | loss  5.49 | ppl   241.65\n",
      "| epoch 281 |     6/   10 batches | lr 0.25 | ms/batch 79.87 | loss  5.66 | ppl   286.40\n",
      "| epoch 281 |     8/   10 batches | lr 0.25 | ms/batch 80.42 | loss  5.61 | ppl   272.62\n",
      "| epoch 281 |    10/   10 batches | lr 0.25 | ms/batch 66.79 | loss  5.61 | ppl   273.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 281 | time:  1.05s | valid loss  5.83 | valid ppl   341.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 282 |     2/   10 batches | lr 0.25 | ms/batch 114.19 | loss  8.39 | ppl  4404.30\n",
      "| epoch 282 |     4/   10 batches | lr 0.25 | ms/batch 69.63 | loss  5.49 | ppl   241.43\n",
      "| epoch 282 |     6/   10 batches | lr 0.25 | ms/batch 74.11 | loss  5.66 | ppl   286.56\n",
      "| epoch 282 |     8/   10 batches | lr 0.25 | ms/batch 83.00 | loss  5.61 | ppl   272.68\n",
      "| epoch 282 |    10/   10 batches | lr 0.25 | ms/batch 63.91 | loss  5.61 | ppl   273.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 282 | time:  0.93s | valid loss  5.83 | valid ppl   341.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 283 |     2/   10 batches | lr 0.24 | ms/batch 126.27 | loss  8.39 | ppl  4406.03\n",
      "| epoch 283 |     4/   10 batches | lr 0.24 | ms/batch 74.50 | loss  5.49 | ppl   241.61\n",
      "| epoch 283 |     6/   10 batches | lr 0.24 | ms/batch 69.60 | loss  5.66 | ppl   286.62\n",
      "| epoch 283 |     8/   10 batches | lr 0.24 | ms/batch 66.51 | loss  5.61 | ppl   272.56\n",
      "| epoch 283 |    10/   10 batches | lr 0.24 | ms/batch 57.80 | loss  5.61 | ppl   273.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 283 | time:  0.88s | valid loss  5.83 | valid ppl   341.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 284 |     2/   10 batches | lr 0.24 | ms/batch 133.94 | loss  8.39 | ppl  4412.01\n",
      "| epoch 284 |     4/   10 batches | lr 0.24 | ms/batch 69.64 | loss  5.49 | ppl   241.64\n",
      "| epoch 284 |     6/   10 batches | lr 0.24 | ms/batch 67.62 | loss  5.66 | ppl   286.44\n",
      "| epoch 284 |     8/   10 batches | lr 0.24 | ms/batch 70.37 | loss  5.61 | ppl   272.13\n",
      "| epoch 284 |    10/   10 batches | lr 0.24 | ms/batch 52.32 | loss  5.61 | ppl   273.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 284 | time:  0.88s | valid loss  5.83 | valid ppl   341.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 285 |     2/   10 batches | lr 0.24 | ms/batch 94.14 | loss  8.39 | ppl  4407.79\n",
      "| epoch 285 |     4/   10 batches | lr 0.24 | ms/batch 66.01 | loss  5.49 | ppl   241.41\n",
      "| epoch 285 |     6/   10 batches | lr 0.24 | ms/batch 75.45 | loss  5.66 | ppl   286.64\n",
      "| epoch 285 |     8/   10 batches | lr 0.24 | ms/batch 64.22 | loss  5.61 | ppl   272.61\n",
      "| epoch 285 |    10/   10 batches | lr 0.24 | ms/batch 49.02 | loss  5.61 | ppl   273.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 285 | time:  0.80s | valid loss  5.83 | valid ppl   341.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 286 |     2/   10 batches | lr 0.23 | ms/batch 94.78 | loss  8.39 | ppl  4402.69\n",
      "| epoch 286 |     4/   10 batches | lr 0.23 | ms/batch 70.51 | loss  5.49 | ppl   241.54\n",
      "| epoch 286 |     6/   10 batches | lr 0.23 | ms/batch 67.29 | loss  5.66 | ppl   286.37\n",
      "| epoch 286 |     8/   10 batches | lr 0.23 | ms/batch 67.64 | loss  5.61 | ppl   272.44\n",
      "| epoch 286 |    10/   10 batches | lr 0.23 | ms/batch 50.49 | loss  5.61 | ppl   273.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 286 | time:  0.79s | valid loss  5.83 | valid ppl   341.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 287 |     2/   10 batches | lr 0.23 | ms/batch 126.12 | loss  8.39 | ppl  4409.65\n",
      "| epoch 287 |     4/   10 batches | lr 0.23 | ms/batch 69.31 | loss  5.49 | ppl   241.66\n",
      "| epoch 287 |     6/   10 batches | lr 0.23 | ms/batch 65.52 | loss  5.66 | ppl   286.33\n",
      "| epoch 287 |     8/   10 batches | lr 0.23 | ms/batch 66.90 | loss  5.61 | ppl   272.48\n",
      "| epoch 287 |    10/   10 batches | lr 0.23 | ms/batch 49.66 | loss  5.61 | ppl   273.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 287 | time:  0.84s | valid loss  5.83 | valid ppl   341.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 288 |     2/   10 batches | lr 0.23 | ms/batch 96.50 | loss  8.39 | ppl  4403.01\n",
      "| epoch 288 |     4/   10 batches | lr 0.23 | ms/batch 66.63 | loss  5.49 | ppl   241.62\n",
      "| epoch 288 |     6/   10 batches | lr 0.23 | ms/batch 65.00 | loss  5.66 | ppl   286.56\n",
      "| epoch 288 |     8/   10 batches | lr 0.23 | ms/batch 66.60 | loss  5.61 | ppl   272.35\n",
      "| epoch 288 |    10/   10 batches | lr 0.23 | ms/batch 49.49 | loss  5.61 | ppl   273.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 288 | time:  0.78s | valid loss  5.83 | valid ppl   341.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 289 |     2/   10 batches | lr 0.22 | ms/batch 97.95 | loss  8.39 | ppl  4402.44\n",
      "| epoch 289 |     4/   10 batches | lr 0.22 | ms/batch 62.62 | loss  5.49 | ppl   241.81\n",
      "| epoch 289 |     6/   10 batches | lr 0.22 | ms/batch 67.07 | loss  5.66 | ppl   286.58\n",
      "| epoch 289 |     8/   10 batches | lr 0.22 | ms/batch 64.94 | loss  5.61 | ppl   272.34\n",
      "| epoch 289 |    10/   10 batches | lr 0.22 | ms/batch 50.00 | loss  5.61 | ppl   273.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 289 | time:  0.78s | valid loss  5.83 | valid ppl   341.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 290 |     2/   10 batches | lr 0.22 | ms/batch 122.99 | loss  8.39 | ppl  4404.42\n",
      "| epoch 290 |     4/   10 batches | lr 0.22 | ms/batch 62.17 | loss  5.49 | ppl   241.75\n",
      "| epoch 290 |     6/   10 batches | lr 0.22 | ms/batch 68.13 | loss  5.66 | ppl   286.59\n",
      "| epoch 290 |     8/   10 batches | lr 0.22 | ms/batch 68.01 | loss  5.61 | ppl   272.23\n",
      "| epoch 290 |    10/   10 batches | lr 0.22 | ms/batch 50.99 | loss  5.61 | ppl   273.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 290 | time:  0.83s | valid loss  5.83 | valid ppl   341.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 291 |     2/   10 batches | lr 0.22 | ms/batch 99.40 | loss  8.39 | ppl  4408.78\n",
      "| epoch 291 |     4/   10 batches | lr 0.22 | ms/batch 65.13 | loss  5.49 | ppl   241.63\n",
      "| epoch 291 |     6/   10 batches | lr 0.22 | ms/batch 70.82 | loss  5.66 | ppl   286.51\n",
      "| epoch 291 |     8/   10 batches | lr 0.22 | ms/batch 65.66 | loss  5.61 | ppl   271.97\n",
      "| epoch 291 |    10/   10 batches | lr 0.22 | ms/batch 49.38 | loss  5.61 | ppl   274.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 291 | time:  0.80s | valid loss  5.83 | valid ppl   341.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 292 |     2/   10 batches | lr 0.21 | ms/batch 101.16 | loss  8.39 | ppl  4401.98\n",
      "| epoch 292 |     4/   10 batches | lr 0.21 | ms/batch 66.01 | loss  5.49 | ppl   241.74\n",
      "| epoch 292 |     6/   10 batches | lr 0.21 | ms/batch 61.49 | loss  5.66 | ppl   286.41\n",
      "| epoch 292 |     8/   10 batches | lr 0.21 | ms/batch 62.50 | loss  5.61 | ppl   272.31\n",
      "| epoch 292 |    10/   10 batches | lr 0.21 | ms/batch 50.73 | loss  5.61 | ppl   274.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 292 | time:  0.78s | valid loss  5.83 | valid ppl   341.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 293 |     2/   10 batches | lr 0.21 | ms/batch 127.26 | loss  8.39 | ppl  4405.41\n",
      "| epoch 293 |     4/   10 batches | lr 0.21 | ms/batch 68.72 | loss  5.49 | ppl   241.48\n",
      "| epoch 293 |     6/   10 batches | lr 0.21 | ms/batch 64.77 | loss  5.66 | ppl   286.67\n",
      "| epoch 293 |     8/   10 batches | lr 0.21 | ms/batch 63.15 | loss  5.61 | ppl   272.42\n",
      "| epoch 293 |    10/   10 batches | lr 0.21 | ms/batch 51.68 | loss  5.61 | ppl   273.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 293 | time:  0.84s | valid loss  5.83 | valid ppl   341.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 294 |     2/   10 batches | lr 0.21 | ms/batch 98.17 | loss  8.39 | ppl  4403.75\n",
      "| epoch 294 |     4/   10 batches | lr 0.21 | ms/batch 60.24 | loss  5.49 | ppl   241.79\n",
      "| epoch 294 |     6/   10 batches | lr 0.21 | ms/batch 68.55 | loss  5.66 | ppl   286.56\n",
      "| epoch 294 |     8/   10 batches | lr 0.21 | ms/batch 69.38 | loss  5.61 | ppl   272.32\n",
      "| epoch 294 |    10/   10 batches | lr 0.21 | ms/batch 50.51 | loss  5.61 | ppl   273.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 294 | time:  0.78s | valid loss  5.83 | valid ppl   341.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 295 |     2/   10 batches | lr 0.20 | ms/batch 101.77 | loss  8.39 | ppl  4408.08\n",
      "| epoch 295 |     4/   10 batches | lr 0.20 | ms/batch 67.00 | loss  5.49 | ppl   241.59\n",
      "| epoch 295 |     6/   10 batches | lr 0.20 | ms/batch 64.63 | loss  5.66 | ppl   286.55\n",
      "| epoch 295 |     8/   10 batches | lr 0.20 | ms/batch 61.01 | loss  5.61 | ppl   272.31\n",
      "| epoch 295 |    10/   10 batches | lr 0.20 | ms/batch 49.17 | loss  5.61 | ppl   273.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 295 | time:  0.78s | valid loss  5.83 | valid ppl   341.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 296 |     2/   10 batches | lr 0.20 | ms/batch 123.82 | loss  8.39 | ppl  4400.02\n",
      "| epoch 296 |     4/   10 batches | lr 0.20 | ms/batch 65.83 | loss  5.49 | ppl   241.56\n",
      "| epoch 296 |     6/   10 batches | lr 0.20 | ms/batch 68.64 | loss  5.66 | ppl   286.45\n",
      "| epoch 296 |     8/   10 batches | lr 0.20 | ms/batch 67.13 | loss  5.61 | ppl   272.02\n",
      "| epoch 296 |    10/   10 batches | lr 0.20 | ms/batch 51.17 | loss  5.61 | ppl   273.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 296 | time:  0.84s | valid loss  5.83 | valid ppl   341.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 297 |     2/   10 batches | lr 0.20 | ms/batch 102.81 | loss  8.39 | ppl  4407.40\n",
      "| epoch 297 |     4/   10 batches | lr 0.20 | ms/batch 67.96 | loss  5.49 | ppl   241.71\n",
      "| epoch 297 |     6/   10 batches | lr 0.20 | ms/batch 64.71 | loss  5.66 | ppl   286.47\n",
      "| epoch 297 |     8/   10 batches | lr 0.20 | ms/batch 63.00 | loss  5.61 | ppl   272.14\n",
      "| epoch 297 |    10/   10 batches | lr 0.20 | ms/batch 56.16 | loss  5.61 | ppl   273.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 297 | time:  0.80s | valid loss  5.83 | valid ppl   341.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 298 |     2/   10 batches | lr 0.19 | ms/batch 101.99 | loss  8.39 | ppl  4405.34\n",
      "| epoch 298 |     4/   10 batches | lr 0.19 | ms/batch 67.14 | loss  5.49 | ppl   241.60\n",
      "| epoch 298 |     6/   10 batches | lr 0.19 | ms/batch 65.89 | loss  5.66 | ppl   286.34\n",
      "| epoch 298 |     8/   10 batches | lr 0.19 | ms/batch 63.51 | loss  5.61 | ppl   272.24\n",
      "| epoch 298 |    10/   10 batches | lr 0.19 | ms/batch 50.61 | loss  5.61 | ppl   273.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 298 | time:  0.79s | valid loss  5.83 | valid ppl   341.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 299 |     2/   10 batches | lr 0.19 | ms/batch 124.93 | loss  8.39 | ppl  4409.04\n",
      "| epoch 299 |     4/   10 batches | lr 0.19 | ms/batch 70.16 | loss  5.49 | ppl   241.53\n",
      "| epoch 299 |     6/   10 batches | lr 0.19 | ms/batch 65.87 | loss  5.66 | ppl   286.39\n",
      "| epoch 299 |     8/   10 batches | lr 0.19 | ms/batch 65.50 | loss  5.61 | ppl   272.22\n",
      "| epoch 299 |    10/   10 batches | lr 0.19 | ms/batch 48.30 | loss  5.61 | ppl   273.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 299 | time:  0.84s | valid loss  5.83 | valid ppl   341.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 300 |     2/   10 batches | lr 0.19 | ms/batch 101.69 | loss  8.39 | ppl  4400.81\n",
      "| epoch 300 |     4/   10 batches | lr 0.19 | ms/batch 71.18 | loss  5.49 | ppl   241.65\n",
      "| epoch 300 |     6/   10 batches | lr 0.19 | ms/batch 70.94 | loss  5.66 | ppl   286.64\n",
      "| epoch 300 |     8/   10 batches | lr 0.19 | ms/batch 64.57 | loss  5.61 | ppl   272.19\n",
      "| epoch 300 |    10/   10 batches | lr 0.19 | ms/batch 51.66 | loss  5.61 | ppl   273.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 300 | time:  0.81s | valid loss  5.83 | valid ppl   341.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 301 |     2/   10 batches | lr 0.18 | ms/batch 98.75 | loss  8.39 | ppl  4408.52\n",
      "| epoch 301 |     4/   10 batches | lr 0.18 | ms/batch 68.51 | loss  5.49 | ppl   241.33\n",
      "| epoch 301 |     6/   10 batches | lr 0.18 | ms/batch 64.85 | loss  5.66 | ppl   286.47\n",
      "| epoch 301 |     8/   10 batches | lr 0.18 | ms/batch 67.51 | loss  5.61 | ppl   272.32\n",
      "| epoch 301 |    10/   10 batches | lr 0.18 | ms/batch 49.70 | loss  5.61 | ppl   273.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 301 | time:  0.79s | valid loss  5.83 | valid ppl   341.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 302 |     2/   10 batches | lr 0.18 | ms/batch 124.87 | loss  8.39 | ppl  4405.24\n",
      "| epoch 302 |     4/   10 batches | lr 0.18 | ms/batch 65.99 | loss  5.49 | ppl   241.57\n",
      "| epoch 302 |     6/   10 batches | lr 0.18 | ms/batch 69.00 | loss  5.66 | ppl   286.61\n",
      "| epoch 302 |     8/   10 batches | lr 0.18 | ms/batch 65.01 | loss  5.61 | ppl   272.30\n",
      "| epoch 302 |    10/   10 batches | lr 0.18 | ms/batch 50.99 | loss  5.61 | ppl   273.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 302 | time:  0.84s | valid loss  5.83 | valid ppl   341.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 303 |     2/   10 batches | lr 0.18 | ms/batch 98.51 | loss  8.39 | ppl  4405.00\n",
      "| epoch 303 |     4/   10 batches | lr 0.18 | ms/batch 58.65 | loss  5.49 | ppl   241.37\n",
      "| epoch 303 |     6/   10 batches | lr 0.18 | ms/batch 69.38 | loss  5.66 | ppl   286.37\n",
      "| epoch 303 |     8/   10 batches | lr 0.18 | ms/batch 66.71 | loss  5.61 | ppl   272.12\n",
      "| epoch 303 |    10/   10 batches | lr 0.18 | ms/batch 46.49 | loss  5.61 | ppl   273.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 303 | time:  0.77s | valid loss  5.83 | valid ppl   342.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 304 |     2/   10 batches | lr 0.17 | ms/batch 98.17 | loss  8.39 | ppl  4399.82\n",
      "| epoch 304 |     4/   10 batches | lr 0.17 | ms/batch 63.29 | loss  5.49 | ppl   241.45\n",
      "| epoch 304 |     6/   10 batches | lr 0.17 | ms/batch 64.30 | loss  5.66 | ppl   286.25\n",
      "| epoch 304 |     8/   10 batches | lr 0.17 | ms/batch 70.00 | loss  5.61 | ppl   272.01\n",
      "| epoch 304 |    10/   10 batches | lr 0.17 | ms/batch 51.13 | loss  5.61 | ppl   273.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 304 | time:  0.79s | valid loss  5.84 | valid ppl   342.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 305 |     2/   10 batches | lr 0.17 | ms/batch 128.58 | loss  8.39 | ppl  4406.17\n",
      "| epoch 305 |     4/   10 batches | lr 0.17 | ms/batch 65.13 | loss  5.49 | ppl   241.42\n",
      "| epoch 305 |     6/   10 batches | lr 0.17 | ms/batch 70.16 | loss  5.66 | ppl   286.41\n",
      "| epoch 305 |     8/   10 batches | lr 0.17 | ms/batch 62.00 | loss  5.61 | ppl   272.62\n",
      "| epoch 305 |    10/   10 batches | lr 0.17 | ms/batch 48.50 | loss  5.61 | ppl   273.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 305 | time:  0.84s | valid loss  5.84 | valid ppl   342.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 306 |     2/   10 batches | lr 0.17 | ms/batch 98.51 | loss  8.39 | ppl  4399.00\n",
      "| epoch 306 |     4/   10 batches | lr 0.17 | ms/batch 68.85 | loss  5.49 | ppl   241.57\n",
      "| epoch 306 |     6/   10 batches | lr 0.17 | ms/batch 67.38 | loss  5.66 | ppl   286.79\n",
      "| epoch 306 |     8/   10 batches | lr 0.17 | ms/batch 68.14 | loss  5.61 | ppl   272.40\n",
      "| epoch 306 |    10/   10 batches | lr 0.17 | ms/batch 50.24 | loss  5.61 | ppl   273.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 306 | time:  0.80s | valid loss  5.84 | valid ppl   342.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 307 |     2/   10 batches | lr 0.16 | ms/batch 99.82 | loss  8.39 | ppl  4397.02\n",
      "| epoch 307 |     4/   10 batches | lr 0.16 | ms/batch 64.12 | loss  5.49 | ppl   241.38\n",
      "| epoch 307 |     6/   10 batches | lr 0.16 | ms/batch 65.50 | loss  5.66 | ppl   286.62\n",
      "| epoch 307 |     8/   10 batches | lr 0.16 | ms/batch 61.51 | loss  5.61 | ppl   272.30\n",
      "| epoch 307 |    10/   10 batches | lr 0.16 | ms/batch 49.99 | loss  5.61 | ppl   273.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 307 | time:  0.81s | valid loss  5.84 | valid ppl   342.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 308 |     2/   10 batches | lr 0.16 | ms/batch 105.50 | loss  8.39 | ppl  4406.07\n",
      "| epoch 308 |     4/   10 batches | lr 0.16 | ms/batch 66.99 | loss  5.49 | ppl   241.47\n",
      "| epoch 308 |     6/   10 batches | lr 0.16 | ms/batch 69.14 | loss  5.66 | ppl   286.47\n",
      "| epoch 308 |     8/   10 batches | lr 0.16 | ms/batch 67.80 | loss  5.61 | ppl   272.25\n",
      "| epoch 308 |    10/   10 batches | lr 0.16 | ms/batch 54.50 | loss  5.61 | ppl   273.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 308 | time:  0.85s | valid loss  5.84 | valid ppl   342.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 309 |     2/   10 batches | lr 0.16 | ms/batch 134.83 | loss  8.39 | ppl  4401.88\n",
      "| epoch 309 |     4/   10 batches | lr 0.16 | ms/batch 87.32 | loss  5.49 | ppl   241.51\n",
      "| epoch 309 |     6/   10 batches | lr 0.16 | ms/batch 86.83 | loss  5.66 | ppl   286.39\n",
      "| epoch 309 |     8/   10 batches | lr 0.16 | ms/batch 87.25 | loss  5.61 | ppl   272.01\n",
      "| epoch 309 |    10/   10 batches | lr 0.16 | ms/batch 71.75 | loss  5.61 | ppl   273.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 309 | time:  1.04s | valid loss  5.83 | valid ppl   342.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 310 |     2/   10 batches | lr 0.15 | ms/batch 134.43 | loss  8.39 | ppl  4402.43\n",
      "| epoch 310 |     4/   10 batches | lr 0.15 | ms/batch 97.26 | loss  5.49 | ppl   241.73\n",
      "| epoch 310 |     6/   10 batches | lr 0.15 | ms/batch 113.77 | loss  5.66 | ppl   286.47\n",
      "| epoch 310 |     8/   10 batches | lr 0.15 | ms/batch 85.71 | loss  5.61 | ppl   272.16\n",
      "| epoch 310 |    10/   10 batches | lr 0.15 | ms/batch 75.36 | loss  5.61 | ppl   273.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 310 | time:  1.15s | valid loss  5.83 | valid ppl   342.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 311 |     2/   10 batches | lr 0.15 | ms/batch 130.59 | loss  8.39 | ppl  4400.58\n",
      "| epoch 311 |     4/   10 batches | lr 0.15 | ms/batch 121.34 | loss  5.49 | ppl   241.44\n",
      "| epoch 311 |     6/   10 batches | lr 0.15 | ms/batch 88.64 | loss  5.66 | ppl   286.48\n",
      "| epoch 311 |     8/   10 batches | lr 0.15 | ms/batch 88.11 | loss  5.61 | ppl   272.52\n",
      "| epoch 311 |    10/   10 batches | lr 0.15 | ms/batch 64.31 | loss  5.61 | ppl   273.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 311 | time:  1.11s | valid loss  5.84 | valid ppl   342.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 312 |     2/   10 batches | lr 0.15 | ms/batch 131.66 | loss  8.39 | ppl  4399.15\n",
      "| epoch 312 |     4/   10 batches | lr 0.15 | ms/batch 79.07 | loss  5.49 | ppl   241.56\n",
      "| epoch 312 |     6/   10 batches | lr 0.15 | ms/batch 88.30 | loss  5.66 | ppl   286.44\n",
      "| epoch 312 |     8/   10 batches | lr 0.15 | ms/batch 84.90 | loss  5.61 | ppl   272.36\n",
      "| epoch 312 |    10/   10 batches | lr 0.15 | ms/batch 81.03 | loss  5.61 | ppl   273.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 312 | time:  1.03s | valid loss  5.83 | valid ppl   342.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 313 |     2/   10 batches | lr 0.14 | ms/batch 104.08 | loss  8.39 | ppl  4405.96\n",
      "| epoch 313 |     4/   10 batches | lr 0.14 | ms/batch 69.76 | loss  5.49 | ppl   241.59\n",
      "| epoch 313 |     6/   10 batches | lr 0.14 | ms/batch 70.16 | loss  5.66 | ppl   286.59\n",
      "| epoch 313 |     8/   10 batches | lr 0.14 | ms/batch 72.51 | loss  5.61 | ppl   272.41\n",
      "| epoch 313 |    10/   10 batches | lr 0.14 | ms/batch 51.25 | loss  5.61 | ppl   274.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 313 | time:  0.82s | valid loss  5.83 | valid ppl   342.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 314 |     2/   10 batches | lr 0.14 | ms/batch 100.95 | loss  8.39 | ppl  4404.82\n",
      "| epoch 314 |     4/   10 batches | lr 0.14 | ms/batch 64.44 | loss  5.49 | ppl   241.48\n",
      "| epoch 314 |     6/   10 batches | lr 0.14 | ms/batch 69.14 | loss  5.66 | ppl   286.36\n",
      "| epoch 314 |     8/   10 batches | lr 0.14 | ms/batch 65.72 | loss  5.61 | ppl   272.35\n",
      "| epoch 314 |    10/   10 batches | lr 0.14 | ms/batch 58.72 | loss  5.61 | ppl   273.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 314 | time:  0.81s | valid loss  5.83 | valid ppl   341.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 315 |     2/   10 batches | lr 0.14 | ms/batch 102.83 | loss  8.39 | ppl  4403.27\n",
      "| epoch 315 |     4/   10 batches | lr 0.14 | ms/batch 63.15 | loss  5.49 | ppl   241.70\n",
      "| epoch 315 |     6/   10 batches | lr 0.14 | ms/batch 83.06 | loss  5.66 | ppl   286.28\n",
      "| epoch 315 |     8/   10 batches | lr 0.14 | ms/batch 67.99 | loss  5.61 | ppl   272.10\n",
      "| epoch 315 |    10/   10 batches | lr 0.14 | ms/batch 51.85 | loss  5.61 | ppl   273.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 315 | time:  0.83s | valid loss  5.83 | valid ppl   342.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 316 |     2/   10 batches | lr 0.14 | ms/batch 109.71 | loss  8.39 | ppl  4406.26\n",
      "| epoch 316 |     4/   10 batches | lr 0.14 | ms/batch 69.00 | loss  5.49 | ppl   241.56\n",
      "| epoch 316 |     6/   10 batches | lr 0.14 | ms/batch 73.78 | loss  5.66 | ppl   286.49\n",
      "| epoch 316 |     8/   10 batches | lr 0.14 | ms/batch 68.98 | loss  5.61 | ppl   271.91\n",
      "| epoch 316 |    10/   10 batches | lr 0.14 | ms/batch 51.14 | loss  5.61 | ppl   273.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 316 | time:  0.85s | valid loss  5.83 | valid ppl   341.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 317 |     2/   10 batches | lr 0.14 | ms/batch 111.00 | loss  8.39 | ppl  4405.51\n",
      "| epoch 317 |     4/   10 batches | lr 0.14 | ms/batch 74.27 | loss  5.49 | ppl   241.57\n",
      "| epoch 317 |     6/   10 batches | lr 0.14 | ms/batch 75.63 | loss  5.66 | ppl   286.31\n",
      "| epoch 317 |     8/   10 batches | lr 0.14 | ms/batch 69.72 | loss  5.61 | ppl   272.39\n",
      "| epoch 317 |    10/   10 batches | lr 0.14 | ms/batch 56.00 | loss  5.61 | ppl   273.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 317 | time:  0.89s | valid loss  5.83 | valid ppl   341.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 318 |     2/   10 batches | lr 0.14 | ms/batch 149.17 | loss  8.39 | ppl  4402.83\n",
      "| epoch 318 |     4/   10 batches | lr 0.14 | ms/batch 81.92 | loss  5.49 | ppl   241.67\n",
      "| epoch 318 |     6/   10 batches | lr 0.14 | ms/batch 79.50 | loss  5.66 | ppl   286.43\n",
      "| epoch 318 |     8/   10 batches | lr 0.14 | ms/batch 75.22 | loss  5.61 | ppl   272.19\n",
      "| epoch 318 |    10/   10 batches | lr 0.14 | ms/batch 60.64 | loss  5.61 | ppl   273.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 318 | time:  1.00s | valid loss  5.83 | valid ppl   341.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 319 |     2/   10 batches | lr 0.13 | ms/batch 109.36 | loss  8.39 | ppl  4403.22\n",
      "| epoch 319 |     4/   10 batches | lr 0.13 | ms/batch 72.28 | loss  5.49 | ppl   241.60\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[231], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      8\u001b[0m     epoch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 9\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m evaluate(model, val_data)\n\u001b[0;32m     11\u001b[0m     val_ppl \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mexp(val_loss)\n",
      "Cell \u001b[1;32mIn[230], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, train_data\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, bptt)):\n\u001b[0;32m     14\u001b[0m     data, targets \u001b[38;5;241m=\u001b[39m get_batch(train_data, i)\n\u001b[1;32m---> 15\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     output_flat \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, ntokens)\n\u001b[0;32m     17\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output_flat, targets)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[190], line 38\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[1;34m(self, src, src_mask)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m    Unmasked positions are filled with float(0.0).\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     src_mask \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mTransformer\u001b[38;5;241m.\u001b[39mgenerate_square_subsequent_mask(\u001b[38;5;28mlen\u001b[39m(src))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 38\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(output)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\transformer.py:391\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    388\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 391\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[0;32m    394\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\transformer.py:715\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[1;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    714\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, src_mask, src_key_padding_mask, is_causal\u001b[38;5;241m=\u001b[39mis_causal))\n\u001b[1;32m--> 715\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ff_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\normalization.py:201\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\functional.py:2546\u001b[0m, in \u001b[0;36mlayer_norm\u001b[1;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[0;32m   2542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[0;32m   2543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2544\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[0;32m   2545\u001b[0m     )\n\u001b[1;32m-> 2546\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs = 500\n",
    "\n",
    "with TemporaryDirectory() as tempdir:\n",
    "    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(model)\n",
    "        val_loss = evaluate(model, val_data)\n",
    "        val_ppl = math.exp(val_loss)\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "        print('-' * 89)\n",
    "        print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "            f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "        print('-' * 89)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "        scheduler.step()\n",
    "    model.load_state_dict(torch.load(best_model_params_path)) # load best model states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like this phone\n",
      "['i', 'like', 'this', 'phone']\n",
      "tensor([[ 3, 56,  8, 13]])\n",
      "torch.Size([1, 4])\n",
      "torch.int64\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 4, 1370])\n",
      "torch.Size([4, 1370])\n",
      "tensor([[-1.7706,  5.8401,  5.1357,  ..., -0.6910, -0.6759, -0.6924],\n",
      "        [-1.7783,  5.8188,  5.1298,  ..., -0.6968, -0.6570, -0.6561],\n",
      "        [-1.7639,  5.8071,  5.1173,  ..., -0.6922, -0.6557, -0.6722],\n",
      "        [-1.7778,  5.8450,  5.1359,  ..., -0.6988, -0.6747, -0.6756]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "inp = \"I like this phone\"\n",
    "tokens = tokenizer(inp)\n",
    "embed = torch.tensor([vocab(tokens)], dtype=torch.long)\n",
    "print(inp)\n",
    "print(tokens)\n",
    "print(embed)\n",
    "print(embed.shape)\n",
    "print(embed.dtype)\n",
    "print(type(embed))\n",
    "output = model(embed)\n",
    "print(output.shape)\n",
    "output_flat = output.view(-1, ntokens)\n",
    "print(output_flat.shape)\n",
    "print(output_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFF(nn.Module):\n",
    "    def __init__(self,vocab_sz):\n",
    "        super().__init__()\n",
    "        self.linear1=nn.Linear(vocab_sz,100)\n",
    "        self.linear2=nn.Linear(100,10)\n",
    "        self.linear3=nn.Linear(10,2)\n",
    "        self.sm = nn.Softmax()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.sm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1370])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Size([1, 4, 1370])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4485, 0.0504, 0.9616, 0.9986, 0.1029, 0.8820, 0.9473, 0.5201],\n",
      "        [0.4741, 0.2143, 0.6585, 0.6786, 0.6870, 0.8034, 0.9179, 0.7555],\n",
      "        [0.5861, 0.4257, 0.4761, 0.5964, 0.7848, 0.9654, 0.9058, 0.8898],\n",
      "        [0.8389, 0.8983, 0.1311, 0.3281, 0.8423, 0.9665, 0.7154, 0.9933]])\n",
      "tensor([0.4485, 0.0504, 0.9616, 0.9986, 0.1029, 0.8820, 0.9473, 0.5201])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((4,8))\n",
    "print(x)\n",
    "x = x[0,:]\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this allows the possibility of double booking for the same date and time after the first.'\n",
      " 'my sister has one also and she loves it.'\n",
      " \"the one big drawback of the mp player is that the buttons on the phone's front cover that let you pause and skip songs lock out after a few seconds.\"\n",
      " 'the cutouts and buttons are placed perfectly.'\n",
      " 'this is definitely a must have if your state does not allow cell phone usage while driving.'\n",
      " 'these are fabulous!' 'nice sound.'\n",
      " \"i can't use this case because the smell is disgusting.\"\n",
      " 'i really like this product over the motorola because it is allot clearer on the ear piece and the mic.'\n",
      " 'fast service.' 'i found this product to be waaay too big.'\n",
      " \"it plays louder than any other speaker of this size; the price is so low that most would think the quality is lacking, however, it's not.\"\n",
      " 'no buyers remorse on this one!.'\n",
      " 'i had to go to a store and bought a new nokia phone which is working great.'\n",
      " 'poor quality and service.'\n",
      " 'the worst piece of crap ever along with the verizon customer service.'\n",
      " \"no shifting, no bubbling, no peeling, not even a scratch, nothing!i couldn't be more happier with my new one for the droid.\"\n",
      " 'same problem as others have mentioned.'\n",
      " 'this results in the phone being either stuck at max volume or mute.'\n",
      " 'i got the aluminum case for my new palm vx and it worked really well--it has protected my handheld perfectly so far.'\n",
      " 'i am sorry i made this purchase.'\n",
      " 'problem is that the ear loops are made of weak material and break easily.'\n",
      " 'iam very pleased with my purchase.'\n",
      " 'i have had mine for about a year and this christmas i bought some for the rest of the family.'\n",
      " \"essentially you can forget microsoft's tech support.\"\n",
      " \"it's not what it says it is.\" 'battery life is also great!'\n",
      " 'i am very impressed with this headset from plantronics.'\n",
      " 'i like the fact that it rests lightly against your ear, rather than inside.'\n",
      " 'it is cheap, and it feel and look just as cheap.'\n",
      " 'plug was the wrong size.' \"don't bother - go to the store.\"\n",
      " 'piece of trash.' 'jabra ear gels \"r\" the best!.'\n",
      " 'cheap but hey it works.. was pleasantly suprised given the low cost of this item.'\n",
      " \"all the other cases i've tried normally fall apart after a few months but this one seems to be in for the long haul.\"\n",
      " 'also its slim enough to fit into my alarm clock docking station without removing the case.'\n",
      " 'needless to say, i wasted my money.' 'razr battery - good buy.'\n",
      " 'excellent dual-purpose headset.'\n",
      " 'buyer beware, you could flush money right down the toilet.'\n",
      " 'the charger worked for about a week and then completely stopped charging my phone.'\n",
      " 'my only complaint is the standard sound volume is a little low even when turned up to (of )'\n",
      " 'it is very comfortable to wear as well, which is probably the most important aspect about using a case.'\n",
      " 'best of all is the rotating feature, very helpful.'\n",
      " \"also the area where my unit broke).- i'm not too fond of the magnetic strap.\"\n",
      " 'magical help.' \"better than you'd expect.\" 'i love this thing!'\n",
      " 'car charger as well as ac charger are included to make sure you never run out of juice.highy recommended'\n",
      " 'worked perfectly!'\n",
      " 'this is a simple little phone to use, but the breakage is unacceptible.'\n",
      " 'very pleased with this headset.'\n",
      " 'the nano stated it.my son was dissapointed.'\n",
      " 'it is the best charger i have seen on the market yet.'\n",
      " \"i'm really disappointed all i have now is a charger that doesn't work.\"\n",
      " 'nice solid keyboard.'\n",
      " 'it is light, easy to use, and has very clear reception and transmission.'\n",
      " 'last time buying from you.' \"you won't regret it!\"\n",
      " 'it works great with a car charger, especially if you cannot plug in two adapters at the same time.'\n",
      " 'you also cannot take pictures with it in the case because the lense is covered.'\n",
      " 'too bad you have to pay up to $$$ a month for the service!'\n",
      " 'but, in any case, the best part is, you can download these pictures to your laptop using ir, or even send pictures from your laptop to the phone.'\n",
      " 'but it is great, i would really recommend it'\n",
      " 'clear skype calls, long battery life, long range.'\n",
      " 'i am also very happy with the price.'\n",
      " 'the mic there is a joke, and the volume is quite low.'\n",
      " 'works like a charm.. works as advertised.'\n",
      " 'att is not clear, sound is very distorted and you have to yell when you talk.'\n",
      " 'it is easy to turn on and off when you are in the car and the volume controls are quite accessable.'\n",
      " 'perfect for the ps.'\n",
      " \"i got this phone on reccomendation from a relative and i'm glad i did.\"\n",
      " 'not as good as i had hoped.'\n",
      " 'i would definitely recommend the jabra btv for those who are looking for comfort, clarity and a great price!'\n",
      " 'disappointing.' 'big disappointment with calendar sync.'\n",
      " 'when i placed my treo into the case, not only was it not snug, but there was a lot of extra room on the sides.'\n",
      " \"i didn't want the clip going over the top of my ear, causing discomfort.\"\n",
      " 'the phone takes forever to charge like  to  hours literally.'\n",
      " 'i really wanted the plantronics  to be the right one, but it has too many issues for me.the good'\n",
      " 'you can not answer calls with the unit, never worked once!'\n",
      " 'very disappointed in accessoryone.'\n",
      " \"it's uncomfortable and the sound quality is quite poor compared with the phone (razr) or with my previous wired headset (that plugged into an lg).\"\n",
      " 'the text messaging feature is really tricky to use.' 'happy so far!.'\n",
      " 'unfortunately it will not recharge my iphone s, despite connecting it from multiple power sources (imac, external battery, wall outlet, etc).'\n",
      " 'very well made and fits my surefire gx perfectly.'\n",
      " 'the reception has been generally good.'\n",
      " 'battery lasts only a few hours.'\n",
      " 'however, the ear pads come off easily and after only one week i lost one.'\n",
      " 'dont buy it.' 'warning - stay away.'\n",
      " 'i have to jiggle the plug to get it to line up right to get decent volume.'\n",
      " 'crisp and clear.'\n",
      " 'the volume for the ringer is real good (you have choices how loud).'\n",
      " 'this is simply the best bluetooth headset for sound quality!'\n",
      " 'i would highly recommend this.'\n",
      " 'utter crap.. sound quality is terrible.' 'i was hoping for more.'\n",
      " 'i would advise to not purchase this item it never worked very well.'\n",
      " 'the bose noise cancelling is amazing, which is very important for a nyc commuter.'\n",
      " 'the reception is excellent!' 'not enough volume.'\n",
      " \"gets a signal when other verizon phones won't.\"\n",
      " 'steer clear of this product and go with the genuine palm replacementr pens, which come in a three-pack.'\n",
      " 'battery has no life.'\n",
      " 'i am very impressed with the job that motorola did on the sturdiness of this phone.'\n",
      " 'the phone is sturdy and waterproof.'\n",
      " 'a lot of websites have been rating this a very good phone and so do i.'\n",
      " 'great product, fast shipping!.'\n",
      " 'i like design and look of jabra behing the ear headsets and  is pretty comfortible to wear  hours a day without pain in the ear.'\n",
      " 'poor talk time performance.'\n",
      " 'this product is very high quality chinese crap!!!!!!'\n",
      " 'it was quite comfortable in the ear.' 'lasted one day and then blew up.'\n",
      " 'and i just love the colors!'\n",
      " \"i've bought $ wired headphones that sound better than these.\"\n",
      " 'it has a great camera thats mp, and the pics are nice and clear with great picture quality.'\n",
      " 'it feels poorly constructed, the menus are difficult to navigate, and the buttons are so recessed that it is difficult to push them.'\n",
      " 'does not work.' 'they are so cool!' \"it's pretty easy.\"\n",
      " 'returned  hours later.'\n",
      " \"it's so stupid to have to keep buying new chargers, car chargers, cradles, headphones and car kits every time a new phone comes out.\"\n",
      " 'the voice recognition thru the handset is excellent.'\n",
      " 'poor construction.'\n",
      " 'i ended up sliding it on the edge of my pants or back pockets instead.'\n",
      " 'works great!.'\n",
      " 'overall, i am psyched to have a phone which has all my appointments and contacts in and gets great reception.'\n",
      " 'the noise shield is incrediable.'\n",
      " 'my -year old nokia  from tracfone holds the charge a lot better than this.'\n",
      " 'this is a great product..... sure beats using your fingers!.'\n",
      " 'the internet access was fine, it the rare instance that it worked.'\n",
      " \"these headphones were a great find - and i think they are perhaps the best purchase i've made in the last several years - seriously.\"\n",
      " 'i purchased this and within  days it was no longer working!!!!!!!!!'\n",
      " 'and none of the tones is acceptable.'\n",
      " 'i have had problems wit hit dropping signal and more.'\n",
      " 'it is light, has plenty of battery capacity, and is very confortable to wear for somewhat extended periods of time.'\n",
      " \"i've had this bluetoooth headset for some time now and still not comfortable with the way it fits on the ear.\"\n",
      " 'all it took was one drop from about  inches above the kitchen counter and it was cracked.i am not impressed and i am not laughing.'\n",
      " 'i love this phone!.'\n",
      " 'the color is even prettier than i thought it would be, and the graphics are incredibly sharp.'\n",
      " 'it definitely was not as good as my s.' 'painful on the ear.'\n",
      " 'cant get the software to work with my computer.'\n",
      " 'the only thing that i think could improve is the sound leaks out from the headset.'\n",
      " 'great for the jawbone.'\n",
      " 'it holds a charge for a long time, is reasonably comfortable under long-wearing conditions and the quality of sound is tremendous.'\n",
      " 'do not buy if you want to use the holster.'\n",
      " 'i love all the features and form factor.'\n",
      " 'looks good in the picture, but this case was a huge disappointment!!'\n",
      " 'the headsets are easy to use and everyone loves them.'\n",
      " \"i'm trying to return it for a refund.\" 'love this product.'\n",
      " 'they work about  weeks then break.'\n",
      " 'talk about useless customer service.' 'the battery works great!'\n",
      " 'much better than the hard plastic cases.'\n",
      " 'the case is great and works fine with the .'\n",
      " 'i does not maintain a connection with the computer while it is on my lap.'\n",
      " 'i have always used corded headsets and the freedom from the wireless is very helpful.'\n",
      " 'i wish i could return the unit and get back my money.' 'what a waste.'\n",
      " \"doesn't do the job.\" 'those phones are working just fine now.'\n",
      " 'fantastic buy and will get again for whatever my next phone is'\n",
      " 'the update procedure is difficult and cumbersome.'\n",
      " 'a piece of junk that broke after being on my phone for  days!!!'\n",
      " 'an awesome new look for fall !.'\n",
      " 'it lasts less than o minutes, if i actually try to use the phone.my wife has the same phone with the same problem.'\n",
      " 'i have purchased these for both family and friends, and all enjoy their clarity and ease of use'\n",
      " \"not loud enough and doesn't turn on like it should.\"\n",
      " 'this is essentially a communications tool that does not communicate.'\n",
      " 'in addition it feels &amp; looks as if the phone is all lightweight cheap plastic.'\n",
      " 'poor reliability.' 'nice case, feels good in your hands.'\n",
      " 'works great!.' 'restored my phone to like new performance.'\n",
      " 'how can that be?the audio quality is poor.'\n",
      " 'just reading on the specs alone makes you say wow.' 'very comfortable.'\n",
      " 'comfortable, nice range, good battery life.' '(it works!)'\n",
      " 'but now that it is \"out of warranty\" the same problems reoccure.bottom line... put your money somewhere else... cingular will not support it.'\n",
      " 'worthless product.'\n",
      " \"i have read other's reviews here but i haven't had any problem with it.\"\n",
      " \"this is by far the worst purchase i've made on amazon.\"\n",
      " 'nice quality build, unlike some cheap s*** out there.'\n",
      " 'simple, lightweight and great fit.' 'very wind-resistant.'\n",
      " 'i did not bother contacting the company for few dollar product but i learned the lesson that i should not have bought this form online anyway.'\n",
      " 'stay away from this store, be careful.'\n",
      " \"i exchanged the sony ericson za for this and i'm pretty happy with that decision.\"\n",
      " \"also, the phone doesn't seem to accept anything except cbr mps, preferably ripped by windows media player.\"\n",
      " 'the look of it is very sharp and the screen is nice and clear, with great graphics.'\n",
      " 'the only thing that disappoint me is the infra red port (irda).'\n",
      " 'adapter does not provide enough charging current.'\n",
      " 'it is super charged up for use as a small hybrid palmtop/camera/cellphone, and excels in those roles.'\n",
      " 'and the sound quality is great.'\n",
      " 'pros:-good camera - very nice pictures , also has cool styles like black and white, and more.'\n",
      " 'great for ipods too.' \"i'd like to return it.\"\n",
      " \"still waiting...... i'm sure this item would work well.. if i ever recieve it!\"\n",
      " \"but it does get better reception and clarity than any phone i've had before.\"\n",
      " 'excellent hands free tool.' 'good case!.'\n",
      " 'i am pairing this with my iphone, and i could not be happier with it so far.'\n",
      " 'everything worked on the first try.the device was certainly engineered in a clever way and the construction feels good.'\n",
      " '# it works - # it is comfortable.' 'logitech bluetooth headset is a !.'\n",
      " 'works good.' 'horrible, had to switch  times.'\n",
      " \"i went on motorola's website and followed all directions, but could not get it to pair again.\"\n",
      " 'not nearly as good looking as the amazon picture makes it look.'\n",
      " 'great hands free device.' 'price is good too.' 'great phone!.'\n",
      " 'even in my bmw  series which is fairly quiet, i have trouble hearing what the other person is saying.'\n",
      " 'disappointed with battery.'\n",
      " 'if you like a loud buzzing to override all your conversations, then this phone is for you!'\n",
      " 'i would not recommend this item to anyone.'\n",
      " 'as i said above....pretty useless!'\n",
      " \"i got this phone around the end of may and i'm completely unhappy with it.\"\n",
      " 'do not purchase this phone.'\n",
      " 'do not buy for d...wrongly advertised for d.'\n",
      " 'this case seems well made.' 'i would have given no star if i was able.'\n",
      " 'absolutel junk.' ' thumbs up to this seller'\n",
      " 'i have only had it for a few weeks, but so far, so good.'\n",
      " 'does not fit.'\n",
      " 'you have to hold the phone at a particular angle for the other party to hear you clearly.'\n",
      " '$ down the drain.'\n",
      " 'a usable keyboard actually turns a pda into a real-world useful machine instead of just a neat gadget.'\n",
      " 'very displeased.' 'gets the job done.' 'battery is terrible.'\n",
      " 'i came over from verizon because cingulair has nicer cell phones.... the first thing i noticed was the really bad service.'\n",
      " 'blue ant is easy to use.' 'very satisifed with that.'\n",
      " 'the volume switch rocketed out of the unit to a destination unknown.'\n",
      " 'it has been a winner for us.'\n",
      " 'if the two were seperated by a mere + ft i started to notice excessive static and garbled sound from the headset.'\n",
      " 'this item is fantastic and works perfectly!' 'very disappointing.'\n",
      " 'it did not work in my cell phone plug i am very up set with the charger!.'\n",
      " 'who in their right mind is gonna buy this battery?.'\n",
      " 'one of my favorite purchases ever.'\n",
      " 'the eargels channel the sound directly into your ear and seem to increase the sound volume and clarity.'\n",
      " 'the phone was unusable and was not new.'\n",
      " 'sprint - terrible customer service.' 'the phone gets extremely hot!'\n",
      " 'however, the keypads are so tinny that i sometimes reach the wrong buttons.'\n",
      " 'not good enough for the price.' 'great phone.' 'good item, low price.'\n",
      " 'the pairing of the two devices was so easy it barely took a couple minutes before i started making calls with the voice dialing feature.'\n",
      " 'i have a verizon lg phone and they work well together, good reception and range that exceeds  feet line of sight.'\n",
      " 'were jerks on the phone.' 'bad fit, way too big.' 'so far so good!.'\n",
      " 'design flaw?.' 'excellent product for the price.'\n",
      " 'it was an inexpensive piece, but i would still have expected better quality.'\n",
      " 'the \".\" mega pixel camera, being a part of a phone, is reasonably good.'\n",
      " 'i use this product in a motor control center where there is a lot of high voltage humming from the equipment, and it works great!'\n",
      " \"i've dropped my phone more times than i can say, even on concrete and my phone is still great (knock on wood!).\"\n",
      " 'phone now holds charge like it did when it was new.'\n",
      " 'worst customer service ever.' \"couldn't figure it out\"\n",
      " 'excellent service!!!!!!!!.' 'great phone!.'\n",
      " 'you get extra minutes so that you can carry out the call and not get cut off.\"'\n",
      " '!i definitly recommend!!' 'now i know that i made a wise decision.'\n",
      " 'none of the three sizes they sent with the headset would stay in my ears.'\n",
      " 'its extremely slow and takes forever to do anything with it.'\n",
      " 'i want my money back.'\n",
      " 'i love this phone , it is very handy and has a lot of features .'\n",
      " 'thanks again to amazon for having the things i need for a good price!'\n",
      " 'reception is terrible and full of static.'\n",
      " \"the pleather case doesn't fit.\" 'would recommend this item.'\n",
      " 'the case is a flimsy piece of plastic and has no front or side protection whatsoever.'\n",
      " 'so i had to take the battery out of the phone put it all back together and then restart it.'\n",
      " \"i only used it two days, and it wasn't always easy to hear with.\"\n",
      " 'very happy with this product.'\n",
      " 'i bought this to use with my kindle fire and absolutely loved it!'\n",
      " 'in the span of an hour, i had two people exclaim \"whoa - is that the new phone on tv?!?'\n",
      " 'it was a great phone.'\n",
      " \"it's fits like a glove and is strong, secure, and durable.\"\n",
      " 'no additional ear gels provided, and no instructions whatsoever.'\n",
      " 'this fixes all the problems.'\n",
      " 'you need at least  mins to get to your phone book from the time you first turn on the phone.battery life is short.'\n",
      " 'not a good bargain.' 'wi is just superb.'\n",
      " 'it fits comfortably in either ear, the sound is clear and loud, and the charge lasts a couple of days.'\n",
      " \"i don't like this nokia either.\" 'the delivery was on time.'\n",
      " 'a must study for anyone interested in the \"worst sins\" of industrial design.'\n",
      " 'i purcashed this for the car charger and it does not work.'\n",
      " \"i'm happy about this purchase- good quality and low price.\"\n",
      " 'soyo technology sucks.' 'very dissapointing performance.'\n",
      " 'i am more than happy with this product.' 'worked great!.'\n",
      " 'portable and it works.'\n",
      " 'the earpiece on this is too large or too heavy...it keeps falling out of my ear.'\n",
      " 'i have been very satisfied with this cell phone from day one.'\n",
      " 'just does not work.' 'exactly what i wanted.' 'that company is a joke.'\n",
      " 'there was so much hype over this phone that i assumed it was the best, my mistake.'\n",
      " 'oh and i forgot to also mention the weird color effect it has on your phone.'\n",
      " 'if you hate earbugs, avoid this phone by all means.'\n",
      " 'thank you for wasting my money.' 'great value.'\n",
      " 'i was very excited to get this headset because i thought it was really cute.'\n",
      " 'waste of  bucks.' 'easy to pair with my samsung cell.'\n",
      " 'if there is a wind, it is completely useless.' 'worst customer service.'\n",
      " \"couldn't use the unit with sunglasses, not good in texas!\"\n",
      " 'i even fully charged it before i went to bed and turned off blue tooth and wi-fi and noticed that it only had  % left in the morning.'\n",
      " 'phone is sturdy as all nokia bar phones are.'\n",
      " 'unfortunately it did not work.'\n",
      " 'this phone is very fast with sending any kind of messages and web browsing is significantly faster than previous phones i have used.'\n",
      " 'unfortunately the ability to actually know you are receiving a call is a rather important feature and this phone is pitiful in that respect.'\n",
      " 'a pretty good product.'\n",
      " \"it didn't work, people can not hear me when i talk.\" 'great audio!.'\n",
      " 'a week later after i activated it, it suddenly died.'\n",
      " \"i can hear while i'm driving in the car, and usually don't even have to put it on it's loudest setting.\"\n",
      " 'you never know if you pushed it hard enough or the right number of times for the function you want or not.'\n",
      " \"there's a horrible tick sound in the background on all my calls that i have never experienced before.\"\n",
      " \"don't waste your money!.\" 'great charger.'\n",
      " 'the jabra eargels fit my ears very well.'\n",
      " 'you need two hands to operate the screen.this software interface is decade old and cannot compete with new software designs.'\n",
      " 'bluetooth does not work, phone locks up, screens just flash up and now it just makes calls randomly while in my pocket locked.'\n",
      " 'it only recognizes the phone as its storage device.'\n",
      " 'it is a joy to use.'\n",
      " 'i have used several phone in two years, but this one is the best.'\n",
      " 'integrated seamlessly with the motorola razr phone.'\n",
      " 'arrived quickly and much less expensive than others being sold.'\n",
      " 'works well.'\n",
      " 'i ordered this for sony ericsson wi but i think it only worked once (thats when i first used it).'\n",
      " \"very clear, quality sound and you don't have to mess with the sound on your ipod since you have the sound buttons on the headset.\"\n",
      " \"this phone might well be the worst i've ever had in any brand.\"\n",
      " '* comes with a strong light that you can use to light up your camera shots, and even flash sos signals (seriously!'\n",
      " 'does not work for listening to music with the cingular .'\n",
      " 'after charging overnight, these batteries work great.'\n",
      " 'saggy, floppy piece of junk.' 'freezes frequently.'\n",
      " 'the picture resolution is far below what other comparably-priced phones are offering today.'\n",
      " 'it always cuts out and makes a beep beep beep sound then says signal failed.'\n",
      " \"another note about this phone's appearance is that it really looks rather bland, especially in the all black model.\"\n",
      " 'they do not care about the consumer one bit.' 'great bluetooth!.'\n",
      " 'when it opens, the battery connection is broken and the device is turned off.'\n",
      " 'highly recommend for any one who has a blue tooth phone.'\n",
      " 'i had verizon  years ago and really liked their service.'\n",
      " 'worth every penny.' 'everything about this product is wrong.first'\n",
      " 'if you have several dozen or several hundred contacts, then imagine the fun of sending each of them one by one.'\n",
      " 'i was amazed at the quick arrival of the two original lg cell phone batteries and and at a fraction of the price.'\n",
      " 'so i basically threw my money out the window for nothing.'\n",
      " 'great product and price.'\n",
      " \"my phone doesn't slide around my car now and the grip prevents my phone from slipping out of my hand.\"\n",
      " 'if i take a picture, the battery drops a bar, and starts beeping, letting me know its dieing.'\n",
      " 'clear crisp sound.'\n",
      " 'i received my supposedly new motorola  and apparently there was not a match between the phone and the charger.'\n",
      " 'good price.'\n",
      " 'i own a jabra earset and was very happy with it, but the sound quality, especially outgoing, on this is better.'\n",
      " 'after receiving and using the product for just  days it broke.'\n",
      " 'good transmit audio.' 'someone shouldve invented this sooner.'\n",
      " 'good protection and does not make phone too bulky.'\n",
      " 'for a product that costs as much as this one does, i expect it to work far better and with greater ease than this thing does.'\n",
      " 'excellent starter wireless headset.'\n",
      " 'i did not have any problem with this item and would order it again if needed.'\n",
      " 'bt battery junk!.' 'the commercials are the most misleading.'\n",
      " 'not a good item.. it worked for a while then started having problems in my auto reverse tape player.'\n",
      " 'worst software ever used.... if i could give this zero stars i would.'\n",
      " \"if you don't find it, too bad, as again the unit comes with one earpad only.i would not recommend this to anyone.\"\n",
      " 'worst phone ever.'\n",
      " 'the tracfonewebsite is user friendly and makes it easy toactivate, etc.'\n",
      " 'the sound quality for the device is unacceptable.unless you are in a really quiet area, you almost hear nothing.'\n",
      " 'this is infuriating.' 'for the price this was a great deal.'\n",
      " 'nice leather.' 'i love the ringtones because they are so upbeat!'\n",
      " 'great headset, very impressed - h.' 'great phone.'\n",
      " 'the replacement died in a few weeks.'\n",
      " 'also, if your phone is dropped, this case is not going to save it, specially when dropped face down.'\n",
      " 'does everything it should and more.'\n",
      " 'this is so embarassing and also my ears hurt if i try to push the ear plug into my ear.'\n",
      " \"i posted more detailed comments under the grey or black phone, but i have the fire red and it's a great color!\"\n",
      " 'incredible!.' 't-mobile has the best customer service anywhere.'\n",
      " \"don't waste your money and time.\" 'not impressed.'\n",
      " 'yet plantronincs continues to use the same flawed charger design.'\n",
      " 'very prompt service.' 'keep up the good work amazon!!'\n",
      " 'finally, after three or four times the spring of the latch broke and i could not use it any longer on the visor.'\n",
      " 'i might have gotten a defect, but i would not risk buying it again because of the built quality alone.'\n",
      " 'headset works great & was packaged nicely to avoid any damage.'\n",
      " 'great sound and service.' 'the igo chargers and tips are really great.'\n",
      " \"it's really easy.\"\n",
      " 'great it was new packaged nice works good, no problems and it came in less time then i expected!!!!'\n",
      " 'beautiful styling though.' 'the sound quality is excellent as well.'\n",
      " \"don't make the same mistake that i did and please don't buy this phone.\"\n",
      " 'if you plan to use this in a car forget about it.'\n",
      " 'i bought these hoping i could make my bluetooth headset fit better but these things made it impossible to wear.'\n",
      " 'well im satisfied.' 'this is a beautiful phone.'\n",
      " 'the phone loads super!' 'it fits my ear well and is comfortable on.'\n",
      " 'so there is no way for me to plug it in here in the us unless i go by a converter.'\n",
      " 'this is a great deal.'\n",
      " \"uncomfortable in the ear, don't use with lg vx (env).\"\n",
      " \"i tried talking real loud but shouting on the telephone gets old and i was still told it wasn't great.\"\n",
      " 'i am not impressed with this and i would not recommend this item to anyone.'\n",
      " 'i love my  headset.. my jabra bluetooth headset is great, the reception is very good and the ear piece is a comfortable fit.'\n",
      " 'i bought this phone as a replacement for my startac and have regretted it since.'\n",
      " 'not only did the software do a great job of this, i could also make my own ringtones form my existing cds without an internet connection.'\n",
      " \"all in all, i'd expected a better consumer experience from motorola.\"\n",
      " 'the plug did not work very well.' 'definitely a bargain.'\n",
      " 'so just beware.' 'very easy to use.'\n",
      " \"the range is very decent, i've been able to roam around my house with the phone in the living room with no reception/sound quality issues.\"\n",
      " 'the majority of the logitech earbud headsets failed.'\n",
      " 'excellent wallet type phone case.'\n",
      " 'i plugged it in only to find out not a darn thing worked.'\n",
      " 'fits comfortably, came with three sizes of earbud tips.'\n",
      " 'worked very well paired with a treo w and as a skype headset on my pc (using a usb bluetooth transceiver).'\n",
      " 'i checked everywhere and there is no feature for it which is really disappointing.'\n",
      " 'awesome device.' 'battery charge-life is quite long.'\n",
      " \"i've owned this phone for  months now and can say that it's the best mobile phone i've had.\"\n",
      " 'obviously they have a terrible customer service, so you get what you pay for.'\n",
      " \"this is the first phone i've had that has been so cheaply made.\"\n",
      " 'how stupid is that?' 'it also had a new problem.'\n",
      " 'tied to charger for conversations lasting more than  minutes.major problems!!'\n",
      " 'buyer--be very careful!!!!!.'\n",
      " 'the loudspeaker option is great, the bumpers with the lights is very ... appealing.'\n",
      " 'excellent!.' 'great product.' 'horrible, horrible protector.'\n",
      " 'please dont waste your money.' 'excellent product for the price.'\n",
      " 'these are certainly very comfortable and functionality is decent.'\n",
      " 'plan on ordering from them again and again.'\n",
      " \"i was looking for this headset for a long time and now that i've got it i couldn't be happier.\"\n",
      " 'camera color balance is awful.'\n",
      " 'it is practically useless and did not add any kind of boost to my reception after i bought it.'\n",
      " 'the plastic breaks really easy on this clip.' 'really good product.'\n",
      " 'very much disappointed with this company.'\n",
      " \"i've also had problems with the phone reading the memory card in which i always turn it on and then off again.\"\n",
      " 'buy a different phone - but not this.'\n",
      " 'its the best headset i have used.' 'nice design and quality.'\n",
      " 'authentic leather with nice shine and comfort .i recommend you this case !!'\n",
      " 'its a little geeky but i think thats its sex on toast and it rocks and oozes sex right down to its battery embedded sleek stylish leather case.'\n",
      " 'performed awful -- muffled, tinny incoming sound and severe echo for those on the other end of the call.'\n",
      " \"unfortunately it's easy to accidentally activate them with the gentle-touch buttons if you accidentally touch the phone to your face while listening.\"\n",
      " 'basically the service was very bad.'\n",
      " 'great for using with your home stereo.'\n",
      " 'it dit not work most of the time with my nokia .'\n",
      " 'they have been around for years and carries the highest quality of anti-glare screen protector that i have found to date.'\n",
      " 'my headset works just peachy-keen.'\n",
      " 'if you are razr owner...you must have this!' 'defective crap.'\n",
      " 'all three broke within two months of use.' 'just what i wanted.'\n",
      " \"this is an excellent tool, especially when paired with your phone's auto-answer.\"\n",
      " 'works great.'\n",
      " 'i have to use the smallest earpieces provided, but it stays on pretty well.'\n",
      " 'their network coverage in los angeles is horrible.'\n",
      " \"i know that sounds funny, but to me it seemed like sketchy technology that wouldn't work well.well, this one works great.\"\n",
      " 'cumbersome design.'\n",
      " 'perhaps my phone is defective, but people cannot hear me when i use this.'\n",
      " 'new battery works great in phone.' 'best headset ever!!!.'\n",
      " 'item does not match picture.' 'the shipping time was also very quick!'\n",
      " 'very disappointed.' 'what a disappointment' 'worst ever.'\n",
      " 'great software for motorolas.'\n",
      " 'i find this inexcusable and so will probably be returning this phone and perhaps changing carriers.'\n",
      " 'i found the product to be easy to set up and use.'\n",
      " 'setup went very smoothly.' 'i wear glasses and it fits fine with them.'\n",
      " 'this was utterly confusing at first, which caused me to lose a couple of very, very important contacts.'\n",
      " \"i'm pleased.\"\n",
      " 'this pair of headphones is the worst that i have ever had sound-wise.'\n",
      " 'sending it back.'\n",
      " 'then i had to continue pairing it periodically since it somehow kept dropping.'\n",
      " 'i could not recommend these more.'\n",
      " 'my experience was terrible..... this was my fourth bluetooth headset, and while it was much more comfortable than my last jabra (which i hated!!!'\n",
      " 'works like a charm; it work the same as the one i got with the phone.'\n",
      " 'obviously there is a problem with the adhesive.'\n",
      " 'the buttons for on and off are bad.'\n",
      " 'that being said, for a phone, the camera is very nice; many cool effects to play with, and video is decent as well.'\n",
      " 'does not charge the cingular (att)  phone.' 'this pda rocks.'\n",
      " 'it is simple to use and i like it.'\n",
      " \"there's really nothing bad i can say about this headset.\"\n",
      " 'lightweight and works well.' 'awsome device works great!!.'\n",
      " \"if you are looking for a good quality motorola headset keep looking, this isn't it.\"\n",
      " 'good value, works fine - power via usb, car, or wall outlet.'\n",
      " 'the speaker is of low quality so as making the ring tones sound very cheap.'\n",
      " \"i'm still infatuated with this phone.\"\n",
      " 'it works fine on my motorola  cellphone, and a lot better than the worn-out battery did.'\n",
      " 'battery is holding up well.' \"first of all, it doesn't wear well.\"\n",
      " 'this little device has transformed my organizational capability and made my life a whole lot easier.'\n",
      " \"the instructions didn't explain that a microphone jack could be used.\"\n",
      " 'the mic is great.'\n",
      " 'the screen does get smudged easily because it touches your ear and face.'\n",
      " 'disappointed!.' 'i highly recommend this case.'\n",
      " \"this product is clearly not ready for prime time, i don't care how cool it looks, if you can't tell a call is coming in it's worthless.\"\n",
      " 'it seems completely secure, both holding on to my belt, and keeping the iphone inside.'\n",
      " 'i received my orders well within the shipping timeframe, everything was in good working order and overall, i am very excited to have this source.'\n",
      " 'i am very happy'\n",
      " \"i bought this battery with a coupon from amazon and i'm very happy with my purchase.\"\n",
      " 'bad reception.' 'great case and price!'\n",
      " 'i love being able to use one headset for both by land-line and cell.'\n",
      " 'i wasted my little money with this earpiece.'\n",
      " 'i got it because it was so small and adorable.'\n",
      " 'i have had tmobile service for  or  years now, and i am pretty happy with it.'\n",
      " 'every thing on phone work perfectly, she like it.'\n",
      " 'i love this bluetooth!'\n",
      " \"i've tried several different earpieces for my cell phone and this jabra one is the first one i've found that fits my ear comfortably.\"\n",
      " 'easy to use.'\n",
      " 'you could only take  videos at a time and the quality was very poor.'\n",
      " 'very good quality though'\n",
      " \"no ear loop needed, it's tiny and the sound is great.\"\n",
      " 'not good when wearing a hat or sunglasses.' 'better than expected.'\n",
      " 'buttons are too small.'\n",
      " 'all i can do is whine on the internet, so here it goes.the more i use the thing the less i like it.'\n",
      " ':-)oh, the charger seems to work fine.' 'lousy product.'\n",
      " 'the phone crashed completely and now i have to get it replaced.'\n",
      " 'if you like a loud buzzing to override all your conversations, then this phone is for you!'\n",
      " 'light weight, i hardly notice it is there.'\n",
      " 'any ring tone..overall good phone to buy..'\n",
      " 'case was more or less an extra that i originally put on but later discarded because it scratched my ear.'\n",
      " 'bluetooth range is good - a few days ago i left my phone in the trunk, got a call, and carried the conversation without a hitch.'\n",
      " 'the bottowm line...another worthless, cheap gimmick from sprint.'\n",
      " \"i still maintain that monkeys shouldn't make headphones, we just obviously don't share enough dna to copy the design over to humans.\"\n",
      " \"that's a huge design flaw (unless i'm not using it correctly, which i don't think is the case).\"\n",
      " 'i advise everyone do not be fooled!'\n",
      " 'i can barely ever hear on it and am constantly saying \"what?\"'\n",
      " 'motorola finally got the voice quality of a bluetooth headset right.'\n",
      " 'poor quality.' 'i am glad i purchased it.'\n",
      " 'rip off---- over charge shipping.'\n",
      " \"the keyboard is really worthwhile in usefulness and is sturdy enough i don't expect any problems.\"\n",
      " 'bad purchase.'\n",
      " 'this company charge me a restocking fee and still not given me my refund back.'\n",
      " 'i especially love the long battery life.'\n",
      " 'bought mainly for the charger, which broke soon after purchasing.'\n",
      " 'the ear buds only play music in one ear.'\n",
      " 'i highly recommend this modest priced cellular phone.'\n",
      " 'the battery runs down quickly.'\n",
      " 'product is useless, since it does not have enough charging current to charge the  cellphones i was planning to use it with.'\n",
      " \"my phone sounded ok ( not great - ok), but my wife's phone was almost totally unintelligible, she couldn't understand a word being said on it.\"\n",
      " 'the handsfree part works fine, but then the car tries to download the address book, and the treo reboots.overall, i still rate this device high.'\n",
      " 'after a year the battery went completely dead on my headset.'\n",
      " 'i love the look and feel of samsung flipphones.'\n",
      " 'it fits so securely that the ear hook does not even need to be used and the sound is better directed through your ear canal.'\n",
      " 'very cheap plastic, creaks like an old wooden floor.'\n",
      " \"it has everything i need and i couldn't ask for more.\"\n",
      " 'so i bought about  of these and saved alot of money.'\n",
      " 'the reception through this headset is excellent.'\n",
      " 'the ngage is still lacking in earbuds.' 'great product for the price!.'\n",
      " 'first off the reception sucks, i have never had more than  bars, ever.'\n",
      " 'this device is great in several situations:.)'\n",
      " 'very good product, well made.'\n",
      " 'as an earlier review noted, plug in this charger and nothing happens.'\n",
      " 'excellent sound quality.'\n",
      " \"it's kind of embarrassing to use because of how it looks and mostly it's embarrassing how child-like the company is.\"\n",
      " \"makes it easier to keep up with my bluetooth when i'm not wearing it.\"\n",
      " 'what a waste of time!'\n",
      " 'this item is great, installed it, let it charged up overnite and it has been working good every since with no problems'\n",
      " \"doesn't hold charge.\" 'absolutely great.' 'great...no problems at all!.'\n",
      " 'improper description.... i had to return it.'\n",
      " 'earbud piece breaks easily.'\n",
      " \"i connected my wife's bluetooth,(motorola hs) to my phone and it worked like a charm whether the phone was in my pocket or the case.\"\n",
      " \"doesn't last long.\" 'good audio quality.'\n",
      " 'protects the phone on all sides.' 'wont work right or atleast for me.'\n",
      " 'i received my headset in good time and was happy with it.'\n",
      " 'it was that loud.glad to say that the plantronics  maintains a flawless connection to my cell and with no static during normal use.'\n",
      " 'i get absolutely horrible reception in my apartment, where with other phones i have not had this problem.'\n",
      " \"but when i check voice mail at night, the keypad backlight turns off a few seconds into the first message, and then i'm lost.\"\n",
      " 'very good stuff for the price.' 'does not fit.'\n",
      " 'i have bought this nokia cell phone a few weeks ago and it was a nightmare.'\n",
      " 'this product had a strong rubber/petroleum smell that was unbearable after a while and caused me to return it'\n",
      " 'having trouble with volume.' 'great phone.'\n",
      " 'i have tried these cables with my computer and my ipod and it works just fine.'\n",
      " 'this is the phone to get for .... i just bought my sa and all i can say is wow!'\n",
      " \"don't waste your $$$ on this one.\"\n",
      " 'linksys should have some way to exchange a bad phone for a refurb unit or something!'\n",
      " 'i have two more years left in this contract and i hate this phone.'\n",
      " 'product was excellent and works better than the verizon one and boy was it cheaper!'\n",
      " 'excellent bluetooth headset.' 'was not happy.'\n",
      " 'i really recommend this faceplates since it looks very nice, elegant and cool.'\n",
      " \"it didn't charge for me.\" 'useless phone, simply deaf.'\n",
      " 'my sanyo has survived dozens of drops on blacktop without ill effect.'\n",
      " 'this blueant supertooth hands-free phone speaker is awesome.'\n",
      " 'i highly recommend these and encourage people to give them a try.'\n",
      " 'this product is great... it makes working a lot easier i can go to the copier while waiting on hold for something.'\n",
      " 'this one works and was priced right.'\n",
      " 'nice docking station for home or work.'\n",
      " 'excellent product, i am very satisfied with the purchase.'\n",
      " 'trying to make a call on these is an exercise in frustration.'\n",
      " 'due to this happening on every call i was forced to stop using this headset.'\n",
      " \"i've missed numerous calls because of this reason.\"\n",
      " 'comfortable in my hand.'\n",
      " 'my father has the v, and the battery is dying.'\n",
      " 'it would take too long to describe how bad my customer service experience has been with amazon.'\n",
      " 'its well-designed and very sharp -- the blue is a very nice color.'\n",
      " 'i also didn\\'t like the \"on\" button, it felt like it would crack with use.'\n",
      " 'none of the new ones have ever quite worked properly.' \"doesn't work.\"\n",
      " 'plus, i seriously do not believe it is worth its steep price point.'\n",
      " 'much less than the jawbone i was going to replace it with.'\n",
      " 'awkward to use and unreliable.' \"don't make the same mistake i did.\"\n",
      " 'the design might be ergonomic in theory but i could not stand having these in my ear.'\n",
      " 'in my house i was getting dropped coverage upstairs and no coverage in my basement.'\n",
      " 'great price also!' 'poor product.'\n",
      " 'terrible.. my car will not accept this cassette.'\n",
      " 'reaching for the bottom row is uncomfortable, and the send and end keys are not where i expect them to be..'\n",
      " \"it's been my choice headset for years.great sound; good volume; good noise cancellation.\"\n",
      " 'other than that, the leather is nice and soft; the fit is very tight; the cut out for the face is a good shape.'\n",
      " 'i cannot make calls at certain places.'\n",
      " \"you can't beat the price on these.\"\n",
      " 'this case has passed the one year mark and while it shows signs of wear, it is % functional.'\n",
      " 'leopard print is wonderfully wild!.'\n",
      " \"it quit working after i'd used it for about  months, so i just purchased another one because this is the best headset i've ever owned.\"\n",
      " 'really pleased with this product so far.'\n",
      " 'it feels more comfortable than most headsets because i wear glasses and that gets in the way sometimes.'\n",
      " 'i bought it for my mother and she had a problem with the battery.'\n",
      " 'you can even take self portraits with the outside (exterior) display, very cool.'\n",
      " 'i would recommend this.'\n",
      " 'provides good protection and looks classy, too.'\n",
      " 'the bt headset was such a disapoinment.'\n",
      " 'i was sitting in my vehicle, with the cradle on my belt, and the headset lost signal.'\n",
      " 'this phone is slim and light and the display is beautiful.'\n",
      " \"doesn't work at all.. i bougth it for my lc and its not working.\"\n",
      " \"i've had this for nearly  years and it has worked great for me.\"\n",
      " 'i used to talk on it for  -  / hours and the battery would be literally drained and dying.'\n",
      " 'its a total package.' 'avoid this one if you can.'\n",
      " \"this phone tries very hard to do everything but fails at it's very ability to be a phone.\"\n",
      " 'linked to my phone without effort.' 'good , works fine.'\n",
      " 'the headset fulfills my requirements so i am happy with my purchase.'\n",
      " 'really ugly.' 'piece of junk.' 'looks great and is strong.\"'\n",
      " 'however-the riingtones are not the best, and neither are the games.'\n",
      " 'could not get strong enough signal.' 'kind of flops around.'\n",
      " 'the holster that arrived did not match the photo in the ad.'\n",
      " 'i have had this phone for over a year now, and i will tell you, its not that great.'\n",
      " 'it is well made, easy to access the phone and has a handy, detachable belt clip.'\n",
      " 'what possesed me to get this junk, i have no idea...'\n",
      " 'i searched the internet, and found this one to be the best value.'\n",
      " 'it is very comfortable on the ear.' 'poor voice clarity.'\n",
      " 'great pocket pc / phone combination.'\n",
      " 'sound quality on both end is excellent, i use headset to call my wife and ask my wife to use headset to call me !.'\n",
      " 'after  months, screen just went black all of a sudden.'\n",
      " 'i had absolutely no problem with this headset linking to my  blackberry curve!'\n",
      " 'no real improvement.'\n",
      " 'the phone can also take great pictures and even video clips.'\n",
      " 'do not buy do not buyit sucks' 'we would recommend these to others.'\n",
      " 'this is a great phone!.' 'voice recognition is tremendous!'\n",
      " '.... item arrived quickly and works great with my metro pcs samsung sch-r slider phone and sony premium sound in ear plugs.'\n",
      " 'good product - incredible value.'\n",
      " 'i was very impressed with the price of the cases.'\n",
      " 'i was able to do voice dialing in the car with no problem.'\n",
      " \"don't buy this product - it fails!.\" 'internet is excrutiatingly slow.'\n",
      " \"i'm glad i found this product on amazon it is hard to find, it wasn't high priced.\"\n",
      " \"i'll be looking for a new earpiece.\" 'sprint charges for this service.'\n",
      " 'it clicks into place in a way that makes you wonder how long that mechanism would last.'\n",
      " 'amazon sucks.'\n",
      " 'works great, when my cat attacked the phone he scratched the protective strip instead of destroying the screen.'\n",
      " 'love this headset!'\n",
      " \"i usually don't like headbands but this one is very lightweight & doesn't mess up my hair.\"\n",
      " 'dont waste your money...' 'mobile phone tools are a must have.'\n",
      " 'the only very disappointing thing was there was no speakerphone!!!!'\n",
      " 'however, bt headsets are currently not good for real time games like first-person shooters since the audio delay messes me up.'\n",
      " 'good product, good seller.' \"i wouldn't recommend buying this product.\"\n",
      " 'i got the car charger and not even after a week the charger was broken...i went to plug it in and it started smoking.'\n",
      " 'never got it!!!!!'\n",
      " 'i would recommend purchasing the jabra jx- series  which works flawlessly with my moto q, go figure.'\n",
      " 'i recently had problems where i could not stay connected for more than  minutes before being disconnected.'\n",
      " 'display is excellent and camera is as good as any from that year.'\n",
      " 'i was very pleased to see that i could replace my well travled swivel holster for my blackberry flip.'\n",
      " 'sweetest phone!!!' 'reversible plug works great.'\n",
      " 'at first i thought i was grtting a good deal at $., until i plugged it into my phone (vc razr).'\n",
      " \"however, after about a year, the fliptop started to get loose and wobbly and eventually my screen went black and i couldn't receive and place calls.\"\n",
      " 'the sound quality is good and functionality is awesome.'\n",
      " 'it is unusable in a moving car at freeway speed.'\n",
      " 'i wear it everyday and it holds up very well.'\n",
      " \"as many people complained, i found this headset's microphone was very weak.\"\n",
      " 'after the first charge kept going dead after  minutes.'\n",
      " 'he was very impressed when going from the original battery to the extended battery.'\n",
      " 'i only hear garbage for audio.' 'this phone works great.'\n",
      " 'this is a great deal.' 'it has kept up very well.'\n",
      " 'echo problem....very unsatisfactory'\n",
      " 'i have been very happy with the  and have had no complaints from any one regarding my sound quality on their end.'\n",
      " 'i was not impressed by this product.'\n",
      " 'treo and t-mobile refused to replace it again and forced me to buy another phone without any kind of upgrade discount.'\n",
      " 'i highly recommend this device to everyone!'\n",
      " \"all in all, i'm quite satisfied with this purchase.\"\n",
      " 'we are sending it back.'\n",
      " 'items stated as included from the description are not included.'\n",
      " 'i great reception all the time.' 'better than new.' 'bad quality.'\n",
      " 'we have tried  units and they both failed within  months.. pros'\n",
      " \"i've had no trouble accessing the internet, downloading ringtones or performing any of the functions.\"\n",
      " \"best i've found so far .... i've tried  other bluetooths and this one has the best quality (for both me and the listener) as well as ease of using.\"\n",
      " 'this is a very average phone with bad battery life that operates on a weak network.'\n",
      " \"it is so small and you don't even realize that it is there after a while of getting used to it.\"\n",
      " 'comfortable fit - you need your headset to be comfortable for at least an hour at a time, if not for an entire day.'\n",
      " \"i have yet to run this new battery below two bars and that's three days without charging.\"\n",
      " 'just really good.. so far, probably the best bt headset i have ever had.'\n",
      " \"it's aggravating!\"\n",
      " \"i'll be drivng along, and my headset starts ringing for no reason.\"\n",
      " 'the sound is clear and the people i talk to on it are amazed at the quality too.'\n",
      " 'after trying many many handsfree gadgets this is the one that finally works well.'\n",
      " 'all in all i think it was a good investment.' 'love this phone.'\n",
      " 'i give wirefly  star.i will contact cingular/at&t; and inform them of this practice.'\n",
      " \"it doesn't work in europe or asia.\" 'the best phone in market :).'\n",
      " 'it was a waste of my money.' 'product is exactly as described.'\n",
      " 'excellent phone.' 'the nokia ca- usb cable did not work with my phone.'\n",
      " 'the cable looks so thin and flimsy, it is scary.'\n",
      " 'overall, i would recommend this phone over the new walkman.'\n",
      " 'bad choice.' 'will order from them again!'\n",
      " 'also makes it easier to hold on to.' \"i'm returning them.\"\n",
      " 'one thing i hate is the mode set button at the side.'\n",
      " 'verizon tech support walked my through a few procedures, none of which worked and i ended up having to do a hard re-set, wiping out all my data.'\n",
      " 'i received it quickly and it works great!!'\n",
      " \"muddy, low quality sound, and the casing around the wire's insert was poorly super glued and slid off.\"\n",
      " 'these products cover up the important light sensor above the ear outlet.'\n",
      " \"the worst phone i've ever had.... only had it for a few months.\"\n",
      " 'if you simply want a small flip phone -- look elsewhere as the extra bells & whistles are mediocre.'\n",
      " 'i own  of these cases and would order another.'\n",
      " \"i'm a bit disappointed.\"\n",
      " 'this does not fit the palm tungsten e and it broke the first time i tried to plug it in.'\n",
      " 'very disappointed.' 'used and dirty.' \"don't buy this product.\"\n",
      " 'the item received was counterfeit.'\n",
      " 'the accompanied software is almost brilliant.'\n",
      " 'i had to purchase a different case.'\n",
      " 'after arguing with verizon regarding the dropped calls we returned the phones after two days.'\n",
      " 'excellent sound, battery life and inconspicuous to boot!.'\n",
      " 'i have - bars on my cell phone when i am home, but you cant not hear anything.'\n",
      " 'the best electronics of the available fm transmitters.'\n",
      " 'this item worked great, but it broke after  months of use.'\n",
      " 'you get what you pay for i guess.' 'disapointing results.'\n",
      " \"clipping this to your belt will deffinitely make you feel like  cent's up-and-coming.\"\n",
      " 'sounds good reasonably priced and effective, its that simple !'\n",
      " 'great phone.' 'made very sturdy.'\n",
      " 'well packaged, arrived on time, and works as intended.'\n",
      " 'fantastic earphones.'\n",
      " \"after my phone got to be about a year old, it's been slowly breaking despite much care on my part.\"\n",
      " \")setup couldn't have been simpler.\" 'works fine.' \"don't buy it.\"\n",
      " 'i even dropped this phone into a stream and it was submerged for  seconds and it still works great!'\n",
      " 'i ordered this product first and was unhappy with it immediately.'\n",
      " 'great earpiece.'\n",
      " \"don't trust their website and don't expect any helpful support.\"\n",
      " 'otherwise, easy to install and use, clear sound.'\n",
      " 'they made this case too small and is very difficult to install.'\n",
      " 'nice headset priced right.' 'however i needed some better instructions.'\n",
      " 'it worked very well.'\n",
      " \". long lasting battery (you don't have to recharge it as frequentyly as some of the flip phones).\"\n",
      " 'i gave it  stars because of the sound quality.'\n",
      " \"it's very convenient and simple to use - gets job done & makes the car ride so much smoother.\"\n",
      " 'then i exchanged for the same phone, even that had the same problem.'\n",
      " \"i love the camera, it's really pretty good quality.\"\n",
      " 'terrible phone holder.'\n",
      " \"this phone is pretty sturdy and i've never had any large problems with it.\"\n",
      " '[...] down the drain because of a weak snap!'\n",
      " 'i kept catching the cable on the seat and i had to pull the phone out to turn it on an off.'\n",
      " \"i've only had my bluetooth for a few weeks, but i really like it.\"\n",
      " 'great price, too!'\n",
      " 'people couldnt hear me talk and i had to pull out the earphone and talk on the phone.'\n",
      " 'i have this phone and it is a thorn in my side, i really abhor it.'\n",
      " 'the only good thing was that it fits comfortably on small ears.'\n",
      " 'horrible phone.'\n",
      " 'this frog phone charm is adorable and very eye catching.'\n",
      " \"can't upload ringtones from a third party.\"\n",
      " 'audio quality is poor, very poor.'\n",
      " 'it defeats the purpose of a bluetooth headset.' 'not worth it.'\n",
      " \"don't buy this product.\"\n",
      " 'sucked, most of the stuff does not work with my phone.'\n",
      " 'disappointing accessory from a good manufacturer.'\n",
      " \"it doesn't make you look cool.\" 'battery life is real good.'\n",
      " 'poorly contstruct hinge.'\n",
      " \"i'm using it with an iriver spinn (with case) and it fits fine.\"\n",
      " 'good show, samsung.' 'its reception is very very poor.'\n",
      " 'it felt too light and \"tinny.\".'\n",
      " \"a good quality bargain.. i bought this after i bought a cheapy from big lots that sounded awful and people on the other end couldn't hear me.\"\n",
      " \"however, my girl was complain that some time the phone doesn't wake up like normal phone does.\"\n",
      " 'i put the latest os on it (v.g), and it now likes to slow to a crawl and lock up every once in a while.'\n",
      " \"i'm very disappointed with my decision.\" 'it looks very nice.'\n",
      " 'the screen size is big, key pad lit well enough, and the camera quality is excellent for a camera phone.'\n",
      " 'it finds my cell phone right away when i enter the car.'\n",
      " 'very unreliable service from t-mobile !'\n",
      " 'appears to actually outperform the original battery from china that came with my vi.'\n",
      " 'then a few days later the a puff of smoke came out of the phone while in use.'\n",
      " 'the keyboard is a nice compromise between a full qwerty and the basic cell phone number keypad.'\n",
      " 'while i managed to bend the leaf spring back in place, the metal now has enough stress that it will break on the next drop.'\n",
      " 'worthwhile.' 'great choice!' 'stay away from the q!.'\n",
      " 'small, sleek, impressive looking, practical setup with ample storage in place.'\n",
      " 'truly awful.' 'i bought two of them and neither will charge.'\n",
      " 'i am going to have to be the first to negatively review this product.'\n",
      " 'the file browser offers all the options that one needs.handsfree is great.'\n",
      " 'the construction of the headsets is poor.'\n",
      " 'thank you for such great service.' 'its not user friendly.'\n",
      " 'i love this cable - it allows me to connect any mini-usb device to my pc.'\n",
      " 'the battery is unreliable as well as the service use antena.'\n",
      " 'using all earpieces, left or right, this thing will not stay on my ear.'\n",
      " \"their research and development division obviously knows what they're doing.\"\n",
      " 'disappointment.. i hate anything that goes in my ear.'\n",
      " \"mic doesn't work.\"\n",
      " 'for the price on amazon, it is an excellent product, which i would highly recommend.'\n",
      " \"yes it's shiny on front side - and i love it!\"\n",
      " \"the camera on the phone may be used as a dustpan when indoors... i'd rather be using a disposable then this.\"\n",
      " 'very good phone.'\n",
      " 'think it over when you plan to own this one!this sure is the last moto phone for me!'\n",
      " 'what a waste of money and time!.'\n",
      " 'the microphone also works well, but (according to people i have called) it applifies everything.'\n",
      " 'this is cool because most cases are just open there allowing the screen to get all scratched up.'\n",
      " 'this particular model would not work with my motorola q smartphone.'\n",
      " 'the calls drop, the phone comes on and off at will, the screen goes black and the worst of all it stops ringing intermittently.'\n",
      " 'lately they have been extremely nice and helpful on the phone.'\n",
      " 'i recommend igo to anyone with different brand cell phones/mp players in the family.'\n",
      " \"none of it works, just don't buy it.\" 'chinese forgeries abound!.']\n",
      "<class 'numpy.str_'>\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"amazon_cells_labelled.txt\", delimiter='\\t', header=None)\n",
    "data.columns = ['Sentence', 'Class']\n",
    "data['index'] = data.index                                          # add new column index\n",
    "columns = ['index', 'Class', 'Sentence']\n",
    "data = preprocess_pandas(data, columns)                             # pre-process\n",
    "training_data, validation_data, training_labels, validation_labels = train_test_split( # split the data into training, validation, and test splits\n",
    "    data['Sentence'].values.astype('U'),\n",
    "    data['Class'].values.astype('int32'),\n",
    "    test_size=0.10,\n",
    "    random_state=0,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(training_data)\n",
    "print(type(training_data[0]))\n",
    "\n",
    "# vectorize data using TFIDF and transform for PyTorch for scalability\n",
    "#word_vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,2), max_features=50000, max_df=0.5, use_idf=True, norm='l2')\n",
    "#training_data = word_vectorizer.fit_transform(training_data)        # transform texts to sparse matrix\n",
    "#training_data = training_data.todense()                             # convert to dense matrix for Pytorch\n",
    "#vocab_size = len(word_vectorizer.vocabulary_)\n",
    "#validation_data = word_vectorizer.transform(validation_data)\n",
    "#validation_data = validation_data.todense()\n",
    "\n",
    "#train_x_tensor = torch.from_numpy(np.array(training_data)).type(torch.FloatTensor)\n",
    "train_y_tensor = torch.from_numpy(np.array(training_labels)).long()\n",
    "#validation_x_tensor = torch.from_numpy(np.array(validation_data)).type(torch.FloatTensor)\n",
    "validation_y_tensor = torch.from_numpy(np.array(validation_labels)).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, datasetA, datasetB):\n",
    "        self.datasetA = datasetA\n",
    "        self.datasetB = datasetB\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        inp = self.datasetA[i]\n",
    "        tokens = tokenizer(inp)\n",
    "        embed = torch.tensor(vocab(tokens), dtype=torch.long)\n",
    "        label = F.one_hot(self.datasetB[i],num_classes=2)\n",
    "        output = model(embed)\n",
    "        output_flat = output.view(-1, ntokens)[0,:]\n",
    "        return output_flat,label\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.datasetA),len(self.datasetB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900 900\n"
     ]
    }
   ],
   "source": [
    "train_ds = ConcatDataset(training_data,train_y_tensor)\n",
    "val_ds = ConcatDataset(validation_data,validation_y_tensor)\n",
    "print(len(training_data),len(train_y_tensor))\n",
    "train_loader = DataLoader(train_ds,batch_size=16)\n",
    "val_loader = DataLoader(val_ds,batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.5078,  4.3731,  2.8611,  ..., -0.8538,  0.4049, -0.1154],\n",
       "         [ 0.1376,  2.7581,  2.8332,  ..., -0.1256, -0.3346, -0.9207],\n",
       "         [-0.7646,  4.1434,  0.2521,  ..., -0.7375,  0.1870,  0.0129],\n",
       "         [-0.7646,  4.1434,  0.2521,  ..., -0.7375,  0.1870,  0.0129],\n",
       "         [ 0.5078,  4.3731,  2.8611,  ..., -0.8538,  0.4049, -0.1154]],\n",
       "        grad_fn=<StackBackward0>),\n",
       " tensor([[1, 0],\n",
       "         [0, 1],\n",
       "         [1, 0],\n",
       "         [0, 1],\n",
       "         [0, 1]])]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = model\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Starting epoch {epoch+1} of {num_epochs}\")\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch_nr, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device).to(torch.float)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss = loss.to(device)\n",
    "\n",
    "            if (batch_nr%80 == 0):\n",
    "                print(f\"Processing batch number {batch_nr+1} of {len(train_loader)}\")\n",
    "                print(outputs)\n",
    "                print(labels)\n",
    "                print(\"current loss\",loss.item())\n",
    "                \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        train_loss /= len(train_loader.dataset)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device).to(torch.float)\n",
    "            outputs = model(inputs)\n",
    "        \n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    # Save the best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Starting epoch 1 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anton\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch number 1 of 57\n",
      "tensor([[0.5394, 0.4606],\n",
      "        [0.5378, 0.4622],\n",
      "        [0.5383, 0.4617],\n",
      "        [0.5376, 0.4624],\n",
      "        [0.5400, 0.4600],\n",
      "        [0.5383, 0.4617],\n",
      "        [0.5399, 0.4601],\n",
      "        [0.5405, 0.4595],\n",
      "        [0.5412, 0.4588],\n",
      "        [0.5389, 0.4611],\n",
      "        [0.5403, 0.4597],\n",
      "        [0.5392, 0.4608],\n",
      "        [0.5395, 0.4605],\n",
      "        [0.5392, 0.4608],\n",
      "        [0.5405, 0.4595],\n",
      "        [0.5401, 0.4599]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]])\n",
      "current loss 0.6985097527503967\n",
      "Starting epoch 2 of 10\n",
      "Processing batch number 1 of 57\n",
      "tensor([[0.5948, 0.4052],\n",
      "        [0.5938, 0.4062],\n",
      "        [0.5963, 0.4037],\n",
      "        [0.5944, 0.4056],\n",
      "        [0.5949, 0.4051],\n",
      "        [0.5963, 0.4037],\n",
      "        [0.5953, 0.4047],\n",
      "        [0.5960, 0.4040],\n",
      "        [0.5960, 0.4040],\n",
      "        [0.5950, 0.4050],\n",
      "        [0.5935, 0.4065],\n",
      "        [0.5938, 0.4062],\n",
      "        [0.5950, 0.4050],\n",
      "        [0.5970, 0.4030],\n",
      "        [0.5960, 0.4040],\n",
      "        [0.5937, 0.4063]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]])\n",
      "current loss 0.709623396396637\n",
      "Starting epoch 3 of 10\n",
      "Processing batch number 1 of 57\n",
      "tensor([[0.6078, 0.3922],\n",
      "        [0.6069, 0.3931],\n",
      "        [0.6064, 0.3936],\n",
      "        [0.6068, 0.3932],\n",
      "        [0.6061, 0.3939],\n",
      "        [0.6073, 0.3927],\n",
      "        [0.6077, 0.3923],\n",
      "        [0.6054, 0.3946],\n",
      "        [0.6082, 0.3918],\n",
      "        [0.6075, 0.3925],\n",
      "        [0.6069, 0.3931],\n",
      "        [0.6060, 0.3940],\n",
      "        [0.6082, 0.3918],\n",
      "        [0.6068, 0.3932],\n",
      "        [0.6057, 0.3943],\n",
      "        [0.6075, 0.3925]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]])\n",
      "current loss 0.71231609582901\n",
      "Starting epoch 4 of 10\n",
      "Processing batch number 1 of 57\n",
      "tensor([[0.6002, 0.3998],\n",
      "        [0.6037, 0.3963],\n",
      "        [0.6043, 0.3957],\n",
      "        [0.6049, 0.3951],\n",
      "        [0.6050, 0.3950],\n",
      "        [0.6029, 0.3971],\n",
      "        [0.6037, 0.3963],\n",
      "        [0.6044, 0.3956],\n",
      "        [0.6057, 0.3943],\n",
      "        [0.6050, 0.3950],\n",
      "        [0.6051, 0.3949],\n",
      "        [0.5995, 0.4005],\n",
      "        [0.6052, 0.3948],\n",
      "        [0.6000, 0.4000],\n",
      "        [0.6056, 0.3944],\n",
      "        [0.6042, 0.3958]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]])\n",
      "current loss 0.7111107707023621\n",
      "Starting epoch 5 of 10\n",
      "Processing batch number 1 of 57\n",
      "tensor([[0.5997, 0.4003],\n",
      "        [0.6002, 0.3998],\n",
      "        [0.6012, 0.3988],\n",
      "        [0.6018, 0.3982],\n",
      "        [0.6002, 0.3998],\n",
      "        [0.6003, 0.3997],\n",
      "        [0.6002, 0.3998],\n",
      "        [0.5962, 0.4038],\n",
      "        [0.6010, 0.3990],\n",
      "        [0.6025, 0.3975],\n",
      "        [0.6020, 0.3980],\n",
      "        [0.6006, 0.3994],\n",
      "        [0.6027, 0.3973],\n",
      "        [0.5983, 0.4017],\n",
      "        [0.6025, 0.3975],\n",
      "        [0.6003, 0.3997]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]])\n",
      "current loss 0.7107251286506653\n",
      "Starting epoch 6 of 10\n",
      "Processing batch number 1 of 57\n",
      "tensor([[0.5986, 0.4014],\n",
      "        [0.5953, 0.4047],\n",
      "        [0.5974, 0.4026],\n",
      "        [0.5936, 0.4064],\n",
      "        [0.5946, 0.4054],\n",
      "        [0.6001, 0.3999],\n",
      "        [0.5945, 0.4055],\n",
      "        [0.5935, 0.4065],\n",
      "        [0.5978, 0.4022],\n",
      "        [0.5957, 0.4043],\n",
      "        [0.5984, 0.4016],\n",
      "        [0.5963, 0.4037],\n",
      "        [0.5939, 0.4061],\n",
      "        [0.5946, 0.4054],\n",
      "        [0.5945, 0.4055],\n",
      "        [0.5942, 0.4058]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]])\n",
      "current loss 0.7097291350364685\n",
      "Starting epoch 7 of 10\n",
      "Processing batch number 1 of 57\n",
      "tensor([[0.5939, 0.4061],\n",
      "        [0.5883, 0.4117],\n",
      "        [0.5916, 0.4084],\n",
      "        [0.5924, 0.4076],\n",
      "        [0.5876, 0.4124],\n",
      "        [0.5915, 0.4085],\n",
      "        [0.5916, 0.4084],\n",
      "        [0.5898, 0.4102],\n",
      "        [0.5923, 0.4077],\n",
      "        [0.5929, 0.4071],\n",
      "        [0.5936, 0.4064],\n",
      "        [0.5908, 0.4092],\n",
      "        [0.5908, 0.4092],\n",
      "        [0.5927, 0.4073],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5914, 0.4086]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]])\n",
      "current loss 0.708417534828186\n",
      "Starting epoch 8 of 10\n",
      "Processing batch number 1 of 57\n",
      "tensor([[0.5899, 0.4101],\n",
      "        [0.5916, 0.4084],\n",
      "        [0.5903, 0.4097],\n",
      "        [0.5892, 0.4108],\n",
      "        [0.5883, 0.4117],\n",
      "        [0.5866, 0.4134],\n",
      "        [0.5892, 0.4108],\n",
      "        [0.5913, 0.4087],\n",
      "        [0.5849, 0.4151],\n",
      "        [0.5900, 0.4100],\n",
      "        [0.5874, 0.4126],\n",
      "        [0.5880, 0.4120],\n",
      "        [0.5885, 0.4115],\n",
      "        [0.5885, 0.4115],\n",
      "        [0.5885, 0.4115],\n",
      "        [0.5862, 0.4138]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]])\n",
      "current loss 0.7079522013664246\n",
      "Starting epoch 9 of 10\n",
      "Processing batch number 1 of 57\n",
      "tensor([[0.5866, 0.4134],\n",
      "        [0.5898, 0.4102],\n",
      "        [0.5831, 0.4169],\n",
      "        [0.5883, 0.4117],\n",
      "        [0.5845, 0.4155],\n",
      "        [0.5864, 0.4136],\n",
      "        [0.5844, 0.4156],\n",
      "        [0.5830, 0.4170],\n",
      "        [0.5831, 0.4169],\n",
      "        [0.5875, 0.4125],\n",
      "        [0.5873, 0.4127],\n",
      "        [0.5887, 0.4113],\n",
      "        [0.5831, 0.4169],\n",
      "        [0.5876, 0.4124],\n",
      "        [0.5856, 0.4144],\n",
      "        [0.5830, 0.4170]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]])\n",
      "current loss 0.7086120843887329\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[240], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model2\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mLEARNING_RATE)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[239], line 9\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, criterion, optimizer, train_loader, val_loader, num_epochs)\u001b[0m\n\u001b[0;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      8\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m----> 9\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_nr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[237], line 11\u001b[0m, in \u001b[0;36mConcatDataset.__getitem__\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m      9\u001b[0m embed \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(vocab(tokens), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m     10\u001b[0m label \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mone_hot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatasetB[i],num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m output_flat \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, ntokens)[\u001b[38;5;241m0\u001b[39m,:]\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_flat,label\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[190], line 38\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[1;34m(self, src, src_mask)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m    Unmasked positions are filled with float(0.0).\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     src_mask \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mTransformer\u001b[38;5;241m.\u001b[39mgenerate_square_subsequent_mask(\u001b[38;5;28mlen\u001b[39m(src))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 38\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(output)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\transformer.py:391\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    388\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 391\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[0;32m    394\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\transformer.py:715\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[1;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    714\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, src_mask, src_key_padding_mask, is_causal\u001b[38;5;241m=\u001b[39mis_causal))\n\u001b[1;32m--> 715\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ff_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1507\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1504\u001b[0m             tracing_state\u001b[38;5;241m.\u001b[39mpop_scope()\n\u001b[0;32m   1505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m-> 1507\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapped_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1508\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1509\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.0001\n",
    "EPOCHS = 10\n",
    "\n",
    "model2 = FFF(len(vocab))\n",
    "\n",
    "print(device)\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_model(model2, criterion, optimizer, train_loader, val_loader, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1370])\n",
      "[0.46550995111465454, 0.5344901084899902]\n",
      "You seem to like this thing\n",
      "torch.Size([1370])\n",
      "[0.46550995111465454, 0.5344901084899902]\n",
      "You seem to like this thing\n",
      "torch.Size([1370])\n",
      "[0.46550995111465454, 0.5344901084899902]\n",
      "You seem to like this thing\n",
      "torch.Size([1370])\n",
      "[0.9517692923545837, 0.04823073372244835]\n",
      "You seem to dislike this thing\n",
      "torch.Size([1370])\n",
      "[0.5945248007774353, 0.4054751396179199]\n",
      "You seem to dislike this thing\n",
      "torch.Size([1370])\n",
      "[0.9309284090995789, 0.06907162070274353]\n",
      "You seem to dislike this thing\n",
      "torch.Size([1370])\n",
      "[0.46550995111465454, 0.5344901084899902]\n",
      "You seem to like this thing\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    inp = input(\"Leave a review:\")\n",
    "    if inp==\"exit\":\n",
    "        break\n",
    "    tokens = tokenizer(inp)\n",
    "    embed = torch.tensor(vocab(tokens), dtype=torch.long)\n",
    "    output = model(embed)\n",
    "    output_flat = output.view(-1, ntokens)[0,:]\n",
    "    pred = model2(output_flat).tolist()\n",
    "    print(pred)\n",
    "    if(pred[0]>0.5):\n",
    "        print(\"You seem to dislike this thing\")\n",
    "    else:\n",
    "        print(\"You seem to like this thing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
