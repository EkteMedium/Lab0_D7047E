{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "from data_loading_code import preprocess_pandas\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, classification_report\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device is\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "# get data, pre-process and split\n",
    "data = pd.read_csv(\"amazon_cells_labelled.txt\", delimiter='\\t', header=None)\n",
    "data.columns = ['Sentence', 'Class']\n",
    "data['index'] = data.index                                          # add new column index\n",
    "columns = ['index', 'Class', 'Sentence']\n",
    "data = preprocess_pandas(data, columns)                             # pre-process\n",
    "training_data, validation_data, training_labels, validation_labels = train_test_split( # split the data into training, validation, and test splits\n",
    "    data['Sentence'].values.astype('U'),\n",
    "    data['Class'].values.astype('int32'),\n",
    "    test_size=0.10,\n",
    "    random_state=0,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# vectorize data using TFIDF and transform for PyTorch for scalability\n",
    "word_vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,2), max_features=50000, max_df=0.5, use_idf=True, norm='l2')\n",
    "training_data = word_vectorizer.fit_transform(training_data)        # transform texts to sparse matrix\n",
    "training_data = training_data.todense()                             # convert to dense matrix for Pytorch\n",
    "vocab_size = len(word_vectorizer.vocabulary_)\n",
    "validation_data = word_vectorizer.transform(validation_data)\n",
    "validation_data = validation_data.todense()\n",
    "\n",
    "train_x_tensor = torch.from_numpy(np.array(training_data)).type(torch.FloatTensor)\n",
    "train_y_tensor = torch.from_numpy(np.array(training_labels)).long()\n",
    "validation_x_tensor = torch.from_numpy(np.array(validation_data)).type(torch.FloatTensor)\n",
    "validation_y_tensor = torch.from_numpy(np.array(validation_labels)).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pandas.core.indexing._iLocIndexer object at 0x000001E2B9905E00>\n"
     ]
    }
   ],
   "source": [
    "print(data.iloc(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(train_y_tensor, batch_size=5,num_workers=2)\n",
    "it = iter(dl)\n",
    "print(next(it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, datasetA, datasetB):\n",
    "        self.datasetA = datasetA\n",
    "        self.datasetB = datasetB\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        inp = self.datasetA[i]\n",
    "        label = F.one_hot(self.datasetB[i],num_classes=2)\n",
    "        return inp,label\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.datasetA),len(self.datasetB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ConcatDataset(train_x_tensor,train_y_tensor)\n",
    "val_ds = ConcatDataset(validation_x_tensor,validation_y_tensor)\n",
    "train_loader = DataLoader(train_ds,batch_size=5)\n",
    "val_loader = DataLoader(val_ds,batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(nn.Module):\n",
    "    def __init__(self,num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.act = nn.LeakyReLU()\n",
    "\n",
    "        # input: 7277        \n",
    "        self.fc1 = nn.Linear(in_features=7277, out_features=1000) \n",
    "        self.fc2 = nn.Linear(in_features=1000, out_features=100)\n",
    "        self.fc3 = nn.Linear(in_features=100, out_features=10)\n",
    "        self.fc4 = nn.Linear(in_features=10, out_features=2)\n",
    "        self.logSoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #FC Layer 1\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "\n",
    "        #FC Layer 2\n",
    "        x = self.fc2(x)   \n",
    "        x = self.act(x)    \n",
    "\n",
    "        #FC Layer 3\n",
    "        x = self.fc3(x)   \n",
    "        x = self.act(x)    \n",
    "\n",
    "        #FC Layer 4\n",
    "        x = self.fc4(x)    \n",
    "        \n",
    "        #Softmax\n",
    "        out = self.logSoftmax(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = model\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Starting epoch {epoch+1} of {num_epochs}\")\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch_nr, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device).to(torch.float)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss = loss.to(device)\n",
    "\n",
    "            if (batch_nr%20 == 0):\n",
    "                print(f\"Processing batch number {batch_nr+1} of {len(train_loader)}\")\n",
    "                print(\"current loss\",loss.item())\n",
    "                \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        train_loss /= len(train_loader.dataset)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device).to(torch.float)\n",
    "            outputs = model(inputs)\n",
    "        \n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    # Save the best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Starting epoch 1 of 3\n",
      "Processing batch number 1 of 180\n",
      "current loss 0.6826273798942566\n",
      "Processing batch number 21 of 180\n",
      "current loss 0.6822801828384399\n",
      "Processing batch number 41 of 180\n",
      "current loss 0.6822389960289001\n",
      "Processing batch number 61 of 180\n",
      "current loss 0.7323698997497559\n",
      "Processing batch number 81 of 180\n",
      "current loss 0.682338297367096\n",
      "Processing batch number 101 of 180\n",
      "current loss 0.6591402888298035\n",
      "Processing batch number 121 of 180\n",
      "current loss 0.6814399361610413\n",
      "Processing batch number 141 of 180\n",
      "current loss 0.6821044683456421\n",
      "Processing batch number 161 of 180\n",
      "current loss 0.7029614448547363\n",
      "Starting epoch 2 of 3\n",
      "Processing batch number 1 of 180\n",
      "current loss 0.6775676608085632\n",
      "Processing batch number 21 of 180\n",
      "current loss 0.6727981567382812\n",
      "Processing batch number 41 of 180\n",
      "current loss 0.6570238471031189\n",
      "Processing batch number 61 of 180\n",
      "current loss 0.6505029201507568\n",
      "Processing batch number 81 of 180\n",
      "current loss 0.6175647974014282\n",
      "Processing batch number 101 of 180\n",
      "current loss 0.6291241645812988\n",
      "Processing batch number 121 of 180\n",
      "current loss 0.5212859511375427\n",
      "Processing batch number 141 of 180\n",
      "current loss 0.48451662063598633\n",
      "Processing batch number 161 of 180\n",
      "current loss 0.3637100160121918\n",
      "Starting epoch 3 of 3\n",
      "Processing batch number 1 of 180\n",
      "current loss 0.3970664143562317\n",
      "Processing batch number 21 of 180\n",
      "current loss 0.3219691812992096\n",
      "Processing batch number 41 of 180\n",
      "current loss 0.25309860706329346\n",
      "Processing batch number 61 of 180\n",
      "current loss 0.171233132481575\n",
      "Processing batch number 81 of 180\n",
      "current loss 0.21710319817066193\n",
      "Processing batch number 101 of 180\n",
      "current loss 0.19043967127799988\n",
      "Processing batch number 121 of 180\n",
      "current loss 0.10228755325078964\n",
      "Processing batch number 141 of 180\n",
      "current loss 0.1047491580247879\n",
      "Processing batch number 161 of 180\n",
      "current loss 0.05410558730363846\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.0001\n",
    "EPOCHS = 3\n",
    "\n",
    "print(device)\n",
    "model = ANN(num_classes=2).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_model(model, criterion, optimizer, train_loader, val_loader, EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
