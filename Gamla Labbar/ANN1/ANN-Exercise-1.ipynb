{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D7046E Exercise 1 (ANN1)\n",
    "\n",
    "This exercise has three taks where you will deepen your understanding of how artificial neural networks (ANNs) are implemented and trained. First, you will represent digits on an eight-segment display as vectors and hard-code perceptrons that classifies the digits. The purpose of this task is to better understand the basic computational units in ANNs and how inputs can be represented as feature vectors. Secondly, you will implement and train neural networks using [pytorch](https://pytorch.org/) on the seven-segment display data and the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset which you are familiar with from Exercise 0. Finally, you will implement a neural network including the forward (inference) pass and the backward (learning) pass from scratch using numpy. After completing these steps you will know the central building blocks of ANNs.\n",
    "\n",
    "## Literature\n",
    "Before starting with the implementation you should familiarize yourself with two additional chapters in the [deep learning book](https://www.deeplearningbook.org/). This will help you understand the theory behind neural networks and what mathematical formulas are important for the task. The lectures has touched on most of these concepts. Below is a list of recommended sections from the book. If you feel familiar with the contents of these sections, feel free to skip it.\n",
    "\n",
    "* Chapter 6 - Deep feedforward networks\n",
    "    - Section 6.0 - Discusses what do we mean by feedfoward networks and terminology such as input layer, output layer and hidden layer.\n",
    "    - Section 6.2 - Discusses what gradient based learning is and what cost functions are.\n",
    "    - Section 6.5 - Explains back propagation. Important here are the formulas 6.49 - 6.52.\n",
    "* Chapter 8 - Optimization for Training Deep Models\n",
    "    - Section 8.1.3 - Presents differences between batch (deterministic) and mini-batch (stochastic) algorithms.\n",
    "    \n",
    "## Libraries\n",
    "\n",
    "Before starting with the implementations you need to import the following libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-09T10:35:40.465251700Z",
     "start_time": "2024-01-09T10:35:37.353754400Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-09T10:35:56.967807200Z",
     "start_time": "2024-01-09T10:35:56.925697300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## First we need to define all the vectors corresponding to the various digits and add them to a list for easy access\n",
    "# Please finish the list of digit vectors\n",
    "\n",
    "x = [\n",
    "    numpy.array([1,1,1,1,1,1,0]), # 0\n",
    "    numpy.array([0,1,1,0,0,0,0]), # 1\n",
    "    numpy.array([1,1,0,1,1,0,1]), # 2\n",
    "    numpy.array([1,1,1,1,0,0,1]), # 3\n",
    "    numpy.array([0,1,1,0,0,1,1]), # 4\n",
    "    numpy.array([1,0,1,1,0,1,1]), # 5\n",
    "    numpy.array([1,0,1,1,1,1,1]), # 6\n",
    "    numpy.array([1,1,1,0,0,0,0]), # 7\n",
    "    numpy.array([1,1,1,1,1,1,1]), # 8\n",
    "    numpy.array([1,1,1,1,0,1,1]), # 9\n",
    "]\n",
    "\n",
    "# And we print one of the vectors to show you how to get a specific vector\n",
    "print(f'Digit 5 corresponds to the vector {x[5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Task 1.3: Check the solution\n",
    "\n",
    "Execute the cell below to see whether the network managed to learn to make the correct predictions.\n",
    "Can you figure out what the learned weights and biases are, and how similar they are to your hardcoded solutions in the first task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-09T14:42:41.322260300Z",
     "start_time": "2024-01-09T14:42:41.058524Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the mini-batch size\n",
    "batch_size = 1000\n",
    "\n",
    "# Download the dataset and create the dataloaders\n",
    "mnist_train = datasets.MNIST(\"./\", train=True, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Dataset is split 8:2\n",
    "train_size = int(0.8 * len(mnist_train))\n",
    "val_size = len(mnist_train) - train_size\n",
    "mnist_train, mnist_val = random_split(mnist_train, [train_size, val_size])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=False)\n",
    "mnist_test = datasets.MNIST(\"./\", train=False, download=True, transform=transforms.ToTensor())\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(mnist_val, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "to_onehot = nn.Embedding(10, 10)\n",
    "to_onehot.weight.data = torch.eye(10)\n",
    "\n",
    "def plot_digit(data):\n",
    "    data = data.view(28, 28)\n",
    "    plt.imshow(data, cmap=\"gray\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "plot_digit(images[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1\n",
    "\n",
    "Implement a 2-layer neural network using pytorch as well as a procedure for training and testing it. The training protocol should include both training and validation. Thus you need to split the training data into a training set (for which the error is backpropagated to update the parameters) and a validation set (which will not be used to directly update the model parameters, and instead be used to keep track of how good the model is at unseen data). \n",
    "\n",
    "The weights of the model which performs the best on the validation data should be stored and then be used for the final check on the test set. Validation sets are often created by taking a fraction of the training data (often, but not always, around 20%) at random. In Pytorch you might want to use [random_split](https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split) for this. Using random split would require you to edit the way the Dataloaders are created.\n",
    "\n",
    "You are free to choose any optimizer and loss function. Just note that some loss functions require the labels to be 1-hot encoded. As you will not use convolutional layers for this exercise (will be introduced later in the course), the inputs need to be transformed to 1d tensors (see [view](https://pytorch.org/docs/stable/tensors.html?highlight=view#torch.Tensor.view)).\n",
    "\n",
    "**GOAL:** You should evaluate the network from the epoch with best validation score (early stopping) on the test set aiming to reach at least 85% accuracy.\n",
    "\n",
    "**Remember** to run all your code before grading so that the teacher doesn't have to wait around for long training runs. Plot the training and validation losses for each epoch.\n",
    "\n",
    "*Hint:* Validation and Testing loops are very similar to training except they don't use backpropagation. Additionally testing should only be performed once, while validation should be performed continually to make sure training is proceeding as intended and to save the parameters of the best epoch.\n",
    "\n",
    "*Hint:* Storing the best model is more difficult than just assigning it to a variable as this only means you have two variables referencing the same network instance in memory (not a copy of the best betwork and one containing the current network). Instead you ned to make a copy of the network which can be achived with [deepcopy](https://docs.python.org/3/library/copy.html). Other ways to store models include saving them as a file which can be done with [torch.save](https://pytorch.org/tutorials/beginner/saving_loading_models.html).\n",
    "\n",
    "*Hint:* Everytime you train a network with random parameter initialization and random batches you get networks with different performance. Sometimes just running the training again can be enough to get a better result. However, if you do this too many times you run the risk of training (overfitting) on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code initializes the neural network\n",
    "from copy import deepcopy\n",
    "\n",
    "correct_test = 0\n",
    "total_test = 0\n",
    "test_accuracy = 0\n",
    "one_d_tensor = torch.DoubleTensor\n",
    "\n",
    "# nn.Sequential can be given a list of neural network modules\n",
    "network = nn.Sequential(\n",
    "    nn.Linear(784, 100),  # First layer of the network takes the entire image and reduces it to 100 dimensions\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 10)  # The second layer takes those 100 dimensions and reduces them into estimated values for each digit\n",
    ")\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = optim.SGD(network.parameters(), lr=0.1)\n",
    "\n",
    "# Initialize the loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Decide the number of epochs to train for (one epoch is one optimization iteration on the entire dataset)\n",
    "epochs = 15\n",
    "best_val_loss = float('inf')\n",
    "best_model = None\n",
    "\n",
    "# For each epoch\n",
    "for epoch in range(epochs):\n",
    "    network.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # For each batch of data (since the dataset is too large to run all data through the network at once)\n",
    "    for batch_nr, (images, labels) in enumerate(train_loader):\n",
    "        # Reshape the images to a single vector (28*28 = 784)\n",
    "        images = images.view(-1, 784)\n",
    "        # Predict for each digit in the batch what class they belong to\n",
    "        prediction = network(images)\n",
    "        # Calculate the loss of the prediction by comparing to the expected output\n",
    "        loss = loss_function(prediction, labels)\n",
    "        # Backpropagate the loss through the network to find the gradients of all parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Update the parameters along their gradients\n",
    "        optimizer.step()\n",
    "        # Clear stored gradient values\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Validation loop\n",
    "    network.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_images, val_labels in val_loader:\n",
    "            val_images = val_images.view(-1, 784)\n",
    "            val_prediction = network(val_images)\n",
    "            val_loss += loss_function(val_prediction, val_labels).item()\n",
    "            _, predicted = torch.max(val_prediction, 1)\n",
    "            total_test += val_labels.size(0)\n",
    "            correct_test += (predicted == val_labels).sum().item()\n",
    "            test_accuracy = correct_test / total_test\n",
    "        if test_accuracy >= 0.85:\n",
    "            print(f'Validation Accuracy: {test_accuracy:.2%}')\n",
    "            best_model = deepcopy(network.state_dict())\n",
    "            break\n",
    "\n",
    "    average_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    # Print training and validation loss\n",
    "    print(f'Epoch [{epoch + 1}/{epochs}], Training Loss: {average_loss:.4f}, Validation Loss: {average_val_loss:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 189.85707447559895\n",
      "Epoch 2/25, Loss: 78.92555122692234\n",
      "Epoch 3/25, Loss: 43.549837555039275\n",
      "Epoch 4/25, Loss: 29.05472949062305\n",
      "Epoch 5/25, Loss: 19.968473441754373\n",
      "Epoch 6/25, Loss: 14.575056776546084\n",
      "Epoch 7/25, Loss: 11.05615583273368\n",
      "Epoch 8/25, Loss: 7.584834397876871\n",
      "Epoch 9/25, Loss: 5.2754942043243425\n",
      "Epoch 10/25, Loss: 3.663674112280544\n",
      "Epoch 11/25, Loss: 2.536832771815574\n",
      "Epoch 12/25, Loss: 1.828554286143176\n",
      "Epoch 13/25, Loss: 1.253232685645703\n",
      "Epoch 14/25, Loss: 0.8430116012466082\n",
      "Epoch 15/25, Loss: 0.559396854219661\n",
      "Epoch 16/25, Loss: 0.3739471618762161\n",
      "Epoch 17/25, Loss: 0.26532834548000717\n",
      "Epoch 18/25, Loss: 0.21725520265596243\n",
      "Epoch 19/25, Loss: 0.21711750241482533\n",
      "Epoch 20/25, Loss: 0.20714639461384934\n",
      "Epoch 21/25, Loss: 0.2022158339925319\n",
      "Epoch 22/25, Loss: 0.20852119458875162\n",
      "Epoch 23/25, Loss: 0.20612269629048643\n",
      "Epoch 24/25, Loss: 0.21280928538182176\n",
      "Epoch 25/25, Loss: 0.22983628055542965\n",
      "Epoch [25/25], Validation Accuracy: 12.20%\n",
      "hej\n",
      "Test Accuracy: 83.16%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import math\n",
    "\n",
    "epochs = 25  # Set the number of epochs to train for\n",
    "D_in = 784   # Input size, images are 28x28 = 784 element vectors\n",
    "D_out = 10   # Output size, 10 digit classes\n",
    "H1 = 100     # Hidden layer size\n",
    "gamma = 1e-5 # Learning rate\n",
    "batch_size = 250\n",
    "# Define network with one hidden layer, random initial weights\n",
    "w1 = np.random.randn(D_in, H1)\n",
    "w2 = np.random.randn(H1, D_out)\n",
    "\n",
    "# Training iterations\n",
    "\n",
    "# Train for a number of epochs\n",
    "for epoch in range(epochs):\n",
    "    # Training by looping over training set\n",
    "    for inputs, labels in train_loader:\n",
    "        \n",
    "        inputs = inputs.numpy()\n",
    "        labels = labels.numpy()\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            # iterate through the mini-batch and perform forward pass and backward pass\n",
    "            x = inputs[i].reshape((1, D_in))\n",
    "            y = np.eye(10)[labels[i]]    # 1-hot encoding\n",
    "\n",
    "            # Forward pass\n",
    "            h = np.dot(x, w1)\n",
    "            h_relu = np.maximum(0, h)\n",
    "            y_pred = np.dot(h_relu, w2)\n",
    "\n",
    "            # Compute loss function, squared error\n",
    "            squared_error = (y_pred - y) ** 2\n",
    "            loss = np.sum(squared_error) / 2\n",
    "            # sum_squared_error = np.sum(squared_error)\n",
    "            # loss = sum_squared_error / y.size\n",
    "            \n",
    "            # Compute gradients of square-error loss with respect to w1 and w2 using backpropagation\n",
    "            #dL_dy_pred = -(y - y_pred)\n",
    "            #dRelu = 1 if x >= 1 else 0\n",
    "            grad_y_pred = y_pred - y\n",
    "            grad_w2 = np.dot(h_relu.T, grad_y_pred)\n",
    "            grad_h_relu = np.dot(grad_y_pred, w2.T)\n",
    "            grad_h = h.copy()\n",
    "            grad_h[h < 0] = 0\n",
    "            grad_w1 = np.dot(x.T, grad_h)\n",
    "                    # Update weights (stochastic gradient \n",
    "            w1 -= gamma * grad_w1\n",
    "            w2 -= gamma * grad_w2\n",
    "\n",
    "        # Print loss at the end of each epoch\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss}')\n",
    "\n",
    "    #validate the model\n",
    "    total_val = 0\n",
    "    correct_val = 0\n",
    "\n",
    "    for val_inputs, val_labels in val_loader:\n",
    "        for i in range(batch_size):\n",
    "            x_val = val_inputs[i].view(1, -1).numpy()\n",
    "            y_val = np.eye(10)[val_labels[i]]    # 1-hot encoding\n",
    "\n",
    "            # Forward pass on validation set\n",
    "            h_val = np.dot(x_val, w1)\n",
    "            h_relu_val = np.maximum(0, h_val)\n",
    "            y_pred_val = np.dot(h_relu_val, w2)\n",
    "\n",
    "            # Compute validation accuracy\n",
    "            total_val += 1\n",
    "            correct_val += np.argmax(y_pred_val) == np.argmax(y_val)\n",
    "\n",
    "val_accuracy = correct_val / total_val\n",
    "print(f'Epoch [{epoch + 1}/{epochs}], Validation Accuracy: {val_accuracy:.2%}')\n",
    "print(\"hej\")\n",
    "            \n",
    "#t training and validation loss\n",
    "\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "for test_inputs, test_labels in test_loader:\n",
    "    for i in range(batch_size):\n",
    "        x_test = test_inputs[i].reshape((1, D_in)).numpy()\n",
    "        y_test = np.eye(10)[test_labels[i]]    # 1-hot encoding\n",
    "\n",
    "        # Forward pass on test set\n",
    "        h_test = np.dot(x_test, w1)\n",
    "        h_relu_test = np.maximum(0, h_test)\n",
    "        y_pred_test = np.dot(h_relu_test, w2)\n",
    "\n",
    "        # Compute test accuracy\n",
    "        total_test += 1\n",
    "        correct_test += np.argmax(y_pred_test) == np.argmax(y_test)\n",
    "\n",
    "test_accuracy = correct_test / total_test\n",
    "print(f'Test Accuracy: {test_accuracy:.2%}')           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## End\n",
    "\n",
    "You have now reached the end of ANN1. When you have completed and understood the task above please make sure that all results inluding plots have been computed and then schedule a meeting with a teacher. The teacher will then assess orally that you (the lab group) has completed the exercise and that you understand its essental elements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
