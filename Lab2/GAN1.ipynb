{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset, random_split\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "from torchvision.transforms import v2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, classification_report\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device is\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "        # input: 100      \n",
    "        self.fc1 = nn.Linear(in_features=100, out_features=128) \n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=784)\n",
    "\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x) #100->128\n",
    "        x = self.act(x) #128->128\n",
    "        x = self.fc2(x) #128->784 (21x21)\n",
    "        x = self.sig(x) #784->784 (21x21)\n",
    "        return x\n",
    "    \n",
    "def Generator_loss(output_fake):\n",
    "    return -torch.log(output_fake)\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "        # input: 784     \n",
    "        self.fc1 = nn.Linear(in_features=784, out_features=128) \n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=1)\n",
    "\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x) #784->128\n",
    "        x = self.act(x) #128->128\n",
    "        x = self.fc2(x) #128->1\n",
    "        x = self.sig(x) #1->1\n",
    "        return x\n",
    "    \n",
    "def Discriminator_loss(output_real, output_fake):\n",
    "    return -(torch.log(output_real)+torch.log(1-output_fake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the mini-batch size\n",
    "batch_size = 1000\n",
    "\n",
    "# Download the dataset and create the dataloaders\n",
    "mnist_train = datasets.MNIST(\"./\", train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_test = datasets.MNIST(\"./\", train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Dataset is split 8:2\n",
    "train_size = int(0.8 * len(mnist_train))\n",
    "val_size = len(mnist_train) - train_size\n",
    "mnist_train, mnist_val = random_split(mnist_train, [train_size, val_size])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(mnist_val, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "to_onehot = nn.Embedding(10, 10)\n",
    "to_onehot.weight.data = torch.eye(10)\n",
    "\n",
    "def plot_digit(data):\n",
    "    data = data.view(28, 28)\n",
    "    plt.imshow(data, cmap=\"gray\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "plot_digit(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(model_d, criterion_d, optimizer_d, model_g, criterion_g, optimizer_g, epochs, train_loader, k, batch_sz):\n",
    "    it = iter(train_loader)\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch:\",epoch)  \n",
    "          \n",
    "        #Train Discriminator \n",
    "        for batch_nr in range(k):\n",
    "            try:\n",
    "                images,_ = next(it)\n",
    "            except:\n",
    "                it = iter(train_loader)\n",
    "                images,_ = next(it)          \n",
    "            out_real = model_d(torch.flatten(images,start_dim=1).to(device))\n",
    "            out_fake = model_d(model_g(torch.rand(batch_sz,1,100).to(device)))\n",
    "            optimizer_d.zero_grad()\n",
    "            loss_d = criterion_d(out_real,out_fake).to(device)\n",
    "            loss_d.backward()\n",
    "            optimizer_d.step()\n",
    "\n",
    "        #Train Generator\n",
    "        out_fake = model_d(model_g(torch.rand(batch_sz,1,100).to(device)))\n",
    "        optimizer_g.zero_grad()\n",
    "        loss_g = criterion_g(out_fake).to(device)\n",
    "        loss_g.backward()\n",
    "        optimizer_g.step()\n",
    "\n",
    "        plot_digit(torch.reshape(out_fake[0,:],(21,21)))\n",
    "\n",
    "    \n",
    "    \n",
    "    #End of for loop\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m criterion_gen \u001b[38;5;241m=\u001b[39m Generator_loss\n\u001b[0;32m     12\u001b[0m criterion_dis \u001b[38;5;241m=\u001b[39m Discriminator_loss\n\u001b[1;32m---> 14\u001b[0m train_models(disciminator, criterion_dis, optimizer_dis, generator, criterion_gen, optimizer_gen, epochs, train_loader, k, batch_size)\n",
      "Cell \u001b[1;32mIn[40], line 17\u001b[0m, in \u001b[0;36mtrain_models\u001b[1;34m(model_d, criterion_d, optimizer_d, model_g, criterion_g, optimizer_g, epochs, train_loader, k, batch_sz)\u001b[0m\n\u001b[0;32m     15\u001b[0m     optimizer_d\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     16\u001b[0m     loss_d \u001b[38;5;241m=\u001b[39m criterion_d(out_real,out_fake)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 17\u001b[0m     loss_d\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     18\u001b[0m     optimizer_d\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#Train Generator\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ludvi\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    524\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ludvi\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:259\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    250\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    251\u001b[0m     (inputs,)\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (torch\u001b[38;5;241m.\u001b[39mTensor, graph\u001b[38;5;241m.\u001b[39mGradientEdge))\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m()\n\u001b[0;32m    256\u001b[0m )\n\u001b[0;32m    258\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[1;32m--> 259\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _make_grads(tensors, grad_tensors_, is_grads_batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[1;32mc:\\Users\\ludvi\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:132\u001b[0m, in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 132\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    133\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    134\u001b[0m         )\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mis_floating_point:\n\u001b[0;32m    136\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    137\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    138\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    139\u001b[0m         )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "lr = 0.0001\n",
    "k = 1\n",
    "epochs = 3\n",
    "\n",
    "generator = Generator().to(device)\n",
    "disciminator = Discriminator().to(device)\n",
    "\n",
    "optimizer_gen = torch.optim.SGD(generator.parameters(), lr=lr)\n",
    "optimizer_dis = torch.optim.SGD(disciminator.parameters(), lr=lr)\n",
    "\n",
    "criterion_gen = Generator_loss\n",
    "criterion_dis = Discriminator_loss\n",
    "\n",
    "train_models(disciminator, criterion_dis, optimizer_dis, generator, criterion_gen, optimizer_gen, epochs, train_loader, k, batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
